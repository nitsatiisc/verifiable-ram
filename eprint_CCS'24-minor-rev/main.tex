%\newif\ifDraft
%\Drafttrue % comment this out for submission
\documentclass{llncs}
%\documentclass[runningheads]{llncs}
\usepackage[a4paper,margin=2.7cm]{geometry}
\input{packages.tex}  %<-------------- Add new packages here
\input{notation}
\input{macros}


\title{Batching-Efficient RAM using Updatable Lookup Arguments}

\author{Moumita Dutta\inst{1}\orcidlink{0009-0009-5135-5091} \and Chaya Ganesh\inst{1}\orcidlink{0000-0002-2909-9177} \and Sikhar Patranabis\inst{2}\orcidlink{0000-0002-2309-7939} \and Shubh Prakash\inst{1}\orcidlink{0009-0008-8180-8402} \and Nitin Singh\inst{2}\orcidlink{0009-0009-7824-042X}}

\institute{Indian Institute of Science\\
	\email{\{moumitadutta,chaya,shubhprakash\}@iisc.ac.in}
	\and
	IBM Research India \\
	\email{sikhar.patranabis@ibm.com,nitisin1@in.ibm.com}
}




\begin{document}
	\let\oldaddcontentsline\addcontentsline
	\def\addcontentsline#1#2#3{}
	\maketitle
	\def\addcontentsline#1#2#3{\oldaddcontentsline{#1}{#2}{#3}}
%	\maketitle
	
	
	
	\begin{abstract}

		RAM (random access memory) is an important primitive in verifiable computation. In this paper, we focus on realizing RAM with efficient batching property, i.e, proving a batch of $m$ updates on a RAM of size $N$ while incurring a cost that is sublinear in $N$. Classical approaches based on Merkle-trees or address ordered transcripts to model RAM correctness are either concretely inefficient, or incur linear overhead in the size of the RAM. Recent works explore cryptographic accumulators based on unknown-order groups (RSA, class-groups) to model the RAM state. While recent RSA accumulator based approaches offer significant improvement over classical methods, they incur linear overhead in the size of the accumulated set to compute witnesses, as well as prohibitive constant overheads.
		
		\medskip
		
		We realize a batching-efficient RAM with superior asymptotic and concrete costs as compared to existing approaches. Towards this: (i) we build on recent constructions of lookup arguments to allow efficient lookups even in presence of table \textit{updates}, and (ii) we realize a variant of sub-vector relation addressed in prior works, which we call {\em committed index lookup}. We combine the two building blocks to realize batching-efficient RAM with sublinear dependence on size of the RAM. Our construction incurs an amortized proving cost of $\wt{O}(m\log m + \sqrt{mN})$ for a batch of $m$ updates on a RAM of size $N$. Our results also benefit the recent arguments for sub-vector relation, by enabling them to be efficient in presence of updates to the table. We believe that this is a contribution of independent interest. 
		
		\medskip

		We implement our solution to evaluate its concrete efficiency. Our experiments show that it offers significant improvement over existing works on batching-efficient accumulators/RAMs, with a substantially reduced resource barrier.


	\end{abstract}
	
	\pagestyle{plain}
	

	\clearpage
	\tableofcontents
	
	\clearpage
	
\section{Introduction}\label{sec:introduction}

\input{introduction}



\section{Related Work}\label{sec:rel-work}
	\input{rel-work}

\section{Preliminaries}\label{sec:prelims}
    \input{prelims}

	\input{tech-overview}

\section{Memory Consistency for RAM}\label{sec:model-for-ram}
\input{poly-proto-ram}

\section{Improved Batching-Efficient RAM}\label{sec:batch-efficient-ram}
\input{batching-efficient-ram-revised}

\section{Experiments}\label{sec:experiments}
\input{experiments}

\section*{Acknowledgments}
The research of the second author is supported in part by IBM Global University Program Academic Award and Rising Star Award,  Intel Corporation.

\bibliographystyle{alpha}
\bibliography{cryptobib/abbrev3,cryptobib/crypto,main}

\appendix
\input{appendix}

\end{document}
\endinput