Efficient modelling of the random access memory (RAM) primitive is a widely studied problem in
verifiable computation (VC), due to its inherent usefulness in modelling several computations of interest.
Encapsulating RAM semantics in VC circuits is also challenging; (i) arithmetic/boolean circuits do not
adequately model random access, and (ii) incorporating entire memory as gates in a circuit is prohibitive.
Several novel techniques have been proposed to work around the above limitations of circuit based representation
of RAM. Among them, merkle tree based accumulators to model the RAM state are popular
~\cite{EPRINT:BFRSBW13,compwithstate,C:BCTV14} as they can efficiently
prove updates to the state, without modelling the entire memory in the constraint system. Other
approaches based on {\em address ordered time-scripts} avoid the concrete costs of tree based approaches
by letting the prover provide inputs and outputs of RAM operations in a non-deterministic manner, which
are then checked to satisfy consistency of {\em loads} and {\em stores}.
Several works such as \cite{NDSS:WSRBW15,USENIX:BCTV14,C:BCGTV13,SP:ZGKPP18} implement and improve variants
of the aforementioned approach. Most transcript based realizations of RAM only consider it to be transient,
i.e, its state is useful only during the execution of a program, and do not consider {\em persistence} of the
RAM state across several executions. Another feature, which has only been consisdered in recent works
\cite{USENIX:OWWB20,CCS:CFHKKO22} is {\em batching}, where a verifiable update of RAM state is required
for a batch of $m$ updates, with $m$ being much smaller than the RAM size. While the approaches based on tree
based accumulators realized using collision resistant hash functions suffer from high concrete costs and poor
ability to batch proofs, those based on checking consistency using transcripts incur a linear overhead in the
size of the RAM.

\subsection{Batching Efficient RAM}\label{subsec:batching-efficient-ram}
The focus of our work, as in recent efforts \cite{USENIX:OWWB20,CCS:CFHKKO22} is on batching efficient
realization of the RAM primitive. This is a natural setting in several recent applications of verifiable
computation, most notably in the context of blockchain {\em rollups}. In such a scenario, one is required
to show that a batch of $m$ transactions correctly updates the state of a table of account balances, which
is maintained off-chain by the rollup provider. Here the batch-size $m$ ranges from few hundreds to few
thousands, whereas the table itself could contain several million accounts.
Like prior work on batching efficient
RAMs ~\cite{USENIX:OWWB20,CCS:CFHKKO22} our work is also motivated by the problem of enabling more efficient rollup for tables, which are
naturally modelled as RAMs. While the aforementioned works substantially mitigate disadvantages of both the
merkle-tree based approaches and transcript based approaches by using RSA accumulators to model the state,
they still incur large prover costs and memory requirements even for modest sized batches. The approaches
in ~\cite{USENIX:OWWB20,CCS:CFHKKO22} encode complex modular arithmetic over RSA groups and {\em hash
to prime} functions as arithmetic circuits results in a fixed overhead of around $10$ million R1CS constraints
at batch sizes of $m=1000$ (this overhead is significantly larger for ~\cite{USENIX:OWWB20}), which already
is prohibitive on a modest hardware. In addition, the witness computation for each update incurs cost
linear in the size of accumulated set, which authors seek to mitigate through pre-computation and
parallel/distributed processing. Our solution, by contrast is almost ``circuit-free''. Our entire
RAM operation is modelled as a polynomial protocol, which is readily transformed into an argument of
knowledge using a polynomial commitment scheme. In an application such as rollups, the only part we
need expressed as a circuit is the verification of ED-DSA digital signatures on transactions,
which for is around 500 constraints per verification.
Combining our novel update protocol with highly efficient
lookup arguments such as \cite{CCS:CFHKKO22}, our online proof generation can prove around $0.5$ million
updates at an average of just $120$ seconds per batch of $1000$ updates on a commodity laptop. Even
this performance is under worst-case assumption that each update is to a different position in the table.
The expensive off-line computation, mainly consisting of highly parallelize FFTs over groups requires around $3$ hours for a
table of size 1 million, with a single-threaded implementation. Clearly, with more potent hardware and
parallelization, one can process a table of size upto $10$ million in a few hours. Our approach also
features a setup with universal and updatable SRS, which lends it a superior security posture. We
compare our work with the prior approaches more elaborately in Section ~\ref{sec:experiments}.








