Efficient modelling of the random access memory (RAM) primitive is a widely studied problem in
verifiable computation (VC), due to its inherent usefulness in modelling several computations of interest.
Encapsulating RAM semantics in VC circuits is also challenging; since (i) arithmetic/boolean circuits do not
adequately model random access, and (ii) incorporating entire memory as gates in a circuit is prohibitive.
Several novel techniques have been proposed to work around the above limitations of circuit based representation
of RAM. Among them, merkle tree based accumulators to model the RAM state are popular~\cite{EPRINT:BFRSBW13,compwithstate,C:BCTV14} as they can efficiently
prove updates to the state, without modelling the entire memory in the constraint system. Other
approaches based on {\em address ordered time-scripts} avoid the concrete costs of tree based approaches
by letting the prover provide inputs and outputs of RAM operations in a non-deterministic manner, which
are then checked to satisfy consistency of {\em loads} and {\em stores}.
Several works such as \cite{NDSS:WSRBW15,USENIX:BCTV14,C:BCGTV13,SP:ZGKPP18} implement and improve variants
of the aforementioned approach. Most transcript based realizations of RAM only consider it to be transient,
i.e, its state is useful only during the execution of a program, and do not consider {\em persistence} of the
RAM state across several executions. Another feature, which has only been considered in recent works
\cite{USENIX:OWWB20,CCS:CFHKKO22} is {\em batching}, where a verifiable update of RAM state is required
for a batch of $m$ updates, with $m$ being much smaller than the RAM size. While the approaches based on tree
based accumulators realized using collision resistant hash functions suffer from high concrete costs and poor
ability to batch proofs, those based on checking consistency using transcripts incur a linear overhead in the
size of the RAM.

\subsection{Batching-Efficient RAM}\label{subsec:batching-efficient-ram}
There have been several recent efforts~\cite{USENIX:OWWB20,CCS:CFHKKO22} on batching-efficient
realization of the RAM primitive. This is a natural setting in applications of verifiable
computation, most notably in the context of blockchain {\em rollups}. Here, one is required
to show that a batch of $m$ transactions correctly updates the state of a table of account balances, which
is maintained off-chain by the rollup provider. Here the batch-size $m$ ranges from few hundreds to few
thousands, whereas the table itself could contain several million accounts.
Like prior work on batching-efficient
RAMs~\cite{USENIX:OWWB20,CCS:CFHKKO22} our work is also motivated by the problem of enabling more efficient rollup for tables, which are
naturally modelled as RAMs. While the aforementioned works substantially mitigate disadvantages of both the
Merkle-tree based approaches and transcript-based approaches by using RSA accumulators to model the state,
they still incur large prover costs and memory requirements even for modest sized batches. The approach
in~\cite{USENIX:OWWB20,CCS:CFHKKO22} of encoding complex modular arithmetic over RSA groups and {\em hash
to prime} functions as arithmetic circuits results in a fixed overhead of around $10$ million R1CS constraints
at batch sizes of $m=1000$ (this overhead is significantly larger for ~\cite{USENIX:OWWB20}), which already
is prohibitive on a modest hardware. In addition, the witness computation for each update incurs cost
linear in the size of accumulated set, which authors seek to mitigate through pre-computation and
parallel/distributed processing. Our solution, by contrast is almost ``circuit-free''. Our entire
RAM operation is modelled as a polynomial protocol, which is readily transformed into an argument of
knowledge using a polynomial commitment scheme. In an application such as rollups, the only part of the statement 
that is expressed as a circuit is the verification of EDDSA digital signatures on transactions,
which for is around 500 constraints per verification.
Combining our novel update protocol with highly efficient
lookup arguments such as~\cite{CCS:CFHKKO22}, our online proof generation can prove around $0.5$ million
updates at an average of just $120$ seconds per batch of $1000$ updates on a commodity laptop. Even
this performance is under worst-case assumption that each update is to a different position in the table.
The expensive offline computation, mainly consisting of highly parallelized FFTs over groups, with a single-threaded implementation requires around $3$ hours for a
table of size 1 million. 
%The expensive offline computation, mainly consisting of highly parallelized FFTs over groups, requires around $3$ hours for a table of size 1 million, with a single-threaded implementation. 
Clearly, with more potent hardware and
parallelization, one can process a table of size upto $10$ million in a few hours. 
Our approach also provides guarantees in the updatable SRS setting, in constrast to trusted setup required for RSA parameter generation in RSA accumulator based approach.
%Our approach is also in the updatable SRS setting in constrast to trusted setup required for RSA parameter generation. 
We compare our work with the prior approaches more elaborately in Section ~\ref{sec:experiments}.


\subsection{Lookup arguments}

Caulk~\cite{CCS:ZBKMNS22} provides lookup arguments where a vector of size $m$ is proved to be a subvector of a large vector of size $N$ (table), while maintaining privacy regarding the positions of the subvector (called \emph{position-hiding linkability}) where the prover complexity is $O(m^2+m\log N)$ with $O(N\log N)$ preprocessing and $O(N)$ storage. Caulk+~\cite{EPRINT:PosKat22} improves over Caulk to $O(m^2)$ prover complexity by removing the dependency on the size of the large vector $N$, and CQ~\cite{EPRINT:EagFioGab22} further improves it to $O(m\log m)$, while retaining the same $O(N\log N)$ preprocessing. However, the aforementioned works only consider the setting where the table does not change, whereas we consider updates to the table. Unlike preceding works, we do not consider zero-knowledge in our construction, though we believe it can be augmented via minor modifications.

%Caulk~\cite{CCS:ZBKMNS22} provides with lookup arguments where a vector of size $m$ is proved to be a subvector of a large vector of size $N$, while maintaining privacy regarding the positions of the subvector (called \emph{position-hiding linkability}) where the prover complexity is $O(m^2+m\log N)$ with $O(N\log N)$ preprocessing and $O(N)$ storage. Caulk+~\cite{EPRINT:PosKat22} improves over Caulk to $O(m^2)$ prover complexity by removing the dependency on the size of the large vector $N$, and CQ~\cite{EPRINT:EagFioGab22} further improves it to $O(m\log m)$, while retaining the same $O(N\log N)$ preprocessing. 
%However, we aim to provide committed index lookup arguments that can `link' a set of indices in the vector of size $N$ to the `claimed' subvector of size $m$, that helps in ensuring that we can always `lookup' relevant indices from a committed vector. Additionally, note that any updates to the large table requires $O(N\log N)$ recomputation each time in the prior works. Our work ensures a batching friendly preprocessing can be done whenever there are incremental changes to the vector of size $N$, and avoid performing the $O(N\log N)$ recomputation per update. This modification extends to the other works as well. We also present techniques to extend lookup argument \cite{EPRINT:EagFioGab22}, which uses homomorphic commitments, to provide a committed index lookup argument.

Lasso~\cite{lasso} gives a new family of lookup arguments where the prover's cost only grows with table entries that are
accessed by the lookup operations, for tables that are structured. Our work applies to tables without any structure (as is the case with rollup application), and additionally we can handle \emph{updatable}%\chaya{agree on spelling of updatable throughout} 
tables. Lasso also defines a committed index lookup relation like us and gives a reduction between indexed and unindexed lookups (subvector relation); however, their reduction needs an upper bound on the elements of the table. We provide a different reduction without this restriction. 
A recent work called Jolt~\cite{jolt} expresses all constraints of a computation as lookups by creating a lookup table that contains the entire evaluation table of each instruction $f$ in an instruction set.
This approach of using table lookups requires memory checking arguments as well. However Jolt uses lookup arguments to \emph{model} CPU operations, whereas our work uses lookup arguments to reduce the size of memory in memory-checking methods. \chaya{could add a line here or in conclusion that our techniques could be used to improve Jolt's mem consistency approach.}



