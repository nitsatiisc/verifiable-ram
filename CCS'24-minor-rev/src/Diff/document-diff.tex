	%%
%DIF LATEXDIFF DIFFERENCE FILE


%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
	\providecommand\BibTeX{{%
			Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
%DIF 61-70c61-70
%DIF < \setcopyright{acmlicensed}
%DIF < \copyrightyear{2018}
%DIF < \acmYear{2018}
%DIF < \acmDOI{XXXXXXX.XXXXXXX}
%DIF < 
%DIF < %% These commands are for a PROCEEDINGS abstract or paper.
%DIF < \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%DIF < 	conference title from your rights confirmation emai}{June 03--05,
%DIF < 	2018}{Woodstock, NY}
%DIF < %%
%DIF -------
%\setcopyright{acmlicensed} %DIF > 
%\copyrightyear{2018} %DIF > 
%\acmYear{2018} %DIF > 
%\acmDOI{XXXXXXX.XXXXXXX} %DIF > 
% %DIF > 
%%% These commands are for a PROCEEDINGS abstract or paper. %DIF > 
%\acmConference[Conference acronym 'XX]{Make sure to enter the correct %DIF > 
%	conference title from your rights confirmation emai}{June 03--05, %DIF > 
%	2018}{Woodstock, NY} %DIF > 
%%% %DIF > 
%DIF -------
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
%DIF 76c76
%DIF < \acmISBN{978-1-4503-XXXX-X/18/06}
%DIF -------
%\acmISBN{978-1-4503-XXXX-X/18/06} %DIF > 
%DIF -------


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%\usepackage{llncsconf}
%\usepackage{times}
%\usepackage{amsmath,amssymb, amsthm,amsfonts,enumitem,tikz, subcaption, mdframed}
\usepackage{amsmath, amsthm,amsfonts,enumitem,tikz, subcaption, mdframed}
%DIF 116a116
\usepackage{multirow} %DIF > 
%DIF -------
%\usepackage{amssymb}
\usepackage{tcolorbox}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{pgfplots}
\usepackage{booktabs, tabularx}
%DIF 125a126
\usepackage{colortbl} %DIF > 
%DIF -------
\usepackage{url}
\usetikzlibrary{arrows,arrows.meta,shapes.arrows, calc, positioning,matrix}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{definition}{Definition}[section]


%\input{notation}
%! Author = nitin
%! Date = 03/11/23
%DIF 137c139
%DIF < 
%DIF -------
\renewcommand{\vec}{\mathbf} %DIF > 
%DIF -------
\newcommand{\npol}{\mathsf{NP}}
\newcommand{\enc}[2]{\mathsf{Enc}_{{}_{#2}}(\vec{#1})}
\newcommand{\vecA}{\vec{a}}
\newcommand{\vecB}{\vec{b}}
\newcommand{\vecC}{\vec{c}}
\newcommand{\vecD}{\vec{d}}
\newcommand{\vecP}{\vec{p}}
\newcommand{\vecQ}{\vec{q}}
\newcommand{\vecS}{\vec{s}}
\newcommand{\vecT}{\vec{T}}
\newcommand{\vecX}{\vec{x}}
\newcommand{\vecY}{\vec{y}}
\newcommand{\vecZ}{\vec{z}}
\newcommand{\vecW}{\vec{w}}
\newcommand{\vecI}{\vec{I}}
\newcommand{\vecind}{\overrightarrow{\ind}}
\newcommand{\vecop}{\overrightarrow{\op}}
\newcommand{\vecval}{\overrightarrow{\val}}
\newcommand{\setind}{I}
\newcommand{\ins}{\mathsf{ins}}
\newcommand{\ind}{\mathsf{ind}}
%DIF 159c161
%DIF < \newcommand{\op}{\mathsf{op}}
%DIF -------
\newcommand{\op}{{op}} %DIF > 
%DIF -------
\newcommand{\vecins}{\overrightarrow{\ins}}

%%% Base table and cache table
\newcommand{\vecTbase}{\vec{T}_{\mathsf{b}}}
\newcommand{\vecTcache}{\vec{T}_{\mathsf{ch}}}
\newcommand{\Tbasepoly}[1]{T_{\mathsf{b}}({#1})}
\newcommand{\Tcachepoly}[1]{T_{\mathsf{ch}}({#1})}
\newcommand{\Qbasepoly}[1]{Q_{\mathsf{b}}({#1})}
\newcommand{\Qcachepoly}[1]{Q_{\mathsf{ch}}({#1})}
%DIF 169-170c171-172
%DIF < \newcommand{\Qcachepolyone}[1]{Q'_{\mathsf{ch}}({#1})}
%DIF < \newcommand{\Qcachepolytwo}[1]{Q''_{\mathsf{ch}}({#1})}
%DIF -------
\newcommand{\Qcachepolyone}[1]{Q^{(1)}_{\mathsf{ch}}({#1})} %DIF > 
\newcommand{\Qcachepolytwo}[1]{Q^{(2)}_{\mathsf{ch}}({#1})} %DIF > 
%DIF -------
\newcommand{\Zinv}[1]{(\mathbb{Z}_#1)^{\times}}
\newcommand{\Zn}{\mathbb{Z}_N}
\newcommand{\setH}{\mathbb{K}}
\newcommand{\setV}{\mathbb{V}}
\newcommand{\setN}{\mathbb{H}}
\newcommand{\vpolyN}{Z_\mathbb{H}}
\newcommand{\mroots}{\mathbb{V}}
\newcommand{\nroots}{\mathbb{H}}
\newcommand{\iroots}{\mathbb{H}_I}
\newcommand{\BG}{\mathsf{BG}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\Fp}{{\mathbb{F}_p}}
\newcommand{\Zp}{{\mathbb{Z}_p}}
\newcommand{\Zq}{{\mathbb{Z}_q}}
\newcommand{\G}{\mathbb{G}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\V}{\mathsf{V}}
\newcommand{\Z}{\mathsf{Z}}

\newcommand{\Gone}{\mathbb{G}_1}
\newcommand{\Gtwo}{\mathbb{G}_2}
\newcommand{\GT}{\mathbb{G}_T}
\newcommand{\srs}{\mathsf{srs}}
\newcommand{\cm}{\mathsf{cm}}
\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\outp}{\rightarrow}
\newcommand{\pp}{\mathsf{pp}}
\newcommand{\pk}{\ensuremath{\mathsf{pk}}}
\newcommand{\bal}{\ensuremath{\mathsf{bal}}}
\newcommand{\txno}{\ensuremath{\mathsf{txno}}}
\newcommand{\load}{\ensuremath{\mathsf{LD}}}
\newcommand{\store}{\ensuremath{\mathsf{STR}}}
\newcommand{\RAM}[2]{\ensuremath{\mathsf{RAM}}_{\mathcal{#1}, {#2}}}
\newcommand{\RAMOp}[1]{\ensuremath{\mathcal{O}_{{}_\mathcal{#1}}}}
\newcommand{\Rupd}[1]{\ensuremath{\mathsf{Upd}_{\mathcal{#1}}}}
\newcommand{\LRAM}[3]{\ensuremath{\mathsf{LRAM}_{\mathcal{#1},#2,#3}}}
\newcommand{\LTr}{\ensuremath{\mathcal{L}_{\mathsf{Tr}}}}
\newcommand{\tr}{\ensuremath{\mathsf{tr}}}
\newcommand{\TOT}{\ensuremath{\mathsf{TimeTr}}}
\newcommand{\AOT}{\ensuremath{\mathsf{AddrTr}}}
%DIF 211-212c213-215
%DIF < \newcommand{\evalH}[1]{\ensuremath{\vec{#1}_{|_\setH}}}
%DIF < \newcommand{\evalV}[1]{\ensuremath{\vec{#1}_{|_\setV}}}
%DIF -------
\newcommand{\evalH}[1]{\ensuremath{{#1}_{|_\setH}}} %DIF > 
\newcommand{\evalV}[1]{\ensuremath{{#1}_{|_\setV}}} %DIF > 
\newcommand{\evalN}[1]{\ensuremath{{#1}_{|_\setN}}} %DIF > 
%DIF -------
\newcommand{\LLconcat}{\ensuremath{\tilde{\mathcal{L}}_{\mathsf{concat}}}}
\newcommand{\Lperm}{\ensuremath{\mathcal{L}_{\mathsf{perm}}}}
\newcommand{\RRAM}[1]{\ensuremath{R}_{\mathsf{ram},#1}}
\newcommand{\RLOOK}{\ensuremath{\mathcal{R}^{\mathsf{lookup}}_{\srs,N,m}}}
%DIF 217a220
\newcommand{\RSVEC}{\ensuremath{\mathcal{R}^{\mathsf{subvec}}_{\srs,N,m}}} %DIF > 
%DIF -------
\newcommand{\CRAM}{\ensuremath{\mathcal{R}^{\mathsf{ram}}_{\srs,N,m}}}
\newcommand{\CLRAM}{\ensuremath{\mathcal{R}^{\mathsf{LRAM}}_{\srs,m}}}
%DIF 219c223
%DIF < \newcommand{\val}[2]{\ensuremath{\mathsf{V}_{{#2},{#1}}}}
%DIF -------
\newcommand{\val}[2]{\ensuremath{\langle {#2}\rangle_{#1}}} %DIF > 
%DIF -------
\newcommand{\uniq}[1]{\ensuremath{\mathsf{uniq}(\vec{{#1}})}}
% group encodings
\newcommand{\gone}[1]{\ensuremath{\left[{#1}\right]_1}}
\newcommand{\gtwo}[1]{\ensuremath{\left[{#1}\right]_2}}
\newcommand{\gany}[1]{\ensuremath{\left[{#1}\right]_g}}

% provers, verifiers, adversaries
\newcommand{\prover}{\ensuremath{\mathcal{P}}}
\newcommand{\verifier}{\ensuremath{\mathcal{V}}}
%DIF 229a233-235
\newcommand{\subvecprover}{\prover_{\mathsf{sv}}} %DIF > 
\newcommand{\subvecverifier}{\verifier_{\mathsf{sv}}} %DIF > 
\newcommand{\argoutput}[4]{\langle {#1}({#3}), {#2}\rangle({#4})} %DIF > 
%DIF -------
\newcommand{\Adv}{\ensuremath{\mathcal{A}}}
\newcommand{\kzg}{\ensuremath{\mathsf{KZG}}}
\newcommand{\kzgprove}{\ensuremath{\mathsf{KZG.Prove}}}
\newcommand{\kzgverify}{\ensuremath{\mathsf{KZG.Verify}}}
\newcommand{\kzgcommit}{\ensuremath{\mathsf{KZG.Commit}}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\ext}{\ensuremath{\mathcal{E}}}
%\newcommand{\L}{\mathcal{L}}

%%%Author Comments--------------------------------------------
\newcommand{\moumita}[1]{\textcolor{orange}{M: #1}}
\newcommand{\chaya}[1]{\textcolor{cyan}{Chaya: #1}}
\newcommand{\sikhar}[1]{\textcolor{red}{Sikhar: #1}}
\newcommand{\nitin}[1]{\textcolor{blue}{Nitin: #1}}
\newcommand{\shubh}[1]{\textcolor{green}{Shubh: #1}}

%\newcommand{\moumita}[1]{\textcolor{orange}{}}
%\newcommand{\chaya}[1]{\textcolor{cyan}{}}
%\newcommand{\sikhar}[1]{\textcolor{red}{}}
%\newcommand{\nitin}[1]{\textcolor{blue}{}}
%\newcommand{\shubh}[1]{\textcolor{green}{}}

%group elements and commitments
\newcommand{\eltone}[1]{[#1]_1}
\newcommand{\elttwo}[1]{[#1]_2}
\newcommand{\elany}[1]{[#1]_g}
\newcommand{\elt}[1]{[\,{#1}\,]}
\newcommand{\Comone}[1]{\mathsf{Com}_1(#1)}
\newcommand{\Comtwo}[1]{\mathsf{Com}_2(#1)}
\newcommand{\ComT}[1]{\mathsf{Com}_T(#1)}

\newtheorem{fact}{Fact}[section]

%%%KZG 
\newcommand{\Com}[1]{\mathsf{Com}(#1)}
\newcommand{\KZGsetup}{\mathsf{KZG.Setup}}
\newcommand{\KZGcommit}{\mathsf{KZG.Commit}}
\newcommand{\KZGopen}{\mathsf{KZG.Prove}}
\newcommand{\KZGverify}{\mathsf{KZG.Verify}}
\newcommand{\out}{\mathsf{out}}
\newcommand{\ppt}{\mathrm{PPT}}
% \newcommand{\in}{\mathsf{in}}
\newcommand{\rangeproof}{\mathsf{Range}}

\newcommand{\setup}{\mathsf{Setup}}
\newcommand{\commit}{\mathsf{Commit}}
\newcommand{\open}{\mathsf{Open}}
\newcommand{\verify}{\mathsf{Verify}}


%-- polynomial commitment scheme--
\newcommand{\pc}{\mathsf{PC}}
\newcommand{\pcsetup}{\mathsf{Setup}}
\newcommand{\pccommit}{\mathsf{Commit}}
\newcommand{\pcopen}{\mathsf{open}}
\newcommand{\pceval}{\mathsf{eval}}
\newcommand{\pccheck}{\mathsf{check}}

%%%Elements
\newcommand{\abf}{\textbf{a}}
\newcommand{\bbf}{\textbf{b}}
\newcommand{\cbf}{\textbf{c}}
\newcommand{\ibf}{\textbf{i}}
\newcommand{\jbf}{\textbf{j}}
\newcommand{\kbf}{\textbf{k}}
\newcommand{\pbf}{\textbf{p}}
\newcommand{\qbf}{\textbf{q}}
\newcommand{\rbf}{\textbf{r}}
\newcommand{\sbf}{\textbf{s}}
\newcommand{\vbf}{\textbf{v}}
\newcommand{\xbf}{\textbf{x}}
\newcommand{\ybf}{\textbf{y}}
\newcommand{\zbf}{\textbf{z}}
\newcommand{\Tbf}{\textbf{T}}
%DIF 303c310
%DIF < \newcommand{\Sbf}{\textbf{S}}
%DIF -------
\newcommand{\Sbf}{\mathbf{S}} %DIF > 
%DIF -------
\newcommand{\Mbf}{\textbf{M}}

%%%Sets
\newcommand{\doubleH}{\mathbb{H}}
\newcommand{\Hi}{{\mathbb{H}_I}}
\newcommand{\doubleV}{\mathbb{V}}
\newcommand{\Vm}{{\mathbb{V}_m}}
\newcommand{\zeroset}{\{0\}}


%% Bilinear group generator
\newcommand{\bg}{\ensuremath{\mathsf{BG}}}
%% Security parameter
\newcommand{\secp}{\ensuremath{\lambda}}
\newcommand{\negl}{\ensuremath{\mathsf{negl}}}



%%
%% end of the preamble, start of the body of the document source.
%DIF PREAMBLE EXTENSION ADDED BY LATEXDIFF
%DIF UNDERLINE PREAMBLE %DIF PREAMBLE
\RequirePackage[normalem]{ulem} %DIF PREAMBLE
\RequirePackage{color}\definecolor{RED}{rgb}{1,0,0}\definecolor{BLUE}{rgb}{0,0,1} %DIF PREAMBLE
\providecommand{\DIFaddtex}[1]{{\protect\color{blue}\uwave{#1}}} %DIF PREAMBLE
\providecommand{\DIFdeltex}[1]{{\protect\color{red}\sout{#1}}}                      %DIF PREAMBLE
%DIF SAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddbegin}{} %DIF PREAMBLE
\providecommand{\DIFaddend}{} %DIF PREAMBLE
\providecommand{\DIFdelbegin}{} %DIF PREAMBLE
\providecommand{\DIFdelend}{} %DIF PREAMBLE
\providecommand{\DIFmodbegin}{} %DIF PREAMBLE
\providecommand{\DIFmodend}{} %DIF PREAMBLE
%DIF FLOATSAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddFL}[1]{\DIFadd{#1}} %DIF PREAMBLE
\providecommand{\DIFdelFL}[1]{\DIFdel{#1}} %DIF PREAMBLE
\providecommand{\DIFaddbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFaddendFL}{} %DIF PREAMBLE
\providecommand{\DIFdelbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFdelendFL}{} %DIF PREAMBLE
%DIF HYPERREF PREAMBLE %DIF PREAMBLE
\providecommand{\DIFadd}[1]{\texorpdfstring{\DIFaddtex{#1}}{#1}} %DIF PREAMBLE
\providecommand{\DIFdel}[1]{\texorpdfstring{\DIFdeltex{#1}}{}} %DIF PREAMBLE
\newcommand{\DIFscaledelfig}{0.5}
%DIF HIGHLIGHTGRAPHICS PREAMBLE %DIF PREAMBLE
\RequirePackage{settobox} %DIF PREAMBLE
\RequirePackage{letltxmacro} %DIF PREAMBLE
\newsavebox{\DIFdelgraphicsbox} %DIF PREAMBLE
\newlength{\DIFdelgraphicswidth} %DIF PREAMBLE
\newlength{\DIFdelgraphicsheight} %DIF PREAMBLE
% store original definition of \includegraphics %DIF PREAMBLE
\LetLtxMacro{\DIFOincludegraphics}{\includegraphics} %DIF PREAMBLE
\newcommand{\DIFaddincludegraphics}[2][]{{\color{blue}\fbox{\DIFOincludegraphics[#1]{#2}}}} %DIF PREAMBLE
\newcommand{\DIFdelincludegraphics}[2][]{% %DIF PREAMBLE
	\sbox{\DIFdelgraphicsbox}{\DIFOincludegraphics[#1]{#2}}% %DIF PREAMBLE
	\settoboxwidth{\DIFdelgraphicswidth}{\DIFdelgraphicsbox} %DIF PREAMBLE
	\settoboxtotalheight{\DIFdelgraphicsheight}{\DIFdelgraphicsbox} %DIF PREAMBLE
	\scalebox{\DIFscaledelfig}{% %DIF PREAMBLE
		\parbox[b]{\DIFdelgraphicswidth}{\usebox{\DIFdelgraphicsbox}\\[-\baselineskip] \rule{\DIFdelgraphicswidth}{0em}}\llap{\resizebox{\DIFdelgraphicswidth}{\DIFdelgraphicsheight}{% %DIF PREAMBLE
				\setlength{\unitlength}{\DIFdelgraphicswidth}% %DIF PREAMBLE
				\begin{picture}(1,1)% %DIF PREAMBLE
					\thicklines\linethickness{2pt} %DIF PREAMBLE
					{\color[rgb]{1,0,0}\put(0,0){\framebox(1,1){}}}% %DIF PREAMBLE
					{\color[rgb]{1,0,0}\put(0,0){\line( 1,1){1}}}% %DIF PREAMBLE
					{\color[rgb]{1,0,0}\put(0,1){\line(1,-1){1}}}% %DIF PREAMBLE
				\end{picture}% %DIF PREAMBLE
			}\hspace*{3pt}}} %DIF PREAMBLE
} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddbegin}{\DIFaddbegin} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddend}{\DIFaddend} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelbegin}{\DIFdelbegin} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelend}{\DIFdelend} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddbegin}{\DIFOaddbegin \let\includegraphics\DIFaddincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddend}{\DIFOaddend \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelbegin}{\DIFOdelbegin \let\includegraphics\DIFdelincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelend}{\DIFOaddend \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddbeginFL}{\DIFaddbeginFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddendFL}{\DIFaddendFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelbeginFL}{\DIFdelbeginFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelendFL}{\DIFdelendFL} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddbeginFL}{\DIFOaddbeginFL \let\includegraphics\DIFaddincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddendFL}{\DIFOaddendFL \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelbeginFL}{\DIFOdelbeginFL \let\includegraphics\DIFdelincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelendFL}{\DIFOaddendFL \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
%DIF END PREAMBLE EXTENSION ADDED BY LATEXDIFF

\begin{document}
	
	%%
	%% The "title" command has an optional parameter,
	%% allowing the author to define a "short title" to be used in page headers.
	\title{Batching-Efficient RAM using Updatable Lookup Arguments}
	
	%%
	%% The "author" command and its associated commands are used to define
	%% the authors and their affiliations.
	%% Of note is the shared affiliation of the first two authors, and the
	%% "authornote" and "authornotemark" commands
	%% used to denote shared contribution to the research.
	%	\author{Ben Trovato}
	%	\authornote{Both authors contributed equally to this research.}
	%	\email{trovato@corporation.com}
	%	\orcid{1234-5678-9012}
	%	\author{G.K.M. Tobin}
	%	\authornotemark[1]
	%	\email{webmaster@marysville-ohio.com}
	%	\affiliation{%
	%		\institution{Institute for Clarity in Documentation}
	%		\streetaddress{P.O. Box 1212}
	%		\city{Dublin}
	%		\state{Ohio}
	%		\country{USA}
	%		\postcode{43017-6221}
	%	}
	%	
	%	\author{Lars Th{\o}rv{\"a}ld}
	%	\affiliation{%
	%		\institution{The Th{\o}rv{\"a}ld Group}
	%		\streetaddress{1 Th{\o}rv{\"a}ld Circle}
	%		\city{Hekla}
	%		\country{Iceland}}
	%	\email{larst@affiliation.org}
	%	
	%	\author{Valerie B\'eranger}
	%	\affiliation{%
	%		\institution{Inria Paris-Rocquencourt}
	%		\city{Rocquencourt}
	%		\country{France}
	%	}
	%	
	%	\author{Aparna Patel}
	%	\affiliation{%
	%		\institution{Rajiv Gandhi University}
	%		\streetaddress{Rono-Hills}
	%		\city{Doimukh}
	%		\state{Arunachal Pradesh}
	%		\country{India}}
	%	
	%	\author{Huifen Chan}
	%	\affiliation{%
	%		\institution{Tsinghua University}
	%		\streetaddress{30 Shuangqing Rd}
	%		\city{Haidian Qu}
	%		\state{Beijing Shi}
	%		\country{China}}
	%	
	%	\author{Charles Palmer}
	%	\affiliation{%
	%		\institution{Palmer Research Laboratories}
	%		\streetaddress{8600 Datapoint Drive}
	%		\city{San Antonio}
	%		\state{Texas}
	%		\country{USA}
	%		\postcode{78229}}
	%	\email{cpalmer@prl.com}
	%	
	%	\author{John Smith}
	%	\affiliation{%
	%		\institution{The Th{\o}rv{\"a}ld Group}
	%		\streetaddress{1 Th{\o}rv{\"a}ld Circle}
	%		\city{Hekla}
	%		\country{Iceland}}
	%	\email{jsmith@affiliation.org}
	%	
	%	\author{Julius P. Kumquat}
	%	\affiliation{%
	%		\institution{The Kumquat Consortium}
	%		\city{New York}
	%		\country{USA}}
	%	\email{jpkumquat@consortium.net}
	%	
	%%
	%% By default, the full list of authors will be used in the page
	%% headers. Often, this list is too long, and will overlap
	%% other information printed in the page headers. This command allows
	%% the author to define a more concise list
	%% of authors' names for this purpose.
	%	\renewcommand{\shortauthors}{Trovato et al.}
	
	%%
	%% The abstract is a short summary of the work to be presented in the
	%% article.
	\begin{abstract}
		%RAM (random access memory) is a useful primitive in verifiable computation.
		%The RAM primitive allows one to efficiently verify that execution of a program $\mathcal{P}$
		%with read/write access to memory state $\mathsf{st}_0$ results in the final state $\mathsf{st}_1$.
		%The efficiency requires that verification is cheaper than naive execution of $\mathcal{P}$
		%(typically poly-logarithmic) and only requires access to {\em succinct} digests (e.g. Merkle hash) of the state(s).
		%Persistence refers to the fact that state digests allow proving updates to the state across several computations
		%in an incremental fashion.
		
		RAM (random access memory) is an important primitive in verifiable computation.
		In this paper, we focus on realizing RAM with efficient batching property, i.e,
		proving a batch of $m$ updates on a RAM of size $N$ while incurring a cost that is sublinear in $N$.
		Classical approaches based on Merkle-trees or address ordered transcripts to model
		RAM correctness are either concretely inefficient, or incur linear overhead in the size of the RAM.
		Recent works explore cryptographic accumulators based on unknown-order groups (RSA, class-groups)
		to model the RAM state. While recent RSA accumulator based approaches
		offer significant improvement over classical methods,
		they incur linear overhead in the size of the
		accumulated set to compute witnesses, as well as prohibitive constant overheads.
		
		\DIFdelbegin \DIFdel{In this paper, we }\DIFdelend \DIFaddbegin \DIFadd{We }\DIFaddend realize a batching-efficient RAM with superior asymptotic and concrete
		costs as compared to existing approaches. Towards this: (i) we build on recent constructions of lookup arguments to allow efficient
		lookups even in presence of table \textit{updates}, and (ii) we realize a variant of sub-vector relation
		addressed in prior works, which we call {\em committed index lookup}. We combine the two
		building blocks to realize batching-efficient RAM with sublinear ($\sqrt{N}$) dependence on
		size of the RAM. Our construction incurs an amortized proving cost of $O(m\log m + \sqrt{mN})$
		for a batch of $m$ updates on a RAM of size $N$.
		Our results also benefit the recent arguments for sub-vector relation, by
		enabling them to be efficient in presence of updates to the table.
		We believe that this is a contribution of independent interest. 
		
		We implement our solution to evaluate its concrete efficiency.
		Our experiments show that it offers significant improvement over existing works on
		batching-efficient accumulators/RAMs, with a substantially reduced resource barrier.
		
		
	\end{abstract}
	
	%%
	%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
	%% Please copy and paste the code instead of the example below.
	%%
	%	\begin{CCSXML}
	%		<ccs2012>
	%		<concept>
	%		<concept_id>00000000.0000000.0000000</concept_id>
	%		<concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
	%		<concept_significance>500</concept_significance>
	%		</concept>
	%		<concept>
	%		<concept_id>00000000.00000000.00000000</concept_id>
	%		<concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
	%		<concept_significance>300</concept_significance>
	%		</concept>
	%		<concept>
	%		<concept_id>00000000.00000000.00000000</concept_id>
	%		<concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
	%		<concept_significance>100</concept_significance>
	%		</concept>
	%		<concept>
	%		<concept_id>00000000.00000000.00000000</concept_id>
	%		<concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
	%		<concept_significance>100</concept_significance>
	%		</concept>
	%		</ccs2012>
	%	\end{CCSXML}
	
	%	\ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
	%	\ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
	%	\ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
	%	\ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
	%	
	%%
	%% Keywords. The author(s) should pick words that accurately describe
	%% the work being presented. Separate the keywords with commas.
	\keywords{Succinct Arguments, Efficient RAM, Indexed lookups, Accumulators, Rollups}
	%% A "teaser" image appears between the author and affiliation
	%% information and the body of the document, and typically spans the
	%% page.
	
	
	%	\received{20 February 2007}
	%	\received[revised]{12 March 2009}
	%	\received[accepted]{5 June 2009}
	
	%%
	%% This command processes the author and affiliation and title
	%% information and builds the first part of the formatted document.
	\maketitle
	
	\thispagestyle{plain}
	
	\pagestyle{plain}
	
	\section{Introduction}\label{sec:introduction}
	\DIFaddbegin 
	
	%DIF > 	\input{introduction}
	
	\DIFaddend General purpose {\em Succinct Non-interactive Arguments of Knowledge} (SNARKs) enable one to generate succinct
	proofs of membership of a statement in an $\npol$ relation expressed as an arithmetic circuit. These proofs are
	extremely cheap to verify, which makes them useful for {\em Verifiable Computation} (VC), where a \DIFdelbegin \DIFdel{low-powered
	}\DIFdelend \DIFaddbegin \DIFadd{resource-constrained
	}\DIFaddend client (e.g.\DIFaddbegin \DIFadd{, a }\DIFaddend mobile phone), can outsource an expensive computation to an untrusted server, and later
	verify the correctness of the \DIFdelbegin \DIFdel{results }\DIFdelend \DIFaddbegin \DIFadd{computation }\DIFaddend at a minimal cost.
	\DIFdelbegin \DIFdel{However,
		arithmetic circuit based }\DIFdelend \DIFaddbegin 
	
	\smallskip
	
	\noindent{\bf Modeling RAM in Verifiable Computation.} \DIFadd{It turns out that arithmetic circuit-based }\DIFaddend representations are inefficient in expressing relations involving the result of a program execution on memory/state\DIFdelbegin \DIFdel{, which }\DIFdelend \DIFaddbegin \DIFadd{. Such relations }\DIFaddend frequently arise in the context of verifiable computation\DIFdelbegin \DIFdel{.
		These include scenarios such as }\DIFdelend \DIFaddbegin \DIFadd{, in scenarios that require }\DIFaddend proving the correctness of \DIFdelbegin \DIFdel{a }\DIFdelend query execution against a database, \DIFdelbegin \DIFdel{proving the correctness of }\DIFdelend inference from a decision tree, or \DIFdelbegin \DIFdel{proving correct update of }\DIFdelend \DIFaddbegin \DIFadd{updates on a }\DIFaddend table of account balances\DIFaddbegin \DIFadd{~(e.g., }\DIFaddend when a batch of transactions\DIFdelbegin \DIFdel{(e.g. transfers)
		are applied to it.
		In }\DIFdelend \DIFaddbegin \DIFadd{, such as account transfers, is applied to the table).
	}
	
	\DIFadd{In the }\DIFaddend aforementioned examples, \DIFaddbegin \DIFadd{objects such as }\DIFaddend database tables, decision trees\DIFdelbegin \DIFdel{and accounts table are }\DIFdelend \DIFaddbegin \DIFadd{, and accounts tables can be }\DIFaddend naturally
	modelled as \DIFdelbegin \DIFdel{addressable memory and }\DIFdelend \DIFaddbegin \DIFadd{instances of addressable memory, or more generally, random access memory~(RAM), where }\DIFaddend one needs to prove that \DIFdelbegin \DIFdel{it is }\DIFdelend \DIFaddbegin \DIFadd{the RAM has been }\DIFaddend accessed/updated in accordance with the correct execution of the computation. \DIFdelbegin \DIFdel{Accordingly, there is }\DIFdelend \DIFaddbegin \DIFadd{There exists }\DIFaddend a rich and expanding body of work \DIFdelbegin \DIFdel{to efficiently model the abstraction of addressable memory in verifiable computations. While complete acknowledgement }\DIFdelend \DIFaddbegin \DIFadd{on efficiently modeling abstractions of RAM in verifiable computation. While a complete treatment }\DIFaddend of this vast body of work is beyond
	the scope of this paper (a fairly recent survey in \cite{WB15} is a good starting point)\DIFdelbegin \DIFdel{we summarise key approaches towards
		modelling RAM in verifiable computation. Often we demand additional properties }\DIFdelend \DIFaddbegin \DIFadd{, we mention two additional properties that are often demanded }\DIFaddend of the RAM primitive\DIFdelbegin \DIFdel{, such as
	}\DIFdelend \DIFaddbegin \DIFadd{: }\DIFaddend {\em persistence} \DIFdelbegin \DIFdel{- which is }\DIFdelend \DIFaddbegin \DIFadd{-- }\DIFaddend the ability to persist the RAM state across several computations, and {\em batching} \DIFdelbegin \DIFdel{-
	}\DIFdelend \DIFaddbegin \DIFadd{--
	}\DIFaddend where verifiable update of the RAM state is required for small batches of updates. These properties are also the focus of this work.
	
	%DIF > summarise key approaches towards
	%DIF > modelling RAM in verifiable computation. Often we demand additional properties of the RAM primitive, such as
	%DIF > {\em persistence} - which is the ability to persist the RAM state across several computations, and {\em batching} -
	%DIF > where verifiable update of the RAM state is required for small batches of updates. 
	\DIFaddbegin 
	
	\smallskip
	
	\noindent{\bf Application to Blockchain Rollups.} \DIFaddend Batching-efficient RAM \DIFdelbegin \DIFdel{primitive }\DIFdelend is especially relevant in the context of blockchain {\em rollups} ~\cite{rollup},
	an umbrella term for recent efforts to scale blockchains by moving expensive computation off the blockchain to the so-called {\em layer two}\DIFaddbegin \DIFadd{~(}\DIFaddend or L2\DIFaddbegin \DIFadd{) }\DIFaddend chains. The blockchain only needs to verify \DIFdelbegin \DIFdel{the }\DIFdelend succinct proofs attesting to the correctness of the off-chain computation. This approach is popularly called \DIFdelbegin \DIFdel{rollup }\DIFdelend \DIFaddbegin \textit{\DIFadd{rollup}} \DIFaddend as it allows verifying the result of several (rolled-up) transactions modifying the L2 state, as part of one transaction verified on the main chain.
	This simultaneously \DIFdelbegin \DIFdel{helps the }\DIFdelend \DIFaddbegin \DIFadd{improves }\DIFaddend scalability and lowers the cost (e.g.\DIFaddbegin \DIFadd{, }\DIFaddend gas fees) per transaction due to succinct verification. We consider improving efficiency of rollups an important motivation for our work, \DIFdelbegin \DIFdel{though in this work we do not go into }\DIFdelend \DIFaddbegin \DIFadd{but avoid }\DIFaddend precise details of a smart-contract based
	instantiation of our solution.
	
	\subsection{Our Contribution}\label{subsec:ourwork} 
	We present batching-efficient RAM construction, which advances the efforts
	towards achieving {\em verifiable outsourcing of state update} such as in ~\cite{EPRINT:BFRSBW13}
	and more recently in ~\cite{USENIX:OWWB20, CCS:CFHKKO22}.
	The most popular approaches to succinctly represent
	state involve accumulators based on Merkle-trees ~\cite{C:Merkle87}, or ones based on groups of unknown order
	(e.g. RSA, class-groups) ~\cite{C:CamLys02,C:BonBunFis19,USENIX:OWWB20, CCS:CFHKKO22}.
	The updates to the state are effected by insertions or deletions in the  accumulated set.
	\DIFdelbegin \DIFdel{By contrast, in this work}\DIFdelend \DIFaddbegin \DIFadd{In this work, }\DIFaddend we
	model the state as an addressable memory (RAM) described by vector $\vecT$, which stores \DIFdelbegin \DIFdel{a }\DIFdelend value $v_i$ at address $i$.
	We denote this as $\vecT[\,i\,]=v_i$. The RAM supports two operations, viz, {\em loads} expressed
	as $v_i := \vecT[\,i\,]$, and {\em stores} expressed as $\vecT[\,i\,]=v_i$.
	%denoted by the tuple $(\load, i, v_i)$ and (ii) {\em indexed update} ($\vecT[\,i\,] := v_i$), denoted
	%by the tuple $(\store, i, v_i)$.
	We think of addresses $i\in [0,N]$ for some $N\in \mathbb{Z}$ while the
	values $v_i\in \F$ for some finite field $\F$. In our construction, we represent both the RAM and operations on it
	as polynomials, and use appropriate polynomial commitment schemes to obtain succinct commitments (digests) to them.
	In this paper, we do not require commitments to be {\em hiding}, as our focus is on succinctness.
	We consider privacy as an orthogonal goal, one we believe is easily achievable
	via small adaptations to our construction.\smallskip
	
	
	\DIFdelbegin %DIFDELCMD < \begin{comment}%DIFDELCMD < 
	%DIFDELCMD < \noindent{\bf Application to Blockchain Rollups}: We consider the application of persistent RAM primitive to a common rollup scenario.
	%DIFDELCMD < Our application is a simplified adaptation of rollup protocols such as ZkSync ~\cite{ZkSync}.
	%DIFDELCMD < We assume a layer two (L2) chain \textsf{DemoChain}, which issues its clients \textsf{DemoCoins}. The clients have accounts on
	%DIFDELCMD < L2, and they transfer \textsf{DemoCoins} to each other using L2 transactions. Such a system will also have mechanism to transfer
	%DIFDELCMD < funds to and from main-chain (e.g. Ethereum) accounts, but we
	%DIFDELCMD < ignore those details here.
	%DIFDELCMD < Each client is assigned a unique identifier $i$, and associated account information  is maintained as
	%DIFDELCMD < a tuple $(\pk_i,\bal_i,\txno_i)$ which refer respectively to the public key for verifying client's signature on transactions,
	%DIFDELCMD < account balance (number of coins owned by the client) and total number of transactions made from the account (to prevent replay attacks).
	%DIFDELCMD < The L2 state thus consists of the above information for all accounts. We model an L2 chain supporting upto $N$ accounts as a RAM $\vecT$
	%DIFDELCMD < of size $N$, where $\vecT[\,i\,]=(\pk_i,\bal_i,\txno_i)$ denotes client $i$'s account information. A transfer transaction of amount $v$
	%DIFDELCMD < from client $i$ to client $j$ involves two load operations, and two store operations on the RAM (for reading and updating the referenced
	%DIFDELCMD < accounts). These transactions are submitted by clients directly to a designated L2 chain operator \textsf{DemoOperator}, who batches $m$
	%DIFDELCMD < such transactions and updates the RAM state accordingly. The operator maintains the entire state off-chain and locally computes the updates for
	%DIFDELCMD < each batch of transactions. Only the succinct digest of the state (polynomial commitments) is stored on the main chain, and the proof of
	%DIFDELCMD < validity of the state update is verified by a main-chain transaction. In later sections, we provide more details on the application and the
	%DIFDELCMD < performance achieved when instantiated using our primitive.
	%DIFDELCMD < \end{comment}
	%DIFDELCMD < %%%
	\DIFdelend \DIFaddbegin \begin{comment}
	\noindent{\bf Application to Blockchain Rollups}: We consider the application of persistent RAM primitive to a common rollup scenario.
	Our application is a simplified adaptation of rollup protocols such as ZkSync ~\cite{ZkSync}.
	We assume a layer two (L2) chain \textsf{DemoChain}, which issues its clients \textsf{DemoCoins}. The clients have accounts on
	L2, and they transfer \textsf{DemoCoins} to each other using L2 transactions. Such a system will also have mechanism to transfer
	funds to and from main-chain (e.g. Ethereum) accounts, but we
	ignore those details here.
	Each client is assigned a unique identifier $i$, and associated account information  is maintained as
	a tuple $(\pk_i,\bal_i,\txno_i)$ which refer respectively to the public key for verifying client's signature on transactions,
	account balance (number of coins owned by the client) and total number of transactions made from the account (to prevent replay attacks).
	The L2 state thus consists of the above information for all accounts. We model an L2 chain supporting upto $N$ accounts as a RAM $\vecT$
	of size $N$, where $\vecT[\,i\,]=(\pk_i,\bal_i,\txno_i)$ denotes client $i$'s account information. A transfer transaction of amount $v$
	from client $i$ to client $j$ involves two load operations, and two store operations on the RAM (for reading and updating the referenced
	accounts). These transactions are submitted by clients directly to a designated L2 chain operator \textsf{DemoOperator}, who batches $m$
	such transactions and updates the RAM state accordingly. The operator maintains the entire state off-chain and locally computes the updates for
	each batch of transactions. Only the succinct digest of the state (polynomial commitments) is stored on the main chain, and the proof of
	validity of the state update is verified by a main-chain transaction. In later sections, we provide more details on the application and the
	performance achieved when instantiated using our primitive.
	\end{comment}
	\DIFaddend 
	
	We summarize our contributions below.
	\begin{itemize}[leftmargin=2em]
		\item As our first contribution, we propose {\em update friendly lookup arguments}, which addresses
		the strict dependence of recent constructions on table-specific \DIFdelbegin \DIFdel{pre-computed }\DIFdelend \DIFaddbegin \DIFadd{pre-processing }\DIFaddend parameters. Our
		innovation extends the utility of table-specific parameters to enable efficient lookups from tables,
		which are within certain \DIFdelbegin \DIFdel{hamming }\DIFdelend \DIFaddbegin \DIFadd{Hamming }\DIFaddend distance of the pre-processed table.
		\item We construct {\em committed index lookup} arguments via black-box reduction to
		sub-vector arguments that use homomorphic commitments. A committed index lookup involves
		three committed vectors $\vec{t},\vec{a}$ and $\vec{v}$ satisfying $v_i=t_{a_i}$ for all $i$. Similar
		definition is also used in recent multi-variate lookup arguments in ~\cite{lasso}, where a similar reduction
		to sub-vector arguments is obtained under a more restrictive assumption about the elements of the table.
		\item We crucially employ \DIFaddbegin \DIFadd{the }\DIFaddend above two contributions to construct a {\em batching-efficient} RAM, which
		can \DIFdelbegin \DIFdel{verifiably }\DIFdelend prove a batch of $m$ updates with an amortized prover complexity of $O(m\log m + \sqrt{mN})$,
		with $N$ being the size of the RAM. Our dependence on the RAM size is \DIFdelbegin \DIFdel{sub-linear}\DIFdelend \DIFaddbegin \DIFadd{sublinear}\DIFaddend , in contrast to \DIFaddbegin \DIFadd{the }\DIFaddend linear complexity
		inherent in recent works on batching-efficient RAM using RSA accumulators ~\cite{USENIX:OWWB20,CCS:CFHKKO22} \DIFaddbegin \DIFadd{or using generic memory checking techniques~\mbox{%DIFAUXCMD
				\cite{NDSS:WSRBW15,USENIX:BCTV14,C:BCGTV13,SP:ZGKPP18}}\hskip0pt%DIFAUXCMD
		}\DIFaddend . All of our protocols are public-coin, and can be made non-interactive using standard techniques~\cite{C:FiaSha86}. 
		
		\item We implement our scheme in Rust\footnote{\url{https://anonymous.4open.science/r/updatableRAM/}}.
		Experimentally, we show that our scheme performs significantly better than \DIFdelbegin \DIFdel{the aforementioned }\DIFdelend prior works,
		and is eminently deployable on a commodity hardware.
	\end{itemize}
	
	
	\subsection{Techniques}\label{subsec:techniques}
	\DIFdelbegin %DIFDELCMD < \noindent{\bf Update-friendly Lookup Arguments}%%%
	\DIFdel{: Starting point for our work are the recent }\DIFdelend \DIFaddbegin 
	
	\DIFadd{We present a brief summary of our techniques below. A more detailed technical overview appears in Section~\ref{sec:tech-overview}.
	}
	
	\smallskip
	
	\noindent{\bf Update-friendly Lookup Arguments.} \DIFadd{Our starting point is the recent line of works on }\DIFaddend lookup arguments which prove
	that a vector of size $m$ appears as
	a sub-vector in a large fixed vector (table) of size $N$ with succinct proof sizes and verification, but most notably
	ensuring that prover runs in time \DIFdelbegin \DIFdel{sub-linear }\DIFdelend \DIFaddbegin \DIFadd{sublinear }\DIFaddend in the size of the table ($N$). The pioneering work ~\cite{CCS:ZBKMNS22}
	obtained prover complexity of $O(m^2+m\log N)$, which was improved in subsequent works to $O(m^2)$ ~\cite{EPRINT:PosKat22},
	$O(m\log^2 m)$ ~\cite{EPRINT:ZGKMR22}, and $O(m\log m)$ ~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
			\cite{EPRINT:EagFioGab22}}\hskip0pt%DIFAUXCMD
	}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
			\cite{EPRINT:EagFioGab22,PKC:CFFLL24}}\hskip0pt%DIFAUXCMD
	}\DIFaddend . However, the \DIFdelbegin \DIFdel{sub-linear }\DIFdelend \DIFaddbegin \DIFadd{sublinear }\DIFaddend prover
	complexity requires table-dependent $O(N\log N)$ pre-processing and $O(N)$ storage. This table-dependent
	pre-processing implies that while
	the aforementioned lookup arguments can be used to obtain efficient ROM (read only memory) semantics
	%\footnote{The protocols for sub-vector relation in ~\cite{CCS:ZBKMNS22, EPRINT:ZGKMR22} also implicitly support indexed lookup semantics},
	they cannot be used as is for RAM (which supports update operations).
	Moreover, an update involving even a single
	index renders the entire $O(N)$ pre-processing unusable for further lookups,
	thus necessitating entire $O(N\log N)$ re-computation. This work is the first effort towards
	mitigating this rigid dependence, thereby increasing the applicability of the recent lookup arguments.
	An important contribution we make here is a new method for computing ``encoded quotients'' used in several
	recent lookup constructions such as~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
			\cite{CCS:ZBKMNS22,EPRINT:PosKat22,EPRINT:EagFioGab22}}\hskip0pt%DIFAUXCMD
	}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
			\cite{CCS:ZBKMNS22,EPRINT:PosKat22,EPRINT:EagFioGab22,PKC:CFFLL24}}\hskip0pt%DIFAUXCMD
	}\DIFaddend .
	Our approach for computing these quotients from pre-computed parameters remains efficient even when
	the table is updated, and it directly applies to all the aforementioned constructions.
	For a table $\delta$-hamming distance away from the pre-processed one, we incur
	$(m+\delta)\log^2(m+\delta)$ additional overhead for proving $m$ lookups. \DIFdelbegin \DIFdel{This additive and }\DIFdelend \DIFaddbegin \DIFadd{To achieve such a }\DIFaddend quasi-linear overhead in \DIFaddbegin \DIFadd{both $m$ and }\DIFaddend $\delta$\DIFdelbegin \DIFdel{rests }\DIFdelend \DIFaddbegin \DIFadd{, we rely }\DIFaddend on novel algebraic algorithms described in Section~\ref{sec:update-protocol}.
	We informally \DIFdelbegin \DIFdel{summarise }\DIFdelend \DIFaddbegin \DIFadd{summarize }\DIFaddend our contribution in this regard below, whereas Theorem~\ref{thm:approx-setup}
	states the precise result.
	\begin{theorem}[Informal]\label{thm:pre-process}
		There exists a deterministic $O(N\log N)$ time algorithm $\textsf{Preprocess}(\vecT)\outp \pp_T$
		which on input $\vecT\in \F^N$, outputs parameters $\pp_T$ of size $O(N)$ such
		that: Given $\pp_T$, vectors $\vecT'\in \F^N$, $\vec{t}\in \F^m$ with $\vec{t}$ being a sub-vector of $\vecT'$
		an argument of knowledge for the same can be computed in time
		$O((m+\delta)\log^2 (m+\delta) + f(m))$ where $\delta=\Delta(\vecT, \vecT')$
		is the \DIFdelbegin \DIFdel{hamming distance of }\DIFdelend \DIFaddbegin \DIFadd{Hamming distance between }\DIFaddend $\vecT$ and $\vecT'$ while $f(m)$ depends on the specific lookup protocol.
	\end{theorem}
	For the constructions based on~\cite{CCS:ZBKMNS22,EPRINT:PosKat22}, we set $f(m)=m^2$ in the above,
	while for~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
			\cite{EPRINT:EagFioGab22}}\hskip0pt%DIFAUXCMD
	}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
			\cite{EPRINT:EagFioGab22,PKC:CFFLL24}}\hskip0pt%DIFAUXCMD
	}\DIFaddend , we have $f(m)=m\log m$.
	\DIFaddbegin 
	
	\DIFaddend \smallskip
	
	\noindent{\bf Committed Index Lookup}: We augment the sub-vector relation in prior lookup arguments which
	considers whether each entry of a given vector appears in the target vector to one that also identifies
	the precise positions where the given vector appears in the target vector. When this relation is checked
	over commitments of the respective vectors; given vector, the target vector and the position vector, we call
	it {\em committed index lookup}. The relation we consider is similar to the one considered in ~\cite{lasso}.
	For lookup arguments with homomorphic commitment \DIFdelbegin \DIFdel{scheme}\DIFdelend \DIFaddbegin \DIFadd{schemes}\DIFaddend , we show that committed index lookup can be obtained
	using \DIFdelbegin \DIFdel{three instantiations of }\DIFdelend \DIFaddbegin \DIFadd{a }\DIFaddend sub-vector lookup \DIFaddbegin \DIFadd{argument }\DIFaddend (Lemma~\ref{lem:generic-transformation}\DIFaddbegin \DIFadd{, Section~\ref{subsec:committed-index-lookup}}\DIFaddend ). Such \DIFdelbegin \DIFdel{reduction }\DIFdelend \DIFaddbegin \DIFadd{a construction }\DIFaddend was
	also considered in~\cite{lasso}, but under a more restrictive assumption \DIFdelbegin \DIFdel{on the }\DIFdelend \DIFaddbegin \DIFadd{that the size of the }\DIFaddend elements in the table \DIFdelbegin \DIFdel{being
	}\DIFdelend \DIFaddbegin \DIFadd{have to be
	}\DIFaddend within a certain bound. \DIFdelbegin \DIFdel{The reduction readily }\DIFdelend \DIFaddbegin \DIFadd{Lemma~\ref{lem:generic-transformation} yields a construction of committed index lookup that uses (a single instance of) the underlying sub-vector protocol in a black-box manner. This immediately }\DIFaddend implies
	efficient constructions of arguments for committed index lookups from
	~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
			\cite{CCS:ZBKMNS22,EPRINT:PosKat22,EPRINT:ZGKMR22,EPRINT:EagFioGab22}}\hskip0pt%DIFAUXCMD
		. In the case of ~\mbox{%DIFAUXCMD
			\cite{EPRINT:PosKat22}}\hskip0pt%DIFAUXCMD
		, we give an explicit adaptation in
		Section ~\ref{subsec:committed-index-lookup} that }\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
			\cite{CCS:ZBKMNS22,EPRINT:PosKat22,EPRINT:ZGKMR22,EPRINT:EagFioGab22,PKC:CFFLL24}}\hskip0pt%DIFAUXCMD
		. In Appendix~\ref{app:committed-index-lookup}, we also present an explicit (non-black-box) adaptation of~\mbox{%DIFAUXCMD
			\cite{EPRINT:PosKat22} }\hskip0pt%DIFAUXCMD
		to obtain a committed index lookup, which again }\DIFaddend incurs costs comparable to \DIFaddbegin \DIFadd{a }\DIFaddend single instance of the underlying sub-vector protocol.
	\DIFaddbegin 
	
	\DIFaddend \smallskip
	
	
	\DIFdelbegin %DIFDELCMD < \noindent{\bf Batching-Efficient RAM from Lookup Arguments}%%%
	\DIFdel{:
	}\DIFdelend \DIFaddbegin \noindent{\bf Batching-Efficient RAM from Lookup Arguments.}
	\DIFaddend Memory checking methods based on {\em address ordered transcripts}
	~\cite{NDSS:WSRBW15,USENIX:BCTV14,C:BCGTV13,SP:ZGKPP18}\DIFaddbegin \DIFadd{, }\DIFaddend which are popularly used in
	efficient RAM abstractions\DIFaddbegin \DIFadd{, }\DIFaddend incur a cost linear in the size of the RAM\DIFdelbegin \DIFdel{, which is prohibitive when
		aiming }\DIFdelend \DIFaddbegin \DIFadd{. This is prohibitive }\DIFaddend for efficient batching. As a key idea in this work, we invoke \DIFdelbegin \DIFdel{``}\DIFdelend committed index lookup \DIFdelbegin \DIFdel{'' }\DIFdelend on
	the large RAMs, to verifiably extract smaller \DIFdelbegin \DIFdel{RAMs}\DIFdelend \DIFaddbegin \DIFadd{sub-RAMs}\DIFaddend , which correspond to indices actually involved in
	the batch update. Then\DIFdelbegin \DIFdel{we can }\DIFdelend \DIFaddbegin \DIFadd{, we }\DIFaddend use the linear time memory-checking techniques to argue the consistency of these
	smaller \DIFdelbegin \DIFdel{RAMs. 
	}\DIFdelend \DIFaddbegin \DIFadd{sub-RAMs. 
	}
	
	\DIFaddend The idea needs to work through some more \DIFdelbegin \DIFdel{issues; }\DIFdelend \DIFaddbegin \DIFadd{details, }\DIFaddend such as showing that the larger RAMs are identical
	on positions not referenced by the batch of updates (considered in Section ~\ref{subsec:proximity-ram}).
	The overall idea is illustrated in Figure ~\ref{fig:blueprint}. We also note that the extracted sub-RAMs can
	have duplicate records, corresponding to multiple updates referencing the same RAM index\DIFdelbegin \DIFdel{, }\DIFdelend \DIFaddbegin \DIFadd{; }\DIFaddend however, memory
	checking methods can \DIFdelbegin \DIFdel{easily }\DIFdelend \DIFaddbegin \DIFadd{be easily adapted to }\DIFaddend handle such cases. Finally, we would still hit the ``rigidity'' of lookup arguments
	in realizing this plan; once the table has changed, lookups are no longer efficient from it. \DIFdelbegin \DIFdel{Fortunately,
		we can }\DIFdelend \DIFaddbegin \DIFadd{To circumvent this,
		we }\DIFaddend use our first contribution on extending the utility of table-specific parameters to defer parameter
	re-computation optimally while still availing efficient lookups. More specifically, if we choose to
	re-compute the full table-specific parameters after $k$ batches (of $m$ updates each),
	the average cost per batch is $O(N\log N/k + mk\log^2(mk) + f(m))$. Here, $f(m)$ as earlier denotes complexity
	of the non-updatable base protocol. Setting $k\approx \sqrt{N/m}$ yields the average cost of $m$ updates as \DIFdelbegin \DIFdel{$O(f(m)+\sqrt{mN})$}\DIFdelend \DIFaddbegin \DIFadd{$\wt{O}(f(m)+\sqrt{mN})$}\DIFaddend ,
	which scales \DIFdelbegin \DIFdel{sub-linearly }\DIFdelend \DIFaddbegin \DIFadd{sublinearly }\DIFaddend with the size of the RAM.
	While the preceding analysis considers the worst case,
	in specific applications (such as account transactions, where few accounts contribute a large volume of transactions), it may be
	possible to further delay the computation of table-specific parameters.
	Thus we have:
	\begin{theorem}[Informal]\label{thm:inc-ver-ram-informal}
		Given $m,N\in \mathbb{N}$, there exists an argument for verifiable RAM which proves updates of batch size $m$ on RAM of size $N$
		with amortized prover complexity of \DIFdelbegin \DIFdel{$O(f(m) + \sqrt{mN})$}\DIFdelend \\ \DIFaddbegin \DIFadd{$\wt{O}(f(m) + \sqrt{mN})$}\DIFaddend .
	\end{theorem}
	
	
	\DIFdelbegin %DIFDELCMD < \noindent{\bf Polynomial Protocol for RAM}%%%
	\DIFdel{: }\DIFdelend \DIFaddbegin \noindent{\bf Polynomial Protocol for RAM.} \DIFaddend There are several ways to implement the ordered transcript based
	memory consistency check on the smaller $O(m)$-sized RAMs, for example by expressing the same as an arithmetic
	circuit. However, for completeness\DIFaddbegin \DIFadd{, }\DIFaddend we also present an argument for RAM \DIFdelbegin \DIFdel{using the above techniques }\DIFdelend as an
	interactive {\em polynomial protocol} ~\cite{Gabizon2019PLONKPO}, which is then compiled into an argument of knowledge using \DIFaddbegin \DIFadd{the }\DIFaddend $\kzg$ ~\cite{AC:KatZavGol10}
	commitment scheme in the algebraic group model (AGM) ~\cite{C:FucKilLos18}. This construction appears in
	Appendix ~\ref{sec:poly-proto-ram-app}.
	\DIFdelbegin \DIFdel{
	}\DIFdelend %DIF > \moumita{Making all references to Sections in Appendix, as Appendix B instead of Section B.}
	
	\DIFdelbegin %DIFDELCMD < \begin{comment}%DIFDELCMD < 
	%DIFDELCMD < Most of the efficient implementations of RAM in verifiable computation ~\cite{C:BCGTV13, NDSS:WSRBW15, SP:ZGKPP18}
	%DIFDELCMD < rely on the {\em address-ordered transcript} to check that a sequence of $\load$s and $\store$s are {\em consistent} with some initial state
	%DIFDELCMD < of the RAM. Using the tuple $(t,\op,\ind, v)$ to denote a RAM instruction with $t$ being the execution {\em timestamp}, $\op\in \{\load,\store\}$ being
	%DIFDELCMD < the operation type, $\ind$ being the index referenced and $v$ denoting the value read/stored, a time ordered execution transcript $\mathsf{tr} = (\ins_1,\ldots,\ins_m)$ is a sequence
	%DIFDELCMD < of instructions where $\ins_i=(t_i,\op_i,\ind_i, v_i)$ and $t_i < t_{i+1}$ for all $1\leq i<m$. The transcript $\mathsf{tr}$ is said to be consistent if
	%DIFDELCMD < for every $\load$ instruction $\ins=(t,\load,\ind,v)$, the value $v$ is the same as that for the most recent $\store$ instruction, which references the same index
	%DIFDELCMD < as $\ins$. This consistency can be efficiently checked by permuting the transcript $\mathsf{tr}$ to produce the sequence
	%DIFDELCMD < $\mathsf{tr}^\ast=(\ins_1^\ast,\ldots,\ins_m^\ast)$ with $\ins_i^\ast=(t_i^\ast,\op_i^\ast,\ind_i^\ast,v_i^\ast)$ which is sorted by index, with
	%DIFDELCMD < timestamp used to break the ties, i.e, $\ind_i^\ast\leq \ind_{i+1}^\ast$ and whenever $\ind_i^\ast=\ind_{i+1}^\ast$, we have $t_i^\ast < t_{i+1}^\ast$.
	%DIFDELCMD < We call the resulting transcript $\mathsf{tr}^\ast$ as address ordered transcript. On such a transcript, the consistency is enforced simply by
	%DIFDELCMD < checking that each $\load$ instruction involves the same value $v$ as the previous instruction, provided the referenced indices are the same.
	%DIFDELCMD < The above approach is easily extended to verify that result of executing a sequence of operations $\{(\op_i,\ind_i,v_i)\}_{i=1}^m$ on
	%DIFDELCMD < RAM $\vecT_0\in \F^n$ is $\vecT_1\in F^n$. In this case, we define a transcript $\mathsf{tr}=(\ins_1,\ldots,\ins_{2n+m})$ where the
	%DIFDELCMD < first $n$ instructions $\ins_i,i\in [n]$ are defined as $\ins_i=(i,\store,i,\vecT_0[\,i\,])$, the next $m$ instructions are defined
	%DIFDELCMD < as $\ins_{n+i},i\in [m]$ as $\ins_{n+i}=(n+i, \op_i, \ind_i, v_i)$ and the last $n$ instructions $\ins_{n+m+i}, i\in [n]$ as
	%DIFDELCMD < $\ins_{n+m+i}=(n+m+i,\load,i,\vecT_1[\,i\,])$. The consistency of the transcript $\mathsf{tr}$ is checked via address ordering permutation as
	%DIFDELCMD < described before. Intuitively, the first $n$ instructions in $\mathsf{tr}$ copy the initial contents of the RAM, the next $m$ instructions essentially
	%DIFDELCMD < capture the sequence of RAM operations executed on the initial RAM, while the final $n$ instructions read out the entire contents of the RAM which
	%DIFDELCMD < is expected to match the contents of $\vecT_1[\,i\,]$. We illustrate this approach in Figure \ref{fig:permutated-transcripts}. One of the contributions of this work is a polynomial protocol that proves that RAM state
	%DIFDELCMD < $\vecT_1\in \F^n$ is obtained from RAM state $\vecT_0\in \F^n$ as a result of sequence of operations $\vec{\op}=\{(\op_i,\ind_i,v_i)\}_{i=1}^m$, where each
	%DIFDELCMD < of the vectors $\vecT_0,\vecT_1$ and $\vec{\op}$ are represented via polynomials. Informally, we show:
	%DIFDELCMD < \begin{theorem}[informal]\label{thm:pp-for-ram-informal}
	%DIFDELCMD < 	For $m,n\in \mathbb{N}$, there exists a polynomial protocol to prove that sequence of operations $\vec{\op}=\{(\op_i,\ind_i,v_i)\}_{i=1}^m$ updates a
	%DIFDELCMD < 	RAM state $\vecT_0\in \F^n$ to RAM state $\vecT_1\in \F^n$ with the prover-complexity of $\tilde{O}(m+n)$.
	%DIFDELCMD < \end{theorem}
	%DIFDELCMD < \end{comment}
	%DIFDELCMD < %%%
	\DIFdelend \DIFaddbegin \begin{comment}
	Most of the efficient implementations of RAM in verifiable computation ~\cite{C:BCGTV13, NDSS:WSRBW15, SP:ZGKPP18}
	rely on the {\em address-ordered transcript} to check that a sequence of $\load$s and $\store$s are {\em consistent} with some initial state
	of the RAM. Using the tuple $(t,\op,\ind, v)$ to denote a RAM instruction with $t$ being the execution {\em timestamp}, $\op\in \{\load,\store\}$ being
	the operation type, $\ind$ being the index referenced and $v$ denoting the value read/stored, a time ordered execution transcript $\mathsf{tr} = (\ins_1,\ldots,\ins_m)$ is a sequence
	of instructions where $\ins_i=(t_i,\op_i,\ind_i, v_i)$ and $t_i < t_{i+1}$ for all $1\leq i<m$. The transcript $\mathsf{tr}$ is said to be consistent if
	for every $\load$ instruction $\ins=(t,\load,\ind,v)$, the value $v$ is the same as that for the most recent $\store$ instruction, which references the same index
	as $\ins$. This consistency can be efficiently checked by permuting the transcript $\mathsf{tr}$ to produce the sequence
	$\mathsf{tr}^\ast=(\ins_1^\ast,\ldots,\ins_m^\ast)$ with $\ins_i^\ast=(t_i^\ast,\op_i^\ast,\ind_i^\ast,v_i^\ast)$ which is sorted by index, with
	timestamp used to break the ties, i.e, $\ind_i^\ast\leq \ind_{i+1}^\ast$ and whenever $\ind_i^\ast=\ind_{i+1}^\ast$, we have $t_i^\ast < t_{i+1}^\ast$.
	We call the resulting transcript $\mathsf{tr}^\ast$ as address ordered transcript. On such a transcript, the consistency is enforced simply by
	checking that each $\load$ instruction involves the same value $v$ as the previous instruction, provided the referenced indices are the same.
	The above approach is easily extended to verify that result of executing a sequence of operations $\{(\op_i,\ind_i,v_i)\}_{i=1}^m$ on
	RAM $\vecT_0\in \F^n$ is $\vecT_1\in F^n$. In this case, we define a transcript $\mathsf{tr}=(\ins_1,\ldots,\ins_{2n+m})$ where the
	first $n$ instructions $\ins_i,i\in [n]$ are defined as $\ins_i=(i,\store,i,\vecT_0[\,i\,])$, the next $m$ instructions are defined
	as $\ins_{n+i},i\in [m]$ as $\ins_{n+i}=(n+i, \op_i, \ind_i, v_i)$ and the last $n$ instructions $\ins_{n+m+i}, i\in [n]$ as
	$\ins_{n+m+i}=(n+m+i,\load,i,\vecT_1[\,i\,])$. The consistency of the transcript $\mathsf{tr}$ is checked via address ordering permutation as
	described before. Intuitively, the first $n$ instructions in $\mathsf{tr}$ copy the initial contents of the RAM, the next $m$ instructions essentially
	capture the sequence of RAM operations executed on the initial RAM, while the final $n$ instructions read out the entire contents of the RAM which
	is expected to match the contents of $\vecT_1[\,i\,]$. We illustrate this approach in Figure \ref{fig:permutated-transcripts}. One of the contributions of this work is a polynomial protocol that proves that RAM state
	$\vecT_1\in \F^n$ is obtained from RAM state $\vecT_0\in \F^n$ as a result of sequence of operations $\vec{\op}=\{(\op_i,\ind_i,v_i)\}_{i=1}^m$, where each
	of the vectors $\vecT_0,\vecT_1$ and $\vec{\op}$ are represented via polynomials. Informally, we show:
	\begin{theorem}[informal]\label{thm:pp-for-ram-informal}
	For $m,n\in \mathbb{N}$, there exists a polynomial protocol to prove that sequence of operations $\vec{\op}=\{(\op_i,\ind_i,v_i)\}_{i=1}^m$ updates a
	RAM state $\vecT_0\in \F^n$ to RAM state $\vecT_1\in \F^n$ with the prover-complexity of $\tilde{O}(m+n)$.
	\end{theorem}
	\end{comment}
	\DIFaddend 
	
	%and ~\ref{fig:subtable-consistency}
	%illustrate the overall idea for the first check with a concrete example.
	
	
	\DIFdelbegin %DIFDELCMD < \begin{comment}%DIFDELCMD < 
	%DIFDELCMD < \begin{figure}[t]
	%DIFDELCMD < 	\begin{subfigure}{0.4\textwidth}
	%DIFDELCMD < 		\centering
	%DIFDELCMD < 		\includegraphics[width=\textwidth]{example-lookup}
	%DIFDELCMD < 		\caption{Isolating subtables using lookup argument}
	%DIFDELCMD < 		\label{fig:subtable-consistency}
	%DIFDELCMD < 	\end{subfigure}
	%DIFDELCMD < 	\begin{subfigure}{0.4\textwidth}
	%DIFDELCMD < 		\centering
	%DIFDELCMD < 		\includegraphics[height=0.3\textheight]{Address-ordered}
	%DIFDELCMD < 		\caption{Checking memory consistency using address ordering}
	%DIFDELCMD < 		\label{fig:permuted-transcripts}
	%DIFDELCMD < 	\end{subfigure}
	%DIFDELCMD < \end{figure}
	%DIFDELCMD < \end{comment}
	%DIFDELCMD < %%%
	\DIFdelend \DIFaddbegin \begin{comment}
	\begin{figure}[t]
	\begin{subfigure}{0.4\textwidth}
	\centering
	\includegraphics[width=\textwidth]{example-lookup}
	\caption{Isolating subtables using lookup argument}
	\label{fig:subtable-consistency}
	\end{subfigure}
	\begin{subfigure}{0.4\textwidth}
	\centering
	\includegraphics[height=0.3\textheight]{Address-ordered}
	\caption{Checking memory consistency using address ordering}
	\label{fig:permuted-transcripts}
	\end{subfigure}
	\end{figure}
	\end{comment}
	
	
	
	\DIFaddend %\input{figure-subtable-consistency}
	
	
	
	
	
	\section{Related Work}\label{sec:rel-work}
	%	\input{rel-work}
	\DIFdelbegin \DIFdel{Efficient modelling of the random access memory (RAM ) }\DIFdelend \DIFaddbegin 
	
	\DIFadd{Efficient modeling of the RAM }\DIFaddend primitive is a widely studied problem in
	verifiable computation (VC), due to its inherent usefulness in \DIFdelbegin \DIFdel{modelling }\DIFdelend \DIFaddbegin \DIFadd{modeling }\DIFaddend several computations of interest.
	Encapsulating RAM semantics in VC circuits is also challenging; \DIFdelbegin \DIFdel{\moumita{since} }\DIFdelend \DIFaddbegin \DIFadd{since }\DIFaddend (i) arithmetic/boolean circuits do not
	adequately model random access, and (ii) incorporating entire memory as gates in a circuit is prohibitive.
	\DIFaddbegin 
	
	\DIFaddend Several novel techniques have been proposed to work around the above limitations of circuit based representation
	of RAM. Among them, \DIFdelbegin \DIFdel{merkle tree based }\DIFdelend \DIFaddbegin \DIFadd{Merkle tree-based }\DIFaddend accumulators to model the RAM state are popular
	~\cite{EPRINT:BFRSBW13,compwithstate,C:BCTV14} as they can efficiently
	prove updates to the state, without \DIFdelbegin \DIFdel{modelling }\DIFdelend \DIFaddbegin \DIFadd{modeling }\DIFaddend the entire memory in the \DIFdelbegin \DIFdel{constraint system}\DIFdelend \DIFaddbegin \DIFadd{arithmetic circuit}\DIFaddend . Other
	approaches based on {\em address ordered time-scripts} avoid the concrete costs of \DIFdelbegin \DIFdel{tree based }\DIFdelend \DIFaddbegin \DIFadd{Merkle tree-based }\DIFaddend approaches
	by letting the prover provide inputs and outputs of RAM operations in a non-deterministic manner, which
	are then checked to satisfy consistency of {\em loads} and {\em stores}.
	Several works such as \cite{NDSS:WSRBW15,USENIX:BCTV14,C:BCGTV13,SP:ZGKPP18} implement and improve variants
	of the aforementioned approach. Most \DIFdelbegin \DIFdel{transcript based }\DIFdelend \DIFaddbegin \DIFadd{transcript-based }\DIFaddend realizations of RAM only consider it to be transient,
	i.e, its state is useful only during the execution of a program, and do not consider {\em persistence} of the
	RAM state across several executions. 
	\DIFaddbegin 
	
	\DIFaddend Another feature, which has only been considered in recent works
	\cite{USENIX:OWWB20,CCS:CFHKKO22} is {\em batching}, where a verifiable update of RAM state is required
	for a batch of $m$ updates, with $m$ being much smaller than the RAM size. \DIFdelbegin \DIFdel{While the approaches based on tree
		based accumulatorsrealized using collision resistant hash functions}\DIFdelend \DIFaddbegin \DIFadd{Both the Merkle tree-based approaches
		and the transcript-based approaches are inefficient with respect to batching.
		While constructions using Merkle tree-based accumulators~(realized from collision-resistant hash functions) }\DIFaddend suffer from high concrete costs and poor
	ability to batch proofs, those based on checking consistency using transcripts incur a linear overhead in the
	\DIFdelbegin \DIFdel{size of the RAM }\DIFdelend \DIFaddbegin \DIFadd{RAM size}\DIFaddend .
	
	\DIFdelbegin \subsection{\DIFdel{Batching-Efficient RAM}}%DIFAUXCMD
	\addtocounter{subsection}{-1}%DIFAUXCMD
	%DIFDELCMD < \label{subsec:batching-efficient-ram}
	%DIFDELCMD < 			%%%
	\DIFdelend \DIFaddbegin \smallskip
	
	\noindent{\bf Batching-Efficient RAM.} \DIFaddend There have been \DIFdelbegin \DIFdel{several }\DIFdelend recent efforts~\cite{USENIX:OWWB20,CCS:CFHKKO22} on batching-efficient
	realization of the RAM primitive\DIFaddbegin \DIFadd{~(see Table~\ref{tab:rel-work} for a summary of these schemes and the associated efficiency parameters)}\DIFaddend . This is a natural setting in applications of verifiable
	computation, most notably in the context of blockchain {\em rollups}. Here, one is required
	to show that a batch of $m$ transactions correctly updates the state of a table of account balances, which
	is maintained off-chain by the rollup provider. Here the batch-size $m$ ranges from few hundreds to few
	thousands, whereas the table itself could contain several million accounts.
	\DIFdelbegin \DIFdel{Like }\DIFdelend \DIFaddbegin \DIFadd{Similar to }\DIFaddend prior work on batching-efficient RAMs~\cite{USENIX:OWWB20,CCS:CFHKKO22}\DIFaddbegin \DIFadd{, }\DIFaddend our work is also motivated
	by the problem of enabling more efficient rollup for tables, which are
	naturally \DIFdelbegin \DIFdel{modelled }\DIFdelend \DIFaddbegin \DIFadd{modeled }\DIFaddend as RAMs.
	\DIFaddbegin 
	
	\DIFaddend While the aforementioned works substantially mitigate disadvantages of both the
	Merkle-tree based approaches and transcript-based approaches by using RSA accumulators to model the state,
	they still incur large prover costs and memory requirements even for modest sized batches. The approaches
	in~\cite{USENIX:OWWB20,CCS:CFHKKO22} encode complex modular arithmetic over RSA groups and {\em hash
		to prime} functions as arithmetic circuits\DIFaddbegin \DIFadd{, which }\DIFaddend results in a fixed overhead of around $10$ million R1CS constraints
	at batch sizes of $m=1000$ (this overhead is significantly larger for ~\cite{USENIX:OWWB20})\DIFdelbegin \DIFdel{, which already
		is }\DIFdelend \DIFaddbegin \DIFadd{. This is already }\DIFaddend prohibitive on a modest hardware. In addition, the witness computation for each update incurs cost
	linear in the size of accumulated set\DIFdelbegin \DIFdel{, which authors }\DIFdelend \DIFaddbegin \DIFadd{. The prior works~\mbox{%DIFAUXCMD
			\cite{USENIX:OWWB20,CCS:CFHKKO22} }\hskip0pt%DIFAUXCMD
	}\DIFaddend seek to mitigate \DIFaddbegin \DIFadd{this }\DIFaddend through pre-computation and
	parallel/distributed processing. \DIFdelbegin \DIFdel{Our solution, by contrast }\DIFdelend \DIFaddbegin \DIFadd{However, the issue of maintaining pre-computed parameters in sync with dynamic accumulator state has not been adequately addressed in~\mbox{%DIFAUXCMD
			\cite{USENIX:OWWB20,CCS:CFHKKO22}}\hskip0pt%DIFAUXCMD
		. For example, in RSA-based accumulators, generating $O(N)$ non-membership witnesses straightforwardly requires $O(N^2)$ time; however, these witnesses become stale once the accumulator state changes, and hence cannot be used as-is for subsequent update proofs. 
	}
	
	\DIFadd{Our approach considers both the cost of online proof generation as well as the offline cost of maintaining pre-computed parameters. In addition, our solution }\DIFaddend is almost ``circuit-free''. Our entire RAM operation is \DIFdelbegin \DIFdel{modelled }\DIFdelend \DIFaddbegin \DIFadd{modeled }\DIFaddend as a polynomial protocol, which is readily transformed into an argument of
	knowledge using a polynomial commitment scheme. In an application \DIFdelbegin \DIFdel{such as }\DIFdelend \DIFaddbegin \DIFadd{of our primitive to }\DIFaddend rollups, the only part of the statement 
	that \DIFdelbegin \DIFdel{is }\DIFdelend \DIFaddbegin \DIFadd{would need to be }\DIFaddend expressed as a circuit is the verification of \DIFdelbegin \DIFdel{EDDSA }\DIFdelend digital signatures on transactions\DIFdelbegin \DIFdel{,
		which for is around 500 }\DIFdelend \DIFaddbegin \DIFadd{~(which is around $500$ }\DIFaddend constraints per verification \DIFdelbegin \DIFdel{.
		Combining our novel update protocol with highly efficient
		lookup arguments such as~\mbox{%DIFAUXCMD
			\cite{CCS:CFHKKO22}}\hskip0pt%DIFAUXCMD
		, our online proof generation can prove around $0.5$ million
		updates at an average of just $120$ seconds per }\DIFdelend \DIFaddbegin \DIFadd{for EDDSA signatures). By suitably balancing the online and offline costs, we can prove a }\DIFaddend batch of $1000$ updates on a \DIFdelbegin \DIFdel{commodity laptop. Even
		this performance is under worst-case assumption that each update is to a different position in the table.
		The expensive offline computation, mainly consisting of highly parallelized FFTs over groups requires around $3$ hours for a
		table of size 1 million , }\DIFdelend \DIFaddbegin \DIFadd{RAM of size $1$ million in an average of $90$ seconds on a commodity laptop }\DIFaddend with a single-threaded implementation. \DIFdelbegin \DIFdel{Clearly, with more potent hardware and
		parallelization, one can process a table of size upto $10$ million in a few hours. Our approach is also in the updatable SRS setting in constrast to trusted setup required for RSA parameter generation. We
		compare our work with the prior approaches more elaborately in Section ~\ref{sec:experiments}}\DIFdelend \DIFaddbegin \DIFadd{Our performance can be substantially improved using a parallel implementation. See Sections~\ref{sec:tech-overview} and~\ref{sec:experiments}  for more detailed discussions on the efficiency of our scheme}\DIFaddend . 
	
	\DIFdelbegin \subsection{\DIFdel{Lookup arguments}}
	%DIFAUXCMD
	\addtocounter{subsection}{-1}%DIFAUXCMD
	\DIFdelend %DIF > Combining our novel update protocol with highly efficient
	%DIF > lookup arguments such as~\cite{EPRINT:EagFioGab22}, our online proof generation can prove around $0.5$ million
	%DIF > updates at an average of just $120$ seconds per batch of $1000$ updates on a commodity laptop. Even
	%DIF > this performance is under worst-case assumption that each update is to a different position in the table.
	%DIF > The expensive offline computation, mainly consisting of highly parallelized FFTs over groups requires around $3$ hours for a
	%DIF > table of size 1 million, with a single-threaded implementation. Clearly, with more potent hardware and
	%DIF > parallelization, one can process a table of size upto $10$ million in a few hours. 
	%DIF > Our approach is also in the updatable SRS setting in constrast to trusted setup required for RSA parameter generation. We
	%DIF > compare our work with the prior approaches more elaborately in Section ~\ref{sec:experiments}.
	
	
	\DIFdelbegin \DIFdel{Caulk~\mbox{%DIFAUXCMD
			\cite{CCS:ZBKMNS22} }\hskip0pt%DIFAUXCMD
		provides with lookup arguments where }\DIFdelend \DIFaddbegin \smallskip
	
\smallskip

	\begin{table*}[tb!]
		\begin{tabular}{|ll|lll|}
			\hline
			\multicolumn{1}{|l|}{\textbf{Scheme}} & \multicolumn{1}{l|}{\textbf{Setup}} & \multicolumn{1}{l|}{\textbf{Proof Size}} & \multicolumn{1}{l|}{\textbf{Prover Work}} & \textbf{Verifier Work} \\ \hline
			\rowcolor{lightgray}
			\multicolumn{5}{|c|}{{Lookup Arguments for Static Tables}} \\ \hline
			\multicolumn{1}{|l|}{Plookup \cite{EPRINT:GabWil20}} & \multirow{10}{*}{{Updatable}}  & \multicolumn{1}{l|}{$5 \Gone, ~9\F$} & \multicolumn{1}{l|}{$O(N) \,\Gone$, $O(N\log N) \,\F$} & $2P$ \\ \cline{1-1}\cline{3-5}
			\multicolumn{1}{|l|}{LogUp~\cite{EPRINT:Habock22b,EPRINT:PapHab23}} &  & \multicolumn{1}{l|}{$O(1)\,\Gone, ~O(1) \,\F$} & \multicolumn{1}{l|}{$O(N) \,\Gone$, $O(N\log N) \,\F$} & $O(1) \,P$ \\ \cline{1-1}\cline{3-5}
			\multicolumn{1}{|l|}{Halo2 \cite{EPRINT:BowGriHop19,Halo2}} & & \multicolumn{1}{l|}{$6 \Gone, ~5\F$} & \multicolumn{1}{l|}{$O(N) \,\Gone$, $O(N\log N) \,\F$} & $2P$ \\ \cline{1-1}\cline{3-5}
			\multicolumn{1}{|l|}{Caulk \cite{CCS:ZBKMNS22}} & & \multicolumn{1}{l|}{$14 \Gone, ~1 \Gtwo, ~4\F$} & \multicolumn{1}{l|}{$15m \,\Gone$, $O(m^2+m\log N) \,\F$} & $4P$ \\ \cline{1-1}\cline{3-5}
			\multicolumn{1}{|l|}{Caulk+ \cite{EPRINT:PosKat22}} &   & \multicolumn{1}{l|}{$7 \Gone, ~1 \Gtwo, ~2\F$} & \multicolumn{1}{l|}{$8m \,\Gone$, $O(m^2) \,\F$} & $3P$ \\ \cline{1-1}\cline{3-5}
			\multicolumn{1}{|l|}{Flookup \cite{EPRINT:GabKho22}} &  & \multicolumn{1}{l|}{$7 \Gone, ~1 \Gtwo, ~2\F$} & \multicolumn{1}{l|}{$O(m) \,\Gone$, $O(m \log^2 m) \,\F$}  & $3P$ \\ \cline{1-1}\cline{3-5}
			\multicolumn{1}{|l|}{Baloo \cite{EPRINT:ZGKMR22}} &  & \multicolumn{1}{l|}{$12 \Gone, ~1 \Gtwo, ~4\F$} & \multicolumn{1}{l|}{$14m \,\Gone$, $O(m \log^2 m) \,\F$} & $5P$ \\ \cline{1-1}\cline{3-5}
			\multicolumn{1}{|l|}{CQ \cite{EPRINT:EagFioGab22}} &   & \multicolumn{1}{l|}{$8 \Gone, ~3\F$} & \multicolumn{1}{l|}{$8m \,\Gone$, $O(m \log m) \,\F$} & $5P$ \\ \cline{1-1}\cline{3-5}
			\multicolumn{1}{|l|}{CQ+ \cite{PKC:CFFLL24}} &  & \multicolumn{1}{l|}{$7 \Gone, ~1\F$} & \multicolumn{1}{l|}{$8m \,\Gone$, $O(m \log m) \,\F$} & $5P$ \\ \cline{1-1}\cline{3-5}
			\multicolumn{1}{|l|}{Locq \cite{PKC:ZSG24}} &   & \multicolumn{1}{l|}{$4 \Gone, ~1 \Gtwo$} & \multicolumn{1}{l|}{$6m \,\Gone$, $m \,\Gtwo$, $O(m \log m) \,\F$} & $4P$ \\ \hline
			%		\cline{1-1}\cline{3-5}
			\multicolumn{1}{|l|}{Lasso \cite{lasso}} &   Transparent & \multicolumn{1}{l|}{$O(1) \,\G, ~O(\log m) \,\F$} & \multicolumn{1}{l|}{$o(cm+cN^{1/c}) \,\Gone$, $O(cm) \F$} & $O(\log m) \,\F$, $O(1) \,\G$ \\ \hline
			\rowcolor{lightgray}
			\multicolumn{5}{|c|}{{Lookup Arguments for Updatable Tables/Batching-Efficient RAM}} \\ \hline
			\multicolumn{1}{|l|}{\begin{tabular}[c]{@{}l@{}}OWWB20~\cite{USENIX:OWWB20}\\ \textcolor{white}{text}\end{tabular}} & \multirow{2}{*}{Trusted}  & \multicolumn{1}{l|}{$O(1)~\G$} & \multicolumn{1}{l|}{$\wt{O}(m)\F,\G,\,\wt{O}(N)\G'$} & $1 P$\\ \cline{1-1}\cline{3-5}
			\multicolumn{1}{|l|}{\begin{tabular}[c]{@{}l@{}}B-INS-ARISA~\cite{CCS:CFHKKO22}\\ \textcolor{white}{text}\end{tabular}} &  & \multicolumn{1}{l|}{$O(1)~\G$} & \multicolumn{1}{l|}{$\wt{O}(m)\F,\G,\,\wt{O}(N)\G'$} & $1P, O(1)~\G'$ \\ \hline
			\multicolumn{1}{|l|}{\begin{tabular}[c]{@{}l@{}}Our work\\ (c.f. Table ~\ref{tbl:efficiency-components})\end{tabular}} & \multicolumn{1}{l|}{Updatable}  & \multicolumn{1}{l|}{$65\Gone,\, 1\Gtwo,\, 43\F$} & \multicolumn{1}{l|}{$\wt{O}(\sqrt{mN})\, \F,\Gone$} & $9P$ \\ \hline
		\end{tabular}
		\caption{\DIFadd{Comparison with state-of-the-art lookup arguments. Here, $N$ denotes the size of the RAM and $m$ denotes the number of updates (typically $m \ll N$). We use $(\F,\Gone,\Gtwo,\GT, e, g_1, g_2,g_t)$ to denote a bilinear group, and $\G'$ to denote an RSA group.  For Lasso, we report the overheads considering the polynomial commitment scheme Sona and assuming \textit{structured} tables~(here, $c$ denotes an arbitrary positive integer). For our scheme, we report performance for the CQ-based instantiation.}}
		\label{tab:rel-work}
		\vspace*{-5mm}
	\end{table*}
	
	\noindent{\bf Lookup Arguments.} \DIFadd{There are several recently proposed constructions of lookup arguments which enable proving
		that }\DIFaddend a vector of size $m$ is \DIFdelbegin \DIFdel{proved to be a subvector of a large }\DIFdelend \DIFaddbegin \DIFadd{a sub-vector of a larger~(predetermined) }\DIFaddend vector of size $N$\DIFdelbegin \DIFdel{, while maintaining privacy regarding the positions of the subvector (called }\emph{\DIFdel{position-hiding linkability}}%DIFAUXCMD
	\DIFdel{)where the prover complexity is $O(m^2+m\log N)$ with $O(N\log N)$ preprocessing and $O(N)$ storage. Caulk+~\mbox{%DIFAUXCMD
			\cite{EPRINT:PosKat22} }\hskip0pt%DIFAUXCMD
		improves over Caulk to $O(m^2)$ prover complexity by removing the dependency on the size of the large vector $N$, and CQ~\mbox{%DIFAUXCMD
			\cite{EPRINT:EagFioGab22} }\hskip0pt%DIFAUXCMD
		further improves it to $O(m\log m)$, while retaining the same $O(N\log N)$ preprocessing. 
		However, we aim to provide committed index lookup arguments that can `link' a set of indices in the vector of size }\DIFdelend \DIFaddbegin \DIFadd{~(see Table~\ref{tab:rel-work}
		for a summary of these schemes and the associated efficiency parameters).
		Of these, the very initial schemes~\mbox{%DIFAUXCMD
			\cite{EPRINT:BowGriHop19,EPRINT:GabWil20,Halo2} }\hskip0pt%DIFAUXCMD
		incur proving costs
		linear in $N$. Starting with Caulk~\mbox{%DIFAUXCMD
			\cite{CCS:ZBKMNS22}}\hskip0pt%DIFAUXCMD
		, many lookup arguments were proposed with
		proving costs that are (largely) independent of }\DIFaddend $N$\DIFdelbegin \DIFdel{to the `claimed' subvector of size $m$, that helps in ensuring that we can always `lookup ' relevant indices from a committed vector.
		Additionally, note that any }\DIFdelend \DIFaddbegin \DIFadd{. Broadly, these schemes can be divided into categories
		based on how they achieve proving costs independent of $N$. The lookup arguments in the first
		category~\mbox{%DIFAUXCMD
			\cite{CCS:ZBKMNS22,EPRINT:PosKat22,EPRINT:GabKho22,EPRINT:EagFioGab22,EPRINT:ZGKMR22,PKC:CFFLL24,PKC:ZSG24} }\hskip0pt%DIFAUXCMD
		use polynomial protocols
		in conjunction with the $\kzg$ polynomial commitment scheme, where the lookup efficiency relies crucially on }\textit{\DIFadd{pre-computed}}
	\DIFadd{$\kzg$ opening proofs for the polynomial encoding the predetermined $N$-sized vector.
		We point out that na\"ively adapting these lookup arguments for updatable tables/RAM is challenging since
		even small }\DIFaddend updates to the \DIFdelbegin \DIFdel{large table requires $O(N\log N)$ recomputation each time in the prior works. Our work ensures a batching friendly preprocessing can be done whenever there are incremental changes to the vector of size $N$, and avoid performing the $O(N\log N)$ recomputation per update.
		This modification extends to the other works as well. We also present techniques to extend lookup argument \mbox{%DIFAUXCMD
			\cite{EPRINT:EagFioGab22}}\hskip0pt%DIFAUXCMD
		, which uses homomorphic commitments, to provide a committed index lookup argument}\DIFdelend \DIFaddbegin \DIFadd{table require re-computing all of the opening proofs,
		which is prohibitively expensive~(requires $\wt{O}(N)$ computation).
		To overcome the ``rigidity'', we propose a new method of constructing lookup arguments which allows
		re-using the pre-computed opening proofs across several batch updates, thus avoiding the need for
		re-computing after each batch~(see Sections~\ref{sec:tech-overview} and~\ref{sec:update-protocol} for more details)}\DIFaddend .
	
	
	\DIFdelbegin \DIFdel{Lasso~\mbox{%DIFAUXCMD
			\cite{lasso} }\hskip0pt%DIFAUXCMD
		gives a new family }\DIFdelend %DIF > Caulk~\cite{CCS:ZBKMNS22} provides with lookup arguments where a vector of size $m$ is proved to be a subvector of a large vector of size $N$, while maintaining privacy regarding the positions of the subvector (called \emph{position-hiding linkability}) where the prover complexity is $O(m^2+m\log N)$ with $O(N\log N)$ preprocessing and $O(N)$ storage. Caulk+~\cite{EPRINT:PosKat22} improves over Caulk to $O(m^2)$ prover complexity by removing the dependency on the size of the large vector $N$, and CQ~\cite{EPRINT:EagFioGab22}, using the ``logarithmic derivative based lookup" of \cite{Eag22,Hab22}, further improves it to $O(m\log m)$, while retaining the same $O(N\log N)$ preprocessing. Additional lookup arguments, namely cq+, cq++, zkcq+~\cite{pkc-lookup-mat} and locq~\cite{PKC:ZSG24} provides further improvements over CQ.
	%DIF > However, we aim to provide committed index lookup arguments that can `link' a set of indices in the vector of size $N$ to the `claimed' subvector of size $m$, that helps in ensuring that we can always `lookup' relevant indices from a committed vector. Additionally, note that the prior works provides lookup arguments for `static' tables, i.e. any `updates' to the large table requires $O(N\log N)$ recomputation each time in the prior works. Our work ensures a batching friendly preprocessing can be done whenever there are incremental changes to the vector of size $N$, and avoid performing the $O(N\log N)$ recomputation per update. This modification extends to the other works as well. We also present techniques to extend lookup argument \cite{EPRINT:EagFioGab22}, which uses homomorphic commitments, to provide a committed index lookup argument.
	\DIFaddbegin 
	
	\smallskip
	
	\noindent{\bf Lasso.} \DIFadd{The second category }\DIFaddend of lookup arguments \DIFdelbegin \DIFdel{where the prover's cost only grows with table entries that are
		accessed by the lookup operations,
		for tables that are structured.
		Our work applies to tables without any structure (as is the case with rollup application), and additionally we can handle }\emph{\DIFdel{updatable}}%DIFAUXCMD
	\DIFdel{tables.
		Lasso also defines a committed index lookup relation like us and gives a reduction between indexed and unindexed lookups (subvector relation); however, their reduction needs an upper bound on the elements }\DIFdelend \DIFaddbegin \DIFadd{is exemplified by the recently proposed Lasso scheme~\mbox{%DIFAUXCMD
			\cite{lasso,jolt}}\hskip0pt%DIFAUXCMD
		,
		which enables efficient lookups for tables with a decomposable structure.
		Informally, a table $\vecT:\{0,1\}^n\to \{0,1\}^k$ is said to have a decomposable structure if there exists
		a decomposition of the table $\vec{T}$ into $c$ sub-tables $\vecT_1,\ldots,\vecT_c:\{0,1\}^{n/c} \to\{0,1\}^\ell$ and a
		succinctly computable function $f$ such that for $x = x_1\|\ldots\|x_c$ where $x_i \in \{0,1\}^{n/c}$, we have
	}\[\DIFadd{\vecT[x] = f(\vecT_1(x_1),\ldots,\vecT_c(x_c))}\]
	\DIFadd{A simple example of such a function is a bit-wise AND~(we refer to~\mbox{%DIFAUXCMD
			\cite{lasso,jolt} }\hskip0pt%DIFAUXCMD
		for a more detailed exposition).
		Lasso crucially leverages this decomposability }\DIFaddend of the table \DIFdelbegin \DIFdel{. We provide a different reduction without this restriction. 
		A recent work called Jolt~\mbox{%DIFAUXCMD
			\cite{jolt} }\hskip0pt%DIFAUXCMD
		expresses all constraints of a computation as lookupsby creating a lookup table that contains the entire evaluation table of each instruction $f$ in an instruction set.
		This approach of using table lookups requires memory checking arguments as well.
		However Jolt uses lookup arguments to }\emph{\DIFdel{model}} %DIFAUXCMD
	\DIFdel{CPU operations, whereas our work uses lookup arguments to reduce the
		size of memory in memory-checking methods.
	}\DIFdelend \DIFaddbegin \DIFadd{to reduce a lookup into a table of size $N=2^n$ into $c$ lookups,
		each into a table of size $N^{1/c}$.
		While this strategy works elegantly for tables with special structure, it is not compatible with arbitrary tables/updates,
		which is the focus of our work~(in particular, in applications such as rollups, we need the ability to handle updates to arbitrary tables).
	}\DIFaddend 
	
	\DIFaddbegin \DIFadd{To summarize, existing lookup arguments achieve efficiency either by leveraging table-specific pre-processing
		or exploiting special structure, both of which do not na\"ively extend to arbitrary dynamic tables.
		We focus on handling batch updates for arbitrary tables, and our techniques can be viewed as enabling the
		utility of table-specific pre-processing even }\textit{\DIFadd{across batch updates}}\DIFadd{.
	}
	
	%DIF > However, 
	%DIF > 
	%DIF > consider a table $T$
	%DIF > 
	%DIF > In the literature, to provide sublinear lookups for large tables, either one cleverly uses preprocessing to ensure optimal online cost or one leverages a predefined structure in the tables in consideration.
	%DIF > Lasso~\cite{lasso} gives a new family of lookup arguments for the second category, where the prover's cost only grows with table entries that are accessed by the lookup operations, for tables that are structured. Lasso provides efficient lookups by leveraging a predefined relation between the large table and the `looked-up' small table, where the large table has tensor-structure and is efficiently `decomposable'. This structure  helps to reduce number of elements which need to be committed, in turn reducing the prover time as well.
	%DIF > On the contrary, our work applies to tables without any structure (as is the case with rollup application), and additionally we can handle \emph{updatable} %\chaya{agree on spelling of updatable throughout} 
	%DIF > tables. 
	
	
	%DIF > Note that lasso crucially leverages the tensor-structure of static tables, and uses techniques that are inherently incompatible with arbitrary updates which is essential for handling arbitrary state updates in our considered application and the prover complexity would still be linear in the large table for arbitrary tables which lacks the tensor-structure. 
	%DIF > Therefore, to support arbitrary updates, our work relies on optimally using and updating the pre-computed parameters instead.
	%DIF > Lasso also defines a committed index lookup relation like us and gives a reduction between indexed and unindexed lookups (subvector relation); however, their reduction needs an upper bound on the elements of the table. We provide a different reduction without this restriction. 
	%DIF > A recent work called Jolt~\cite{jolt} expresses all constraints of a computation as lookups by creating a lookup table that contains the entire evaluation table of each instruction $f$ in an instruction set.
	%DIF > This approach of using table lookups requires memory checking arguments as well. However Jolt uses lookup arguments to \emph{model} CPU operations, whereas our work uses lookup arguments to reduce the size of memory in memory-checking methods. \chaya{could add a line here or in conclusion that our techniques could be used to improve Jolt's mem consistency approach.}
	
	
	
	
	
	
	
	\DIFaddend 
	
	\section{Preliminaries}\label{sec:prelims}
	%	\input{prelims}
	
	\DIFaddbegin \DIFadd{This section presents notations and preliminary background material used in the rest of the paper.
	}
	
	\smallskip
	
	\DIFaddend \noindent\textbf{Notation.} Throughout the paper, we assume a bilinear group generator $\bg$ which on input $\lambda$ outputs
	parameters for the protocols. Specifically $\bg(1^\secp)$ outputs $(\F,\Gone,\Gtwo,\GT, e, g_1, g_2,g_t)$
	where:
	\begin{itemize}[leftmargin=2em, label=-]
		\item $\F=\Fp$ is a prime field of super-polynomial size in $\lambda$, with $p=\lambda^{\omega(1)}$.
		\item $\Gone,\Gtwo$ and $\GT$ are groups of order $p$, and $e$ is an efficiently computable non-degenerate \DIFaddbegin \DIFadd{bilinear
		}\DIFaddend pairing $e:\Gone\times \Gtwo\rightarrow \GT$.
		\item Generators $g_1,g_2$ are uniformly chosen from $\Gone$ and $\Gtwo$ respectively and $g_t=e(g_1,g_2)$.
	\end{itemize}
	We write groups $\Gone$ and $\Gtwo$ additively, and use the shorthand notation $\gone{x}$ and $\gtwo{x}$
	to denote group elements $x\cdot g_1$ and $x\cdot g_2$ respectively for $x\in \F$. We \DIFdelbegin \DIFdel{implicity }\DIFdelend \DIFaddbegin \DIFadd{implicitly }\DIFaddend assume
	that all the setup algorithms for the protocols invoke $\bg$ to generate descriptions of groups and fields
	over which the protocol is instantiated. We use $[n]$ to denote the set of integers $\{1,\ldots,n\}$.
	
	\DIFaddbegin \smallskip
	
	\DIFaddend \noindent{\bf Lagrange Polynomials.}
	%\chaya{move to here from sec 6: Lagrange polynomial, vanishing polynomial, roots of unity}
	We denote the $N$th root of unity by $\xi$ and define the subgroup $\setN$ as $\setN =\{\xi,\ldots,\xi^N\}$. 
	Let $\{\mu_i(X)\}_{i=1}^N$ be the associated
	Lagrange basis polynomials over the set $\setN$; that is, $\mu_i(X) = \prod_{j\neq i} \frac{X-\xi^j}{\xi^{i}-\xi^j}$.
	We denote by $Z_{\setN}$ the vanishing polynomial of $\setN$;  $Z_{\setN}(X)=X^N-1$.
	
	\DIFaddbegin \smallskip
	
	\noindent{\bf Formal Derivatives of Polynomials.} \DIFadd{For a polynomial\\ $f(X) = \sum_{i=0}^{d}a_iX^i \in \F[X]$, we define its formal derivative  to be the polynomial $f'(X) = \sum_{i=1}^{d}ia_iX^{i-1}$.
		%DIF > \[f'(X) = \sum_{i=1}^{d}ia_iX^{i-1}\]
	}
	
	\DIFaddend \subsection{Succinct Arguments of Knowledge}
	Let $\R$ be a NP-relation and $\L$ be the corresponding NP-language, where $\L=\{x : \exists$ $w$ such that $(x,w) \in \R\}$. 
	A \emph{succinct argument of knowledge} consists of a pair of PPT algorithms $(\prover,\verifier)$.
	Given a public instance $x$, the prover $\prover$, convinces the verifier $\verifier$, that $x\in \L$, where the prover additionally has as a witness $w$\DIFdelbegin \DIFdel{, }\DIFdelend \DIFaddbegin \DIFadd{.
		We use the notation $b\gets \argoutput{\prover}{\verifier}{w}{x}$ to denote $\verifier$'s output in the interactive
		protocol involving $\prover$ }\DIFaddend and $\verifier$ \DIFaddbegin \DIFadd{with $w$ as $\prover$'s input and $x$ as the common input}\DIFaddend .
	The \emph{knowledge-soundness} property says that if the verifier is convinced, then an efficient extractor algorithm given oracle access to the prover outputs a witness $w$ such that $(x,w)\in \R$. An argument system is \emph{succinct} if the communication complexity and the complexity of $\verifier$ is polylogarithmic in the size of the witness. We provide formal definitions in Appendix~\ref{sec:aok}.
	
	\DIFdelbegin %DIFDELCMD < \medskip
	%DIFDELCMD < 			%%%
	\DIFdelend \DIFaddbegin \smallskip
	\DIFaddend 
	
	\noindent\textbf{Fiat-Shamir.} An interactive protocol is \emph{public-coin} if the verifier's messages are uniformly random strings. Public-coin protocols can be transformed into non-interactive arguments in the Random Oracle Model (ROM) by using the Fiat-Shamir~\cite{C:FiaSha86} heuristic to derive the verifier's messages as the output of a Random Oracle. 
	
	\DIFdelbegin %DIFDELCMD < \medskip
	%DIFDELCMD < 			%%%
	\DIFdelend \DIFaddbegin \smallskip
	\DIFaddend 
	
	\noindent{\bf Modular Approach.}
	A modular approach for designing efficient succinct arguments consists of two steps; constructing an information theoretic protocol in an idealized model, and then compiling the information-theoretic protocol via a cryptographic compiler to obtain an argument system. 
	Informally, the prover and the verifier interact where the prover provides oracle access to a set of polynomials, and the verifier accepts or rejects by checking certain identities over the polynomials output by the prover and possibly public polynomials known to the verifier. Such a protocol can be compiled into a succinct argument of knowledge by realizing the polynomial oracles using a \emph{polynomial commitment scheme}. A polynomial commitment scheme allows a prover to commit to polynomials, and later verifiably open evaluations at chosen points by giving evaluation proofs. This enables the verifier to probabilistically
	check polynomial identities at random points of $\F$. %We refer the reader to ~\cite[Section 4.2]{Gabizon2019PLONKPO} for details on polynomial protocols and their compilation to SNARKs. 
	Many recent constructions of zkSNARKs~\cite{EC:BunFisSze20,EC:CHMMVW20,Gabizon2019PLONKPO} follow this approach where the information theoretic object is a polynomial interactive oracle proof (PIOP) (or a polynomial protocol), and the cryptoprimitive in the compiler is a polynomial commitment scheme.
	
	\subsection{Security Model} We describe public-coin interactive protocols in the structured reference string (SRS) model where \DIFdelbegin \DIFdel{where }\DIFdelend both the parties have access to a SRS. The SRS in our protocols consists of
	encodings of monomials of the form $\left\{\gone{x^i}\right\}_{a\leq i\leq b}$, $\left\{\gtwo{x^i}\right\}_{c\leq i\leq d}$
	for $x$ chosen uniformly from $\F$ and $a,b,c,d$ are bounded by some polynomial in $\lambda$. It then follows
	from ~\cite{EPRINT:BowGabMie17} that such an SRS can be generated using a universal and updatable setup~\cite{C:GKMMM18} requiring
	only one honest participant. In practice, this is a superior security model compared to requiring a fully
	trusted setup. We use $\srs = (\srs_1,\srs_2)$ to denote the structured reference string of the above form. We say
	that the $\srs$ has degree $Q$ if all the elements of $\srs_i$, $i=1,2$ are of the form $[f(x)]_i$ for a
	polynomial $f\in \F_{<Q}[X]$.
	
	\DIFaddbegin \smallskip
	
	\DIFaddend \noindent{\bf Algebraic Group Model.} 
	We \DIFdelbegin \DIFdel{analyse }\DIFdelend \DIFaddbegin \DIFadd{analyze }\DIFaddend security of our protocols in the Algebraic Group Model (AGM) introduced
	in ~\cite{C:FucKilLos18}. An adversary $\Adv$ is called \textit{algebraic} if every group element output by $\Adv$ is accompanied by a representation of that group element in terms of all the group elements that $\Adv$ has seen so far (input and output).
	In the AGM, an adversary $\Adv$ is restricted to be {\em algebraic}, which in our SRS-based protocol means a PPT algorithm satisfying the following:
	for $i\in \{1,2\}$, whenever $\Adv$ outputs an element $A\in \G_i$, it is accompanied by its \DIFdelbegin \DIFdel{representaion}\DIFdelend \DIFaddbegin \DIFadd{representation}\DIFaddend , $\Adv$ also outputs a vector $\vec{v}$
	over $\F$ such that $A=\langle \vec{v},\srs_i\rangle$.
	%\begin{itemize}[leftmargin=1em]
	%\item For $i\in \{1,2\}$, whenever $\Adv$ outputs an element $A\in \G_i$, it also outputs a vector $\vec{v}$
	%over $\F$ such that $A=\langle \vec{v},\srs_i\rangle$.
	%\end{itemize}
	%\chaya{do we use this in proofs?}Following~\cite{C:FucKilLos18}, we write $[A]$ to denote a group element enhanced with its representation; $[A] = (A,v_1,\ldots,v_k)$.
	%AGM is a useful framework for analyzing security of succinct non-interactive arguments of knowledge (SNARKs) obtained from interactive protocols that are described as {\em polynomial protocols}. 
	
	\DIFaddbegin \smallskip
	
	\DIFaddend \noindent{\bf Real and Ideal Pairing Checks}: For an algebraic adversary $\Adv$ interacting in a protocol with a degree $Q$ SRS over a bilinear group, the verifier
	can use the pairing $e:\Gone\times \Gtwo\rightarrow \GT$ to perform ``ideal check'' of the form $(R_1\cdot T_1)\cdot (R_2\cdot T_2)\equiv 0$, where $R_1,R_2$ are
	vectors of polynomials over $\F$ and $T_1, T_2$ are public matrices over $\F$.
	Under the $Q$-DLOG assumption stated below, the aforementioned ideal check
	is equivalent (except with a negligible probability) to a real pairing check $(a\cdot T_1)\cdot (T_2\cdot b)=0$ with $a$ and $b$ denoting vectors in $\F$ encoding polynomials in $R_1$ and $R_2$ in
	groups $\Gone$ and $\Gtwo$ respectively (see ~\cite[Lemma 2.2]{Gabizon2019PLONKPO}).
	\DIFaddbegin 
	
	\DIFaddend \begin{definition}[Q-DLOG Assumption ~\cite{C:FucKilLos18}]\label{defn:q-dlog}
		Fix an integer $Q$. The $Q$-DLOG assumption for $(\Gone,\Gtwo)$ states that given $\gone{1}$,$\gone{x}$,$\ldots$, \\
		$\gone{x^Q}$, $\gtwo{1},\gtwo{x},\ldots,\gtwo{x^Q}$ for
		uniformly chosen $x\gets \F$, the probability of an efficient $\Adv$ outputting $x$ is $\negl(\secp)$.
	\end{definition}
	
	
	\subsection{KZG Commitment Scheme}
	\label{sec:KZG}
	A polynomial commitment scheme allows the prover to open evaluations of a committed polynomial succinctly (Appendix~\ref{sec:pcs_def}).
	We use the $\kzg$ commitment scheme introduced in ~\cite{AC:KatZavGol10} which satisfies succinctness, completeness and knowledge-soundness (extractability)
	in the algebraic group model, while additionally featuring a universal and updatable setup. We denote the $\kzg$ scheme by the tuple of $\ppt$ algorithms $(\KZGsetup$,$\KZGcommit$, $\KZGopen$, $\KZGverify)$ as defined below.
	%We then establish the isomorphism of the KZG polynomial commitment scheme as a vector commitment scheme.
	\begin{definition}[KZG Polynomial Commitment Scheme]
		Let $(\F,\Gone,\Gtwo,\GT, e, g_1, g_2, g_t)$ be output of bilinear group generator \\
		$\bg(1^\secp)$ for security parameter $\secp$.
		The KZG polynomial commitment scheme is defined as follows:
		\begin{itemize}[leftmargin=1em]
			\item $\KZGsetup$ on input $(1^\secp,d)$, where $d$ is the degree bound, outputs
			$\srs = (\{\eltone{\tau},\ldots,\eltone{\tau^d}\}$ ,$\{\elttwo{\tau},\ldots,\elttwo{\tau^d}\})$.
			\item $\KZGcommit$ on input $(\srs,p(X))$, where $p(X) \in \F_{\leq d}[X]$, outputs $C=\eltone{p(\tau)}$
			\item $\KZGopen$ on input $(\srs,p(X),\alpha)$, where $p(X) \in \F_{\leq d}[X]$ and $\alpha\in \F$, outputs $(v, \pi)$ such that $v=p(\alpha)$ and $\pi=\eltone{q(\tau)}$, for
			\[ q(X)=\frac{p(X)-p(\alpha)}{X-\alpha} \]
			\item $\KZGverify$ on input $(\srs,C,v, \alpha,\pi)$, outputs $1$ if the following equation holds, and $0$ otherwise.
			\[ e(C-v\eltone{1} \DIFaddbegin \DIFadd{+ \alpha\pi}\DIFaddend , \elttwo{1}) = e(\pi, \DIFdelbegin \DIFdel{\elttwo{\tau-\alpha}}\DIFdelend \DIFaddbegin \DIFadd{\elttwo{\tau}}\DIFaddend ) \]
			\DIFaddbegin 
			
			\DIFaddend \end{itemize}
	\end{definition}
	\DIFdelbegin %DIFDELCMD < 
	
	%DIFDELCMD < 			%%%
	\DIFdelend \DIFaddbegin \DIFadd{Note that both sides of the verification equation involve a fixed generator, and hence several proof verifications
		can be batched together to reduce the number of pairing computations.
	}\DIFaddend We also assume (w.l.o.g) analogues of $\KZGcommit$, $\KZGopen$ and \DIFaddbegin \\
	\DIFaddend $\KZGverify$ defined over the group $\Gtwo$.
	We shall use the (non-standard) notation $[p(X)]_i$ to denote $[p(\tau)]_i$ for $i\in \{1,2\}$.
	This allows us a convenient shorthand for referring to ``commitment of the polynomial $p(X)$'' in group $\G_i$.
	Our protocols also use batched KZG proofs to show that polynomial $p(X)$ satisfies $p(\alpha_i)=v_i$ for $i\in [n]$. Let
	$\vec{\alpha}=(\alpha_1,\ldots,\alpha_n)$
	denote the vector of evaluation points and $\vec{v}=(v_1,\ldots,v_n)$ denote the vector of claimed evaluations. Then the batched
	version of $\KZGopen$ is described as follows:
	\begin{itemize}[leftmargin=1em]
		\item $\KZGopen$ on input $(\srs,p(X),\vec{\alpha})$, where $p(X) \in \F_{\leq d}[X]$ and $\vec{\alpha}\in\F^n$,
		outputs $(\vec{v}, \pi)$ with $\vec{v}\in \F^n$ such that $v_i=p(\alpha_i)$ and $\pi=\eltone{q(\tau)}$ where
		\[ q(X)=\frac{p(X)-r(X)}{a(X)} \]
		In the above, $a(X)=(X-\alpha_1)\cdots(X-\alpha_n)$, while $q(X)$ and $r(X)$ are the quotient and remainder polynomials when
		$p(X)$ is divided by $a(X)$.
		%Note that the polynomials $a(X)$ and $r(X)$ can be computed by the verifier since $\{\alpha_i,v_i\}_{i\in [n]}$ are known and
		%$a(X)=(X-\alpha_1)\cdots(X-\alpha_n)$ and $r(\alpha_i)=v_i$ for all $i\in [n]$ such that $0\leq \deg(r(X)) \leq n$.
		\item $\KZGverify$ on input $(\srs,C,\vec{v}, \vec{\alpha}, \pi)$, outputs $1$ if the following equations hold, and $0$ otherwise.
		\begin{align*}
			e(C-\eltone{r(\tau)}, \elttwo{1}) &= e(\pi, \elttwo{a(\tau)})
		\end{align*}
		Here, the verifier interpolates the polynomial $r(X)\in \F_{<n}[X]$ such that $r(\alpha_i)=v_i$.
	\end{itemize}
	
	\noindent\textbf{KZG for \DIFdelbegin \DIFdel{vectors}\DIFdelend \DIFaddbegin \DIFadd{Vectors}\DIFaddend .} For $\vec{f}\in \F^N$, let $\enc{f}{\setN}$ denote the polynomial encoding of $\vec{f}$ over $\setN$ given by $\sum_{i=1}^N f_i\mu_i(X)$.
	We use KZG to commit to \emph{vectors} by committing to its polynomial encoding. In general a vector $\vec{g}$ of size $m$ is encoded by a polynomial $g(X)\in \F_{<m}[X]$
	which interpolates $\vec{g}$ over a subgroup $\setV$ consisting of $m^{th}$ roots of unity in some canonical order. We will explicitly state the subgroups for all sizes
	of vectors that we consider.
	
	%\begin{definition}[KZG Polynomial Commitment Scheme \textcolor{red}{degree check included}]
	%    Let $(p,\Gone,\Gtwo,\GT, e, \eltone{1},\elttwo{1})$ be a bilinear group. The KZG polynomial commitment scheme is defined as follows: 
	%    \begin{itemize}
	%	        \item $\KZGsetup$ on input $(1^\lambda,d)$, where $\lambda$ is the security parameter and $d$ is the degree bound, outputs $\srs = \{ \eltone{\tau},\ldots,\eltone{\tau^d},\elttwo{\tau},\ldots,\elttwo{\tau^d}\}$
	%	        \item $\KZGcommit$ on input $(\srs,p(X))$, where $p(X) \in \F_{\leq d}[X]$, outputs $C=\eltone{p(X)}:=\eltone{p(\tau)}$
	%	        \item $\KZGopen$ on input $(\srs,p(X),\alpha)$, where $p(X) \in \F_{\leq d}[X]$ and $\alpha\in \F$, outputs $(v, \pi)$ such that $v=p(\alpha)$ and $\pi=\Comtwo{q(X)}:=\elttwo{q(\tau)}$, for
	%	        \[ q(X)=\frac{p(X)-p(\alpha)}{X-\alpha} \]
	%	        \item $\KZGverify$ on input $(\srs,C,\deg,\alpha,v,\pi)$, outputs $1$ if the following equation holds, and $0$ otherwise.
	%	        \[ e(C-v, \elttwo{\tau}) = e(\pi, \elttwo{\tau-\alpha}) \]
	%	        { \color{teal} \moumita{var 3.}
	%		        Degree check has not been added. With degree check, $\pi$ is modified with
	%		        \[ q(X)=X^{d-\deg(p(X))+2}\frac{p(X)-p(\alpha)}{X-\alpha} \]
	%		        and the verification equation if modified as 
	%		        \[ e(C-v, \elttwo{\tau}^{d-\deg+2}) = e(\pi, \elttwo{\tau-\alpha}) \]
	%		        } %stays unchanged from var 1
	%	    \end{itemize}
	%\end{definition}
	%\nitin{Defer the commitment to vectors to where we need it}
	\DIFdelbegin %DIFDELCMD < \begin{comment}%DIFDELCMD < 
	%DIFDELCMD < 				\paragraph{Vector Commitment Scheme.} We note that there is a natural isomorphism between the set of vectors of length $n$ and polynomials of degree $n-1$, where any vector $\abf=(a_1,\ldots,a_n) \in \F^n$ has a natural equivalent representation as a polynomial $a(X)=a_1\lambda_1(X)+\ldots,a_n\lambda_n(X)$, where $\{\lambda_i(X)\}_{i\in [n]}$ is basis of $\F_{\leq n-1}[X]$. Hence, when we use the KZG polynomial commitment scheme for a vector $\abf=(a_1,\ldots,a_n) \in \F^n$, we can output the commitment as $\KZGcommit(\srs,a(X))$, where $a(X)=a_1\lambda_1(X)+\ldots,a_n\lambda_n(X)$.
	%DIFDELCMD < 				
	%DIFDELCMD < 				
	%DIFDELCMD < 				Now, for vectors of length $n$, we consider the basis $\{\lambda_i(X)\}$ to be the langrange basis with respect to the set consisting of $n^{th}$ roots of unity $\doubleH=\{\omega,\ldots,\omega^n\}$. The lagrange basis helps us use the property that, for some $i\in [n]$ and a value $\alpha$, $a(\omega^i)=a_i=\alpha$ if and only if there exists a polynomial $W(X)$ such that $W(X)=\frac{a(X)-\alpha}{X-\omega^i}$. 
	%DIFDELCMD < 			%DIFDELCMD < \end{comment}
	%DIFDELCMD < 			%%%
	\DIFdelend \DIFaddbegin \begin{comment}
	\paragraph{Vector Commitment Scheme.} We note that there is a natural isomorphism between the set of vectors of length $n$ and polynomials of degree $n-1$, where any vector $\abf=(a_1,\ldots,a_n) \in \F^n$ has a natural equivalent representation as a polynomial $a(X)=a_1\lambda_1(X)+\ldots,a_n\lambda_n(X)$, where $\{\lambda_i(X)\}_{i\in [n]}$ is basis of $\F_{\leq n-1}[X]$. Hence, when we use the KZG polynomial commitment scheme for a vector $\abf=(a_1,\ldots,a_n) \in \F^n$, we can output the commitment as $\KZGcommit(\srs,a(X))$, where $a(X)=a_1\lambda_1(X)+\ldots,a_n\lambda_n(X)$.
	
	
	Now, for vectors of length $n$, we consider the basis $\{\lambda_i(X)\}$ to be the langrange basis with respect to the set consisting of $n^{th}$ roots of unity $\doubleH=\{\omega,\ldots,\omega^n\}$. The lagrange basis helps us use the property that, for some $i\in [n]$ and a value $\alpha$, $a(\omega^i)=a_i=\alpha$ if and only if there exists a polynomial $W(X)$ such that $W(X)=\frac{a(X)-\alpha}{X-\omega^i}$. 
	\end{comment}
	\DIFaddend %Throughout the draft, for $\abf\in \F^n$, we use $\Comone{.}$ to denote $\KZGcommit(\srs,\abf)$ for vectors of length $n$, using the langrange polynomial basis $\{\lambda_i(X)\}$, with respect to the $n^{th}$ root of unity $\omega$, and for $\bbf \in \F^m$, we use $\Comtwo{.}$ to denote $\KZGcommit(\srs,\bbf)$ for vectors of length $m$, using the langrange polynomial basis $\{\mu_i(X)\}$, with respect to the $m^{th}$ root of unity $\nu$.
	
	
	
	
	\subsection{Lookup Arguments}
	Works on lookup arguments~\cite{CCS:ZBKMNS22,EPRINT:PosKat22,EPRINT:ZGKMR22,EPRINT:EagFioGab22}
	consider proving sub-vector relation over committed vectors, i.e, given commitments $c_t$ and $c_v$ to vectors $\vec{t}\in \F^N$
	and $\vec{v}\in \F^m$, one proves that for all $i\in [m]$, there exists $j\in [N]$ such that $v_i=t_j$ . We \DIFaddbegin \DIFadd{will use
		$\vec{v}\preceq \vec{t}$ to denote that $\vec{v}$ is a sub-vector of $\vec{t}$.
		The definition below summarizes the sub-vector relation as defined in prior works.
	}\begin{definition}
		\label{defn:comm-subvec}
		\DIFadd{We define the }{\em \DIFadd{committed sub-vector}} \DIFadd{relation $\RSVEC$ to consist of tuples
			$((c_t, c_v), (\vec{t}, \vec{v}))$ where $c_t,c_v\in \Gone$, $\vec{t}\in \F^N$, $\vec{v}\in \F^m$ such that
			$\vec{v}\preceq \vec{t}$ and $c_t=\kzgcommit(\srs,\enc{t}{\setN})$ and $c_v=\kzgcommit(\srs,\enc{v}{\setV})$.
	}\end{definition}
	\DIFadd{A committed sub-vector argument is an argument of knowledge for the relation $\RSVEC$.
		Next, we }\DIFaddend consider a slightly modified relation that we call {\em committed index lookup} (called indexed lookup in~\cite{lasso})
	where there is a commitment to the indices where $\vec{v}$ appears in $\vec{t}$. \DIFaddbegin \DIFadd{Formally, we define it as below:
	}\DIFaddend 
	
	\begin{definition}
		\label{defn:comm-index-lookup}
		We define the {\em committed index lookup} relation $\RLOOK$ to consist of tuples
		$((c_t,c_a,c_v),(\vec{t},\vec{a},\vec{v}))$ where $c_t,c_a,c_v\in \Gone$, $\vec{t}\in \F^N$, $\vec{a},\vec{v}\in \F^m$ such
		that $v_i = \vec{t}[a_i]=t_{a_i}$ for all $i\in [m]$ and $c_t = \kzgcommit(\srs, \enc{t}{\setN})$, $c_a=\kzgcommit(\srs,\enc{a}{\setV})$
		and $c_v=\kzgcommit(\srs,\enc{v}{\setV})$.
	\end{definition}
	
	A \DIFaddbegin \DIFadd{committed }\DIFaddend lookup argument is a succinct argument of knowledge for the relation $\RLOOK$. 
	 %DIF > \chaya{by defn, in the NI case, the prover is allowed to come up with an adversarial instance, in this case all commitments.}


	
	
	%DIF > 	\input{tech-overview}
	\DIFaddbegin 
	
	\DIFaddend \section{\DIFdelbegin \DIFdel{Model for RAM}\DIFdelend \DIFaddbegin \DIFadd{Technical Overview}\DIFaddend }\DIFdelbegin %DIFDELCMD < \label{sec:model-for-ram}
	%DIFDELCMD < %%%
	%DIF < \input{poly-proto-ram}
	\DIFdel{In this section, we formally define RAM, operations on RAM and associated objects such as
		execution transcripts. We have earlier used a vector $\vecT\in \F^n$ to model a RAM of size $n$, where the $i^{th}$ entry $\vecT[\,i\,]$
		implicitly corresponds to index (address) $i$. Here, we consider a
		generalisation that will be useful later. We will allow a RAM to explicitly associate a value $v\in \F$ to an index $a$ from an }\DIFdelend \DIFaddbegin \label{sec:tech-overview}
	\DIFadd{As we have alluded to earlier, existing memory-checking based techniques to model RAM computations incur a cost
		that is linear in the size of the RAM. We are interested in the setting where the number of operations whose
		execution is to be verified is much smaller than the size of the RAM. Thus, our goal is to achieve prover complexity
		which is }\DIFaddend {\DIFdelbegin %DIFDELCMD < \em %%%
		\DIFdel{indexspace}\DIFdelend \DIFaddbegin \em \DIFadd{sublinear}\DIFaddend } \DIFdelbegin \DIFdel{$\mathcal{I}\subseteq \F$, such that the }\DIFdelend \DIFaddbegin \DIFadd{in the size of the RAM. Before we proceed, we establish a
		working definition of RAM for the rest of the paper. Informally, a RAM maps indices (addresses) to values, where
		we assume that values come from a finite field $\F$, while indices come from a subset $\setind$ of $\F$. For us,
		$\setind$ will generally be the set $\{1,\ldots,k\}$ for some integer $k$ (which may be different from size of
		the RAM $n$). Finally, for an index, there should be at most one value in the RAM, i.e., the }\DIFaddend association is unambiguous.
	\DIFdelbegin \DIFdel{We make this explicit in the following definition .
	}\DIFdelend \DIFaddbegin \DIFadd{The formal definition of RAM is as follows:
	}\DIFaddend \begin{definition}[RAM]\label{defn:RAM}
		Given $n\in \N$, finite field $\F$ and a set $\mathcal{I}\subseteq \F$, a RAM of size $n$ over indices $\mathcal{I}$
		is a tuple \DIFdelbegin \DIFdel{$T=(\vec{a},\vec{v})\in \mathcal{I}^n\times \F^n$ }\DIFdelend \DIFaddbegin \DIFadd{$\vecT=(\vec{a},\vec{v})\in \mathcal{I}^n\times \F^n$ }\DIFaddend such that $\forall\, i,j\in [n]$  $v_i=v_j$ whenever $a_i=a_j$.
		We think of \DIFdelbegin \DIFdel{$T$ }\DIFdelend \DIFaddbegin \DIFadd{$\vecT$ }\DIFaddend as a table with vectors $\vec{a}$ and $\vec{v}$ denoting its columns. The set of all such
		tables will be denoted by $\RAM{I}{n}$.
		%\begin{align*}
		%        T[\,a\,] = \begin{cases}
		%                   v \text{ if } \, \exists i\in [n] \text{ s.t. } (a,v) = (a_i,v_i), \\
		%                  \bot \text{ otherwise }
		%                \end{cases}
		%\end{align*}
	\end{definition}
	\DIFdelbegin %DIFDELCMD < 
	
	%DIFDELCMD < 			%%%
	\DIFdelend For a table \DIFdelbegin \DIFdel{$T=(\vec{a},\vec{v})\in \RAM{I}{n}$}\DIFdelend \DIFaddbegin \DIFadd{$\vecT=(\vec{a},\vec{v})\in \RAM{I}{n}$}\DIFaddend , we refer to tuples $(a_i,v_i)$, $i\in [n]$ as records of the table \DIFdelbegin \DIFdel{$T$}\DIFdelend \DIFaddbegin \DIFadd{$\vecT$}\DIFaddend .
	We use the access notation \DIFdelbegin \DIFdel{$v=T[a]$ }\DIFdelend \DIFaddbegin \DIFadd{$v=\vecT[a]$ }\DIFaddend to mean that $(a,v)$ is a record of \DIFdelbegin \DIFdel{$T$ }\DIFdelend \DIFaddbegin \DIFadd{$\vecT$ }\DIFaddend (note there can be multiple such records
	according to our definition). When we \DIFdelbegin \DIFdel{refer to a vector $\vecT\in \F^n$ as a RAM, we implicitly
		assume $T=(\setind_n,\vecT)$ with $\setind_n=(1,2,\ldots,n)$.
		In this case we have $T[i]=\vecT[i]$.
	}\DIFdelend \DIFaddbegin \DIFadd{consider RAMs where the first column~(of indices) is of the form $\vec{I}_n=(1,2,\ldots,n)$, we simply 
		denote such RAMs by $\vecT\in\F^n$.
		%DIF > When we refer to a vector $\vecT\in \F^n$ as a RAM, we implicitly
		%DIF > assume the first column of the RAM $\vecT$ to be $\vec{I}_n=(1,2,\ldots,n)$. 
		%DIF > $T=(\vec{I}_n,\vecT)$ with $\vec{I}_n=(1,2,\ldots,n)$. In this case we have $T[i]=\vecT[i]$.
	}\DIFaddend For a RAM \DIFdelbegin \DIFdel{$T\in \RAM{I}{n}$}\DIFdelend \DIFaddbegin \DIFadd{$\vecT\in \RAM{I}{n}$}\DIFaddend , a RAM operation is a three tuple $(\op,a,v)$ with $\op\in \{0,1\}$,
	$a\in \setind$ and $v\in \F$. An operation with $\op=0$ is called a {\em load} operation which denotes reading a value $v$
	mapped to index $a$ in the RAM. Similarly, an operation with $\op=1$ is called a {\em store} operation,
	which denotes associating the value $v$ with index $a$ in the RAM.
	We use $\RAMOp{I}$ to denote the set of all RAM operations with index set $\setind$.
	%DIF < We say than an operation
	%DIF < $(\op,a,v)\in \{0,1\}\times \setind\times \F$ is {\em admissible} with respect to RAM $T\in \RAM{I}{n}$ if
	%DIF < $T[\,a\,]=v$ whenever $\op=0$ (i.e, the operation loads a value which agrees with the value in the RAM at the specified index).
	
	\begin{table*}[bt]
		\begin{tabular}{l|l|l|l|l}
			\hline
			{\bf Component  }                                                                                      & {\bf Protocol} & {\bf Prover Work}      & {\bf Verifier Work} & {\bf Communication}   \\ \hline
			Committed Sub-vector Lookup                                                                     & CQ ~\cite{EPRINT:EagFioGab22}      & $O(m \log m)\,\F$, $O(m)\,\Gone$      & $5P$            & $8\Gone$, $3\F$         \\ \hline
			Committed Index Lookup                                                                          & Fig ~\ref{fig:committed-index-lookup}    & $O(m \log m)\,F$, $O(m)\,\Gone$      & $5P$            & $8\Gone$, $3\F$         \\ \hline
			Localized Update in RAM                                                                                & Fig ~\ref{fig:a-identical}   & $O(m \log^2 m)\, \F$, $O(m)\,\Gone$      & $8P$            & $19\Gone$, $1\Gtwo$, $10\F$ \\ \hline
			Table Specific Preprocessing                                                                    & Fast KZG ~\cite{EPRINT:FeiKho23}      & $O(N \log N)\,\F,\G$      & -             & -               \\ \hline
			Lookup from Approximate Setup                                                                   & Sec ~\ref{sec:update-protocol}    & $O((m+\delta)\log^2(m+\delta))\,\F$, $O(m+\delta)\,\Gone$ & -             & -               \\ \hline
			Polynomial Protocol for RAM                                                                     & Fig ~\ref{fig:covering-protocol}   & $O(m \log m)\F,\, O(m)\G$      & $7P$            & $36\Gone$, $30\F$       \\ \hline
			%	Concatenation of transcripts                                                                    & Fig 8    & $O(m \log m)$      & 2P            & 4G1, 6F         \\ \hline
			%	Permutation of transcripts                                                                      & Fig 9    & $O(m \log m)$       & 2P            & 4G1, 5F         \\ \hline
			%	\begin{tabular}[c]{@{}l@{}}Memory consistency \& \\ address ordering of transcript\end{tabular} & Fig 7    & $O(m \log m)$       & 5P            & 20G1,19F        \\ \hline
			\rowcolor{lightgray}
			{Batching-Efficient RAM}                                                                          & {Fig  ~\ref{fig:complete-listing}}    & {$\widetilde{O}(\sqrt{mN})\,\F,\G$}            & ${9P}$            & {$65\Gone$, $1\Gtwo$, $43\F$}  \\ \hline
		\end{tabular}
		\caption{\DIFadd{Asymptotic efficiency of the component protocols for our scheme. Here $N$ = size of the RAM, $m$ = number of operations
			, $\delta$ = Hamming distance of table for which pre-computed parameters are available from the current table.
			$P$ denotes a pairing evaluation. The performance corresponds to CQ based realization of batching-efficient
			RAM.}}
		\label{tbl:efficiency-components}
		\vspace*{-5mm}
	\end{table*}

	
	
	
	\subsection{\DIFadd{Batching-Efficient RAM: Blueprint}}\label{subsec:batching-efficient-ram-blueprint}
	\DIFadd{We will use a vectors in $\F^N$ to denote the ``large'' RAMs, where index column is implicitly
		assumed to be $(1,\ldots,N)$.
		Let $\vecT,\vec{T'}\in \F^N$ denote the initial and final RAM states, and let $\vec{o}$ be
		a sequence of $m$ operations ($m < N$) which updates $\vecT$ to $\vecT'$. Let $\vec{a}\in \F^m$ denote the vector
		of RAM indices referenced by the operations in $\vec{o}$, i.e, $a_i$ is the index referenced by the $i^{th}$ operation.
		To prove the transformation of $\vecT$ to $\vecT'$ via operation sequence $\vec{o}$, we proceed as follows:
	}\begin{itemize}[leftmargin=1em, label=-]
		\item \DIFadd{We isolate sub-tables $\Sbf=(\vec{a},\vec{v})$ and $\Sbf'=(\vec{a},\vec{v'})$ of $\vecT$ and $\vecT'$ consisting of
			rows corresponding to indices in $\vec{a}$. This requires proving $\vec{v}=\vecT[\vec{a}]$ and $\vec{v'}=
			\vecT'[\vec{a}]$, which we show using }{\em \DIFadd{committed index lookup}} \DIFadd{argument discussed in Section ~\ref{subsec:committed-index-lookup}.
		}
		
		\item \DIFadd{On the isolated sub-tables $\Sbf$ and $\Sbf'$ of size $m$, we use the standard memory checking arguments (c.f. argument
			presented in Appendix \ref{sec:poly-proto-ram-app}) to prove that sequence $\vec{o}$ correctly updates $\Sbf$ to $\Sbf'$ with
			prover complexity of $\wt{O}(m)$.
		}
		
		\item \DIFadd{Finally, we show that the RAMs $\vecT$ and $\vecT'$ are identical outside indices in $\vec{a}$. We describe the protocol
			for proving the same in Section ~\ref{subsec:proximity-ram}.
	}\end{itemize}
	
	\begin{figure}[htbp]
		\centering
		\includegraphics[width=0.49\textwidth]{RAM-Lookup}
		\caption{\DIFaddFL{Illustrating different steps of sublinear lookup protocol between large RAMs $\vecT$ and $\vecT'$.}}
		\label{fig:blueprint}
	\end{figure}
	
	\DIFadd{The blueprint for the above approach is illustrated in Figure ~\ref{fig:blueprint}.
	}
	
	\subsection{\DIFadd{Batching-Efficient RAM: Components}}\label{subsec:batching-efficient-ram-components}
	\DIFadd{We now elaborate on the key technical components in realizing the above blueprint.
	}
	
	\smallskip
	
	\noindent{\bf Committed Index Lookup.} \DIFadd{To limit the size of the RAM on which we use memory-checking techniques,
		our first step is to isolate sub-tables of RAMs $\vecT$ and
		$\vecT'$ corresponding to addresses which are involved in the operations.
		This is achieved by looking up RAMs $\vecT$ and
		$\vecT'$ at indices in the committed vector $\vec{a}$. We could leverage the recent work on efficient lookup
		arguments to verifiably extract $m$ indices from a table of size $N$, in time dependent only on $m$.
		However, there are two technical challenges here. First, the aforementioned lookup arguments only prove the sub-vector
		relation, without linking the extracted vector to the indices in $\vec{a}$. This is easily solved, as there
		is an efficient realization of a }{\em \DIFadd{committed index lookup}} \DIFadd{from a }{\em \DIFadd{committed sub-vector}} \DIFadd{argument,
		where the commitment scheme is homomorphic. The details appear in Section ~\ref{subsec:committed-index-lookup},
		with the complete protocol presented in Figure ~\ref{fig:committed-index-lookup}. The second challenge is much more
		formidable: the efficiency of sub-vector arguments (and the committed index lookup argument derived from them)
		depends on }{\em \DIFadd{expensive}} \DIFadd{table-specific pre-processing. This is acceptable when the table in question is
		static, but is infeasible in our setting requiring updatable tables. This motivates our next technical
		component.
	}
	
	\smallskip
	
	\noindent{\bf Fast Lookup from Approximate Setup.} \DIFadd{We build upon the rich body of work on polynomial protocols enabling efficient lookups from static tables~\mbox{%DIFAUXCMD
			\cite{CCS:ZBKMNS22,EPRINT:PosKat22,EPRINT:ZGKMR22,EPRINT:EagFioGab22}}\hskip0pt%DIFAUXCMD
		, which rely on expensive table-dependent pre-computation
		to optimise online proving performance. We make the first attempt towards breaking this rigid dependence.
		Our key idea is to extend the utility of pre-computed
		parameters for a table $\vecT$, to proving lookups from tables $\vecT'\neq \vecT$.
		We show that for $\delta=\Delta(\vecT, \vecT')$,
		an argument for $m$ lookups from $\vecT'$ incurs an additional prover overhead of $(m+\delta)\log^2(m+\delta)$ over the
		lookup argument for static tables. We note that the overhead is }{\em \DIFadd{quasi}}\DIFadd{-linear in both $m$ and $\delta$.
		Our competitive overhead rests on several innovative applications of algebraic
		algorithms, which are summarised in Appendix ~\ref{subsec:comp-algebra-app}. We then leverage this ability to use ``approximate"
		setup into a }{\em \DIFadd{base + cache}} \DIFadd{strategy; where at all times we maintain pre-computed parameters corresponding to
		a base table $\vecTbase$, and use this setup to prove lookups from the current table $\vecT$. We achieve optimal
		prover effort on average by using parameters for $\vecTbase$ till the current table is at a hamming distance
		at most $\sqrt{mN}$ from $\vecTbase$, beyond which we recompute full parameters for the current table with
		$O(N\log N)$ prover effort. The cycle then repeats with current table as the base table.
	}
	
	\smallskip
	
	\noindent{\bf Naive Approaches are Inadequate.} \DIFadd{We notice that the aforementioned constructions of lookup arguments require linear combination of
		encoded quotients of the form $\gany{(T(X)-T(\xi^i))/(X-\xi^i)}$ for upto $m$ values of $i$ during the proof generation.
		While constructions ~\mbox{%DIFAUXCMD
			\cite{CCS:ZBKMNS22,EPRINT:PosKat22}
		}\hskip0pt%DIFAUXCMD
		consider quotients encoded in the group $\Gtwo$, the protocol in ~\mbox{%DIFAUXCMD
			\cite{EPRINT:EagFioGab22} }\hskip0pt%DIFAUXCMD
		encodes them in $\Gone$.
		We use a generic $[\,\cdot\,]_g$ to
		account for protocol-specific choices. We also see that even a small change to the table requires one to update all the quotients (the polynomial $T(X)$ is
		common to all quotients). Updating all the quotients after each batch is clearly infeasible. One could consider delaying the updation of the quotients, till
		the time they are actually required in a proof, which happens when the corresponding index in the table is involved in lookup. However, each of the $m$ quotients
		is now potentially ``lagging'' by $\delta$ updates, so we would need $\Omega(m\delta)$ group operations to refresh all of them. This gives us multiplicative degradation
		with $\delta$, and is clearly unsustainable for reasonable values of $\delta$. In Section ~\ref{sec:update-protocol}, we present an efficient method
		to directly compute linear combination of upto $O(m)$ encoded quotients of the form $\gany{(T(X)-T(\xi^i))/(X-\xi^i)}$.
	}
	
	\smallskip
	
	\noindent{\bf Localizing changes in RAMs.} \DIFadd{While the above two components allow us to reliably extract sub-RAMs
		corresponding to indices in vector $\vec{a}$, we still need to prove that RAMs are identical outside indices in
		$\vec{a}$. Looking ahead, in terms of polynomials this requires proving that $T(\xi^i)=T^*(\xi^i)$ for $i\not\in
		\{a_i:i\in [m]\}$. Assuming $Z_I(X)$ to be the vanishing polynomial of the set $\{\xi^{a_i}:i\in [m]\}$,
		this is equivalent to proving that $Z_I(X)(T(X) - T^*(X)) = D(X)\vpolyN(X)$ for some polynomial $D$. However, naively
		this involves working with polynomials with degree $O(N)$, which is expensive.
		In Section ~\ref{subsec:proximity-ram} we show a polynomial protocol for the above relation which requires only $O(m\log^2 m)$ prover effort. The protocol
		appears in Figure ~\ref{fig:a-identical}.
	}
	
	\smallskip
	
	\noindent{\bf Polynomial Protocol for Memory Checking.} \DIFadd{To complete the verification, we need to show that
		the smaller RAMs, $\Sbf=(\vec{a},\vec{v})$ and $\Sbf'=(\vec{a},\vec{v}')$ extracted from larger RAMs $\vecT,\vecT'$
		are consistent with respect to the operations. This can be accomplished using standard memory checking techniques
		based on address ordered transcripts, which we formalize in Section ~\ref{sec:model-for-ram}. Later in
		Appendix ~\ref{sec:poly-proto-ram-app} and ~\ref{sec:poly-proto-ram}, we assemble known techniques to present
		a polynomial protocol for memory consistency based on address ordered transcripts.
		This involves encoding several artefacts such as operations, transcripts etc., as polynomials and relations
		among them such as concatenation, permutation and monotonicity as polynomial identities. Our modelling is
		simple and implementation friendly, and helps in realizing a ``circuit-free'' overall construction. Complete
		polynomial protocol for memory checking appears in Figure ~\ref{fig:covering-protocol}, while constituent protocols
		appear in Figures ~\ref{fig:time-ordered-transcript}, ~\ref{fig:encoded-relations} and ~\ref{fig:permutated-transcripts}.
	}
	
	\smallskip
	
	\noindent{\bf Efficiency.} \DIFadd{We conclude the overview with a discussion of efficiency achieved by our scheme, and
		how different components discussed in this section contribute to the overall efficiency. The asymptotic performance
		of our scheme using CQ ~\mbox{%DIFAUXCMD
			\cite{EPRINT:EagFioGab22} }\hskip0pt%DIFAUXCMD
		is summarized in Table ~\ref{tbl:efficiency-components}, with
		efficiency of the overall scheme highlighted in gray. The table also serves as a ready-reckoner for component protocols
		involved in the overall scheme. A more detailed discussion and break-up of the polynomial protocol for RAM
		appears in Table~\ref{tbl:components-poly-ram} in Appendix~\ref{sec:poly-proto-ram-app}. We note that the verification complexity of the overall solution is substantially
		less than the aggregate of component protocols; this is due to the fact that several pairing checks required for
		$\kzg$ verification proofs can be batched together. For concrete instantiation using BLS12-381 curve, RAM size
		of 1 million, the online cost of proving an update of $1000$ operations on a table as a function of its
		Hamming distance from the ``pre-processed'' table is described in Figure ~\ref{fig:batch-ram-proving-time}.
		Other performance metrics are summarised in Tables ~\ref{tbl:performance-comparison} and ~\ref{tbl:offline-proving-time}.
		%DIF > As an illustrative example,the other performance metrics for the same setting are summarized below:
		%DIF > \begin{table}[htbp]
		%DIF > \begin{tabularx}{0.45\textwidth}{|X|X|}
		%DIF > \hline
		%DIF > {\bf Metric} & {\bf Performance} \\ \hline
		%DIF > Parameter Re-computation & 12000 secs \\ \hline
		%DIF > Verification Time & $\approx$ 10 ms \\ \hline
		%DIF > Argument Size & $\approx$ 4.4 KB \\
		%DIF > \hline
		%DIF > \end{tabularx}
		%DIF > \end{table}
		We refer to Section~\ref{sec:experiments} for a more detailed performance evaluation and comparison with prior work.
		As is clear from the tables above, the (offline) parameter re-computation is the most expensive operation.
		We reiterate that all of our reported costs throughout the paper are for a single-threaded implementation on a consumer-grade laptop.
		We believe that parallel implementations can
		substantially speed up parameter re-computation as their cost is dominated by FFTs over group polynomials, which
		are highly parallelizable.
	}
	
	\smallskip
	
	\noindent{\bf Continuity.} \DIFadd{To support applications such as rollups, we also consider it imperative to ensure that online proof generation does not halt during offline parameter re-generation. In other words, offline parameter re-generation should not hinder the operational continuity of the system. In our scheme, we can ensure this by carefully overlapping the offline computation with online proof generation such that the system can }\textit{\DIFadd{instantly}} \DIFadd{switch to using the more recently generated parameters before the online proving time becomes prohibitive. We present more concrete discussions around this scheduling at the end of Section~\ref{sec:experiments}. We note that prior works~\mbox{%DIFAUXCMD
			\cite{USENIX:OWWB20,CCS:CFHKKO22} }\hskip0pt%DIFAUXCMD
		have not addressed this issue of continuity in detail. To the best of our knowledge, we are the first to highlight the issue and present a discussion on a viable approach.
	}
	
	%DIF > aligning the online prover performance curve with the cost of offline computation.  
	%DIF > 
	%DIF > Finally, to maintain {\em liveliness} of the system~(this is particularly important for applications such as rollups), we must carefully align the online prover performance curve with the cost of offline computation. 
	
	
	%DIF > As an example, suppose $\vecT_0$
	%DIF > is the initial pre-processed table at time $t=0$. We generate proofs using pre-computed parameters for $\vecT_0$
	%DIF > till the time $t=t_1$, when the table state $\vecT_1$ is at hamming distance $2^{17}$ from $\vecT_0$. At this point,
	%DIF > from Figure ~\ref{fig:batch-ram-proving-time}, online proof generation takes around $40s$ for batch of $1000$ updates.
	%DIF > At $t=t_1$, we also start an offline parameter computation for the table $\vecT_1$, while continuing to generate
	%DIF > online proofs using parameters for $\vecT_0$. We can generate the next $2^7$ batches of updates at an average of
	%DIF > approximately $12000/128\approx 94s$ each, thus finishing with a table state $\vecT_2$ at hamming distance at most
	%DIF > $2^{18}$ from $\vecT_0$ at $t_2=t_1+12000$. At this point, we should have the pre-computed parameters for $\vecT_1$,
	%DIF > which is at update distance of $2^{17}$ from $\vecT_2$, and thus online proof generation can switch to parameters
	%DIF > for the table $\vecT_1$. This alignment gives us a proving time of $94s$ per batch of $1000$, while ensuring system
	%DIF > is live at all times. Clearly, a faster offline pre-computation using parallel implementation would allow us to stay
	%DIF > at the cheaper end of online proving performance.
	
	
	
	
	
	
	%DIF > which as seen earlier is given by the summation below:
	%DIF > \begin{equation}\label{eq:encoded-quotient}
	%DIF > \gany{\frac{T(X)-T_I(X)}{Z_I(X)}} = \sum_{i\in I}\frac{1}{Z_I'(\xi^i)}\gany{\frac{T(X)-T(\xi^i)}{X-\xi^i}}
	%DIF > \end{equation}
	%DIF > We now describe our approach.
	
	
	
	
	
	
	
	%DIF > The efficiency of the above approach relies crucially on the efficiency of committed index lookup used to
	%DIF > reduce the size of the RAMs for quasi-linear memory checking methods. It is tempting to use the recent lookup arguments
	%DIF > in ~\cite{CCS:ZBKMNS22,EPRINT:PosKat22,EPRINT:EagFioGab22} to prove the correctness of the first step with prover complexity dependent only on $m$.
	%DIF > However, employing them directly is difficult; their table-independent efficiency relies on
	%DIF > table specific expensive pre-computation, which does not help when the table is updatable. This is the problem we solve
	%DIF > in Section ~\ref{sec:update-protocol}, where we modify the prover algorithm for the lookup arguments to remain efficient
	%DIF > with access to pre-computed parameters for an ``approximate'' table.
	
	
	
	\section{\DIFadd{Memory Consistency for RAM}}\label{sec:model-for-ram}
	%DIF > 	\input{poly-proto-ram}
	
	
	\DIFadd{In this section, we briefly review and formalize existing memory-checking techniques to ensure
		correctness of RAM operations. The formal definitions for various relations involved in memory checking
		will be used to describe polynomial protocol for RAM in Appendix~\ref{sec:poly-proto-ram-app}.
	}
	
	\DIFaddend \subsection{Correctness of RAM Update}\label{subsec:ram-update}
	The versatility of the RAM primitive stems from its updatability. While a load operation leaves the RAM unchanged, the store operation
	updates the value in the RAM associated with the referenced index. We model the update via the function
	$\Rupd{I}$ which takes RAM \DIFdelbegin \DIFdel{$T\in \RAM{I}{n}$}\DIFdelend \DIFaddbegin \DIFadd{$\vecT\in \RAM{I}{n}$}\DIFaddend , operation
	$o=(\op,a,v)\in \RAMOp{I}$ as inputs and returns an updated RAM \DIFdelbegin \DIFdel{$T'\in \RAM{I}{n}$}\DIFdelend \DIFaddbegin \DIFadd{$\vecT'\in \RAM{I}{n}$}\DIFaddend .
	The updated RAM \DIFdelbegin \DIFdel{$T'=\Rupd{I}(T,o)$ satisfies
		$T'=T$ }\DIFdelend \DIFaddbegin \DIFadd{$\vecT'=\Rupd{I}(\vecT,o)$ satisfies
		$\vecT'=\vecT$ }\DIFaddend if $\op=0$ while for $\op=1$ it satisfies \DIFdelbegin \DIFdel{$T'[\,a\,]=v$  and $T'[\,x\,]=T[\,x\,]$ }\DIFdelend \DIFaddbegin \DIFadd{$\vecT'[\,a\,]=v$  and $\vecT'[\,x\,]=\vecT[\,x\,]$ }\DIFaddend for $x\neq a$. The central problem
	in verifiable RAM protocols is to establish that a sequence of operations $\vec{o}=(o_1,\ldots,o_m)$ are correct with
	respect to the initial RAM state \DIFdelbegin \DIFdel{$T$ }\DIFdelend \DIFaddbegin \DIFadd{$\vecT$ }\DIFaddend and the final RAM state \DIFdelbegin \DIFdel{$T'$}\DIFdelend \DIFaddbegin \DIFadd{$\vecT'$}\DIFaddend . This involves ensuring
	that all load operations read the value which is consistent with updates to the RAM as a result of preceding
	store operations, and that \DIFdelbegin \DIFdel{$T'$ }\DIFdelend \DIFaddbegin \DIFadd{$\vecT'$ }\DIFaddend is the final state. We say that an operation $o=(\op,a,v)$ is {\em load-consistent}
	with respect to RAM \DIFdelbegin \DIFdel{$T$ if $v=T[a]$ }\DIFdelend \DIFaddbegin \DIFadd{$\vec{T}$ if $v=\vecT[a]$ }\DIFaddend whenever $o$ is a load operation (store operations are vacuously defined to be load-consistent).
	We formally define the notion of consistency below:
	
	\begin{definition}[Consistent Operations]\label{defn:consistent-operations}
		Let $n\in \N$ and \DIFdelbegin \DIFdel{$T,T'\in \RAM{I}{n}$ }\DIFdelend \DIFaddbegin \DIFadd{$\vecT,\vecT'\in \RAM{I}{n}$ }\DIFaddend for some index set $\setind$. We say that a sequence of operations
		$\vec{o}=(o_1,\ldots,o_k)\in \RAMOp{I}^k$ over $\setind$ is {\em consistent} with RAM states
		\DIFdelbegin \DIFdel{$T,T'$ }\DIFdelend \DIFaddbegin \DIFadd{$\vecT,\vecT'$ }\DIFaddend if for all $i\in [k]$, \DIFdelbegin \DIFdel{$T_{i}=\Rupd{I}(T_{i-1},o_i)$ }\DIFdelend \DIFaddbegin \DIFadd{$\vecT_{i}=\Rupd{I}(\vecT_{i-1},o_i)$ }\DIFaddend and operation $o_i$ is load-consistent with respect to \DIFdelbegin \DIFdel{$T_{i-1}$}\DIFdelend \DIFaddbegin \DIFadd{$\vecT_{i-1}$}\DIFaddend . Here
		we assume \DIFdelbegin \DIFdel{$T_0=T$ and $T_k=T'$}\DIFdelend \DIFaddbegin \DIFadd{$\vecT_0=\vecT$ and $\vecT_k=\vecT'$}\DIFaddend .
	\end{definition}
	
	For $m,n\in \N$, let $\LRAM{I}{m}{n}$ denote the language consisting of tuples \DIFdelbegin \DIFdel{$(T,\vec{o},T')$ with $T,T'\in \RAM{I}{n}$ }\DIFdelend \DIFaddbegin \DIFadd{$(\vecT,\vec{o},\vecT')$ with $\vecT,\vecT'\in \RAM{I}{n}$ }\DIFaddend and $\vec{o}\in (\RAMOp{I})^m$
	such that $\vec{o}$ is consistent with \DIFdelbegin \DIFdel{$T,T'$}\DIFdelend \DIFaddbegin \DIFadd{$\vecT,\vecT'$}\DIFaddend .
	Next, we formalize the folklore technique of checking correctness of RAM operations
	using {\em address-ordered transcripts}.
	
	\subsection{Consistency Check via Transcripts}\label{subsec:transcripts}
	A {\em transcript} is time-stamped sequence of operations executed on a RAM.
	More formally, given a RAM \DIFdelbegin \DIFdel{$T=(\vec{a},\vec{v})\in \RAM{I}{n}$}\DIFdelend \DIFaddbegin \DIFadd{$\vecT=(\vec{a},\vec{v})\in \RAM{I}{n}$}\DIFaddend ,
	operation sequence $\vec{o}=(o_1,\ldots,o_m)$ with $o_i=(\bar{\op_i},\bar{a}_i,\bar{v}_i)\in \RAMOp{I}$ and RAM \DIFdelbegin \DIFdel{$T'=(\vec{a}', \vec{v}')\in \RAM{I}{n}$}\DIFdelend \DIFaddbegin \DIFadd{$\vecT'=(\vec{a}', \vec{v}')\in \RAM{I}{n}$}\DIFaddend ,
	the {\em time ordered transcript} for the tuple \DIFdelbegin \DIFdel{$(T,\vec{o},T')$ }\DIFdelend \DIFaddbegin \DIFadd{$(\vecT,\vec{o},\vecT')$ }\DIFaddend is given by the table $\tr$ with $k=2n+m$ rows and four columns
	$\tr = (\vec{t},\vec{\op},\vec{A},\vec{V})$  defined as follows: (i) $\vec{t}=\setind_{k}=(1,\ldots,k)$,
	(ii) $\vec{\op}=0^n||(\bar{\op}_1,\ldots,\bar{\op}_m)||0^n$,
	(iii) $\vec{A}=\vec{a}||(\bar{a}_1,\ldots,\bar{a}_m)||\vec{a}'$ and
	(iv) $\vec{V}=\vec{v}||(\bar{v}_1,\ldots,\bar{v}_m)||\vec{v}'$. The $i^{th}$ row of the table $\tr$ is
	$(t_i,\op_i,A_i,V_i)$ for $i\in [k]$. The first $n$ records in $\tr$ correspond to the contents of \DIFdelbegin \DIFdel{$T$}\DIFdelend \DIFaddbegin \DIFadd{$\vecT$}\DIFaddend ,
	the next $m$ records correspond to the operations in $\vec{o}$ and final $n$
	records correspond to contents of \DIFdelbegin \DIFdel{$T'$}\DIFdelend \DIFaddbegin \DIFadd{$\vecT'$}\DIFaddend . The timestamp column $\vec{t}$ is added to order operations with the same index.
	Notationally, we write \DIFdelbegin \DIFdel{$\tr=\TOT(T,\vec{o},T')$}\DIFdelend \DIFaddbegin \DIFadd{$\tr=\TOT(\vecT,\vec{o},\vecT')$}\DIFaddend .
	
	We call a transcript $\tr=(\vec{t},\vec{\op},\vec{A},\vec{V})$ to be {\em address ordered} if $A_i\leq A_{i+1}$ for $i\in [k-1]$ and
	$t_i < t_{i+1}$ whenever $A_i=A_{i+1}$. For a transcript $\tr=(\vec{t},\vec{\op},\vec{A},\vec{V})$ with $k$ records and a
	permutation $\sigma:[k]\rightarrow [k]$, we use $\sigma(\tr)$ to denote the transcript
	$(\sigma(\vec{t}),\sigma(\vec{\op}),\sigma(\vec{A}),\sigma(\vec{V}))$
	obtained by permuting the records of $\tr$ according to the permutation $\sigma$.
	An address ordered transcript for tuple \DIFdelbegin \DIFdel{$(T,\vec{o},T')$ }\DIFdelend \DIFaddbegin \DIFadd{$(\vecT,\vec{o},\vecT')$ }\DIFaddend is defined as $\tr^\ast=\sigma(\tr)$ where \DIFdelbegin \DIFdel{$\tr=\TOT(T,\vec{o},T')$ }\DIFdelend \DIFaddbegin \DIFadd{$\tr=\TOT(\vecT,\vec{o},\vecT')$ }\DIFaddend and $\sigma$ is
	a permutation such that $\tr^\ast$ is address ordered. We denote it by \DIFdelbegin \DIFdel{$\tr^\ast=\AOT(T,\vec{o},T')$}\DIFdelend \DIFaddbegin \DIFadd{$\tr^\ast=\AOT(\vecT,\vec{o},\vecT')$}\DIFaddend .
	We say that an address ordered transcript $\tr=(\vec{t},\vec{\op},\vec{A},\vec{V})$ satisfies {\em load-store correctness}
	if for all pairs of consecutive records $(t_i,\op_i,A_i,V_i)$ and $(t_{i+1},\op_{i+1},A_{i+1},V_{i+1})$ we have $V_{i+1}=V_i$
	whenever $\op_{i+1}=0$ (load operation) and
	$A_i=A_{i+1}$, i.e, a load operation does not change the value at an index.
	We formally state the folklore technique for enforcing memory consistency in our setting.
	\begin{lemma}\label{lem:consistency-check}
		Let $\F$ be a finite field, $m,n\in \N$ be positive integers and $\setind\subseteq \F$. Then \DIFdelbegin \DIFdel{$(T,\vec{o},T')\in \LRAM{I}{n}{m}$ }\DIFdelend \DIFaddbegin \DIFadd{$(\vecT,\vec{o},\vecT')\in \LRAM{I}{n}{m}$ }\DIFaddend if and only if
		the address ordered transcript \DIFdelbegin \DIFdel{$\tr^\ast=\AOT(T,\vec{o},T')$ }\DIFdelend \DIFaddbegin \DIFadd{$\tr^\ast=\AOT(\vecT,\vec{o},\vecT')$ }\DIFaddend satisfies load-store correctness.
	\end{lemma}
	
	The consistency check in Lemma \ref{lem:consistency-check} can be encoded as an arithmetic circuit of size $\wt{O}(m+n)$, thus
	yielding an argument of knowledge for the language $\LRAM{I}{n}{m}$ with prover complexity quasi-linear in
	$m+n$. For completeness, we present a self-contained argument of knowledge for
	$\LRAM{I}{m}{m}$ ($m=n$) based on the ``polynomial protocol'' framework defined in ~\cite{Gabizon2019PLONKPO}.
	
	
	
	\DIFdelbegin %DIFDELCMD < \begin{comment}%DIFDELCMD < 
	%DIFDELCMD < 				\begin{enumerate}[leftmargin=2em]
	%DIFDELCMD < 					\item $\prover\rightarrow\verifier$: For the transcript $\tr^\ast=(\vec{t^\ast},\vec{\op^\ast},\vec{A^\ast},\vec{V^\ast})$,
	%DIFDELCMD < 					the prover computes encoding polynomials $\wt{\tr^\ast}=({t^\ast}(X),\op^\ast(X), A^\ast(X),V^\ast(X))$. Next, the prover
	%DIFDELCMD < 					computes sets $I_1$ and $I_2$ as in Section \ref{subsec:encoded-relations}, and then computes polynomials
	%DIFDELCMD < 					$Z_b(X)=\prod_{i\in I_b}(X-\omega^i)$ for $b=1,2$.
	%DIFDELCMD < 					The prover also interpolates polynomials $\delta^\ast_A(X)$ and $\delta^\ast_T(X)$ satisfying
	%DIFDELCMD < 					$\delta^\ast_A(\omega^i) = A^\ast_{i+1} - A^\ast_i$ for $i\in I_1$ and $\delta^\ast_T(\omega^i)=t^\ast_{i+1}-t^\ast_i$ for $i\not\in I_1$.
	%DIFDELCMD < 					The prover sends all the above polynomials to the verifier.
	%DIFDELCMD < 					\item $\verifier$ computes: The verifier checks relations (C1)-(C6) in Lemma \ref{lem:addr-ordered-transcript}. The range constraints
	%DIFDELCMD < 					in (C7) can be verified using polynomial protocols in sub-vector lookup arguments such as Caulk+ ~\cite{EPRINT:PosKat22}, CQ
	%DIFDELCMD < 					~\cite{EPRINT:EagFioGab22}.
	%DIFDELCMD < 				\end{enumerate}
	%DIFDELCMD < 				
	%DIFDELCMD < 				
	%DIFDELCMD < 				\subsection{Succinct Argument for Verifiable RAM}\label{subsec:succ-args}
	%DIFDELCMD < 				Polynomial protocols can be compiled into succinct arguments of knowledge using an extractable polynomial commitment scheme. At a
	%DIFDELCMD < 				high level the compilation involves the prover sending ``commitments'' to the message polynomials, whereas the verifier checks
	%DIFDELCMD < 				the polynomial identities probabilistically by requesting evaluation proofs of the polynomials at random points.
	%DIFDELCMD < 				We complile the polynomial protocol for RAM to a succinct argument of knowledge in the Algebraic Group Model (AGM)
	%DIFDELCMD < 				using $\kzg$ as the extractable polynomial commitment scheme. We also leverage homomorphism of $\kzg$ scheme
	%DIFDELCMD < 				to avoid sending commitments and evaluations for polynomials which are linear combinations of previously committed polynomials.
	%DIFDELCMD < 				We now present an argument for verifiable RAM, which combines the polynomial protocols in this section, and instantiates
	%DIFDELCMD < 				them using $\kzg$ commitments. We make standard optimisations of batching several identity checks into one where applicable.
	%DIFDELCMD < 				Let $\srs=\{\gone{\tau^i},\gtwo{\tau^i}\}_{i=0}^d$
	%DIFDELCMD < 				denote the $\kzg$ setup parameters for degree $d\geq k$ for the bilinear group $\mathsf{BG}=(\Gone,\Gtwo,\gone{1},\gtwo{1},e)$. Let
	%DIFDELCMD < 				$\F=\F_p$ be the finite field where $p$ is the order of the groups.
	%DIFDELCMD < 				
	%DIFDELCMD < 				
	%DIFDELCMD < 				\begin{figure}[t]
	%DIFDELCMD < 					\begin{subfigure}{\linewidth}
	%DIFDELCMD < 						\centering
	%DIFDELCMD < 						{\footnotesize
	%DIFDELCMD < 							\begin{enumerate}[leftmargin=1em]
	%DIFDELCMD < 								\item $\prover\rightarrow\verifier$: Send polynomial commitments $\gone{a(X)}$, $\gone{v(X)}$, $\gone{a'(X)}$, $\gone{v'(X)}$,
	%DIFDELCMD < 								$\gone{\bar{\op}(X)}$, $\gone{\bar{a}(X)}, \gone{\bar{v}(X)}$, $\gone{\op(X)}$, $\gone{A(X)}$, $\gone{V(X)}$, $\gone{t^\ast(X)}$,
	%DIFDELCMD < 								$\gone{\op^\ast(X)}$, $\gone{A^\ast(X)}$, $\gone{V^\ast(X)}$, $\gone{\delta_A^\ast(X)}$, $\gone{\delta_T^\ast(X)}$.
	%DIFDELCMD < 								\item $\verifier\rightarrow\prover$: Send $\alpha,\beta,\gamma\gets \F$.
	%DIFDELCMD < 								\item $\prover$ computes: Compute sets $I_1=\{i\in [k]: A^\ast_i\neq A^\ast_{i+1}\}$, and $I_2=[k]\setminus I_1$. Next, compute
	%DIFDELCMD < 								polynomials:
	%DIFDELCMD < 								\begin{align*}
	%DIFDELCMD < 									&Z_1(X)=\prod_{i\in I_1}(X-\omega^i),\quad Z_2(X)=\prod_{i\in I_2}(X-\omega^i), \\
	%DIFDELCMD < 									&H(X) = \begin{bmatrix} 1 & \gamma & \gamma^2 \end{bmatrix}
	%DIFDELCMD < 									\begin{bmatrix}
	%DIFDELCMD < 										a(X) & v(X) & 0 \\
	%DIFDELCMD < 										a'(X) & v'(X) & 0 \\
	%DIFDELCMD < 										\bar{a}(X) & \bar{v}(X) & \bar{\op}(X)
	%DIFDELCMD < 									\end{bmatrix}
	%DIFDELCMD < 									\begin{bmatrix}
	%DIFDELCMD < 										1 \\
	%DIFDELCMD < 										\beta \\
	%DIFDELCMD < 										\beta^2
	%DIFDELCMD < 									\end{bmatrix}, \\
	%DIFDELCMD < 									&G(X) = A(X) + \beta V(X) + \beta^2 \op(X), \\
	%DIFDELCMD < 									&Q(X) = \big(H(X^3) - G(X) - \gamma G(\omega^m X) - \gamma^2 G(\omega^{2m} X)\big)/Z(X), \\
	%DIFDELCMD < 									&f(X) = G(X) + \beta^3 t(X),\, g(X) = A^{\ast}(X) + \beta V^{\ast}(X) + \beta^2 \op^{\ast}(X) + \beta^3 t^{\ast}(X), \\
	%DIFDELCMD < 									&z(X) = \sum_{i=1}^k \lambda_i(X)\prod_{j=1}^{i-1} (\alpha - g(\omega^j))/(\alpha - f(\omega^j)), \\
	%DIFDELCMD < 									&Q_1(X) = \frac{1}{\mathbb{Z}_\setH(X)}\Big( (\alpha - g(X))z(\omega X) - (\alpha - f(X))z(X) \\
	%DIFDELCMD < 									&\qquad\qquad + \beta Z_2(X)(A^\ast(\omega X) - A^\ast(X) - \delta_A^\ast(X))\Big), \\
	%DIFDELCMD < 									&Q_2(X) = \frac{1}{Z_2(X)}\Big((A^\ast(\omega X) - A^\ast(X))
	%DIFDELCMD < 									+ \beta(t^\ast(\omega X) - t^\ast(X)-\delta_T^\ast(X)) \\
	%DIFDELCMD < 									&\qquad\qquad + \beta^2 (\op^\ast(X) - 1)(V^\ast(\omega X) - V^\ast(X))\Big)
	%DIFDELCMD < 								\end{align*}
	%DIFDELCMD < 								\item $\prover\rightarrow \verifier$: Send $\gone{Z_1(X)}$, $\gone{Z_2(X)}$, $\gone{Q(X)}$, $\gone{z(X)}$,
	%DIFDELCMD < 								$\gone{Q(X)}$, $\gone{Q_1(X)}$, $\gone{Q_2(X)}$.
	%DIFDELCMD < 								\item $\verifier\rightarrow\prover$: $s\gets \F$.
	%DIFDELCMD < 								\item $\prover\rightarrow\verifier$: Send evaluations $\val{Z_1}{s}=Z_1(s)$, $\val{Z_2}{s}=Z_2(s)$,
	%DIFDELCMD < 								$\val{G}{s}=G(s)$, $\val{Q}{s}=Q(s)$, $\val{Z}{s}=Z(s)$, $\val{A^\ast}{s}=A^\ast(s)$, $\val{V^\ast}{s}=V^\ast(s)$,
	%DIFDELCMD < 								$\val{\op^\ast}{s}=\op^\ast(s)$, $\val{t}{s}=t(s)$, $\val{t^\ast}{s}=t^\ast(s)$, $\val{z}{s}=z(s)$, $\val{Q_1}{s}=Q_1(s)$,
	%DIFDELCMD < 								$\val{Q_2}{s}=Q_2(s)$, $\val{H}{s^3}=H(s^3)$, $\val{G}{\omega^m s}=G(\omega^m s)$, $\val{\delta^\ast_A}{s}=\delta_A^\ast(s)$,
	%DIFDELCMD < 								$\val{\delta_T^\ast}{s}=\delta_T^\ast(s)$
	%DIFDELCMD < 								$\val{G}{\omega^{2m} s}=G(\omega^{2m} s)$,
	%DIFDELCMD < 								$\val{A^\ast}{\omega s}=A^\ast(\omega s)$, $\val{V^\ast}{\omega s}=V^\ast(s)$, $\val{t^\ast}{\omega s}=t^\ast(\omega s)$,
	%DIFDELCMD < 								$\val{z}{\omega s}=z(\omega s)$, $\val{A^\ast}{\omega}=A^\ast(\omega)$.
	%DIFDELCMD < 								
	%DIFDELCMD < 								\item $\verifier\rightarrow\prover$: Send $r \gets \F$.
	%DIFDELCMD < 								\item $\prover$ computes: Compute batched $\kzg$ proofs:
	%DIFDELCMD < 								\begin{alignat*}{3}
	%DIFDELCMD < 									&P_1 && &\quad=\quad  && &Z_1+r Z_2 + r^2 G + r^3 Q + r^4 Z + r^5 A^{\ast} + r^6 V^{\ast} + r^7 \op^{\ast} \\
	%DIFDELCMD < 									& && & && &\quad  + r^8 t + r^9 t^{\ast} + r^{10} z + r^{11} Q_1 + r^{12} Q_2 + r^{13} \delta_A^{\ast} + r^{14} \delta_T^{\ast} \\
	%DIFDELCMD < 									&P_2 && &\quad=\quad && &A^{\ast} + r V^{\ast} + r^2 z \\
	%DIFDELCMD < 									&\Pi_s && &\quad=\quad && &\kzgprove(P_1, s) \\
	%DIFDELCMD < 									&\Pi_{\omega s} && &\quad=\quad && &\kzgprove(P_2, \omega s) \\
	%DIFDELCMD < 									&\Pi_{s^3} && &\quad=\quad && &\kzgprove(H, s^3) \\
	%DIFDELCMD < 									&\Pi_{\{\omega^m s, \omega^{2m} s\}} && &\quad=\quad && &\kzgprove(G, (\omega^m s, \omega^{2m} s))
	%DIFDELCMD < 								\end{alignat*}
	%DIFDELCMD < 								\item $\prover\rightarrow \verifier$: Send $\Pi_s,\Pi_{\omega s}, \Pi_{s^3},\Pi_{\{\omega^m s, \omega^{2m} s\}}$ and
	%DIFDELCMD < 								$\val{H}{s^3},\val{G}{\omega^m s}, \val{G}{\omega^{2m} s}$.
	%DIFDELCMD < 								
	%DIFDELCMD < 								\item $\verifier$ computes: Compute commitments and evaluations for linear combinations.
	%DIFDELCMD < 								\begin{align*}
	%DIFDELCMD < 									\gone{G(X)} &= \gone{A(X)}+\beta\gone{V(X)}+\beta^2\gone{\op(X)}, \\
	%DIFDELCMD < 									\gone{H(X)} &= \begin{bmatrix} 1 & \gamma & \gamma^2 \end{bmatrix}
	%DIFDELCMD < 									\begin{bmatrix}
	%DIFDELCMD < 										\gone{a(X)} & \gone{v(X)} & \gone{0} \\
	%DIFDELCMD < 										\gone{a'(X)} & \gone{v'(X)} & \gone{0} \\
	%DIFDELCMD < 										\gone{\bar{a}(X)} & \gone{\bar{v}(X)} & \gone{\bar{\op}(X)}
	%DIFDELCMD < 									\end{bmatrix}
	%DIFDELCMD < 									\begin{bmatrix}
	%DIFDELCMD < 										1 \\
	%DIFDELCMD < 										\beta \\
	%DIFDELCMD < 										\beta^2
	%DIFDELCMD < 									\end{bmatrix}, \\
	%DIFDELCMD < 									\gone{P_1(X)} &= \gone{Z_1(X)}+r \gone{Z_2(X)} + r^2 \gone{G(X)} + r^3 \gone{Q(X)} + r^4 \gone{Z(X)} + r^5 \gone{A^{\ast}(X)} \\
	%DIFDELCMD < 									&\qquad + r^6 \gone{V^{\ast}(X)} + r^7 \gone{\op^{\ast}(X)} + r^8 \gone{t(X)} + r^9 \gone{t^{\ast}(X)} + r^{10} \gone{z(X)} \\
	%DIFDELCMD < 									&\qquad + r^{11} \gone{Q_1(X)} + r^{12} \gone{Q_2(X)} + r^{13} \gone{\delta_A^{\ast}(X)} + r^{14} \gone{\delta_T^{\ast}(X)}, \\
	%DIFDELCMD < 									\gone{P_2(X)} &= \gone{A^\ast(X)} + r\gone{V^\ast(X)} + r^2\gone{z(X)},\\
	%DIFDELCMD < 									\val{P_1}{s} &= \val{Z_1}{s}+r \val{Z_2}{s} + r^2 \val{G}{s} + r^3 \val{Q}{s} + r^4 \val{Z}{s} + r^5 \val{A^{\ast}}{s} \\
	%DIFDELCMD < 									&\qquad + r^6 \val{V^{\ast}}{s} + r^7 \val{\op^{\ast}}{s} + r^8 \val{t}{s} + r^9 \val{t^{\ast}}{s} + r^{10} \val{z}{s} \\
	%DIFDELCMD < 									&\qquad + r^{11} \val{Q_1}{s} + r^{12} \val{Q_2}{s} + r^{13} \val{\delta_A^{\ast}}{s} + r^{14} \val{\delta_T^{\ast}}{s}, \\
	%DIFDELCMD < 									\val{P_2}{\omega s} &= \val{A^\ast}{s} + r\val{V^\ast}{s} + r^2\val{z}{s}. \\
	%DIFDELCMD < 									\val{g}{s} &= \val{A^\ast}{s} + \beta \val{V^\ast}{s} + \beta^2 \val{\op^\ast}{s} + \beta^3 \val{t^\ast}{s}, \\
	%DIFDELCMD < 									\val{f}{s} &= \val{G}{s} + \beta^3 \val{t}{s}.
	%DIFDELCMD < 								\end{align*}
	%DIFDELCMD < 								
	%DIFDELCMD < 								\item $\verifier$ checks polynomial identities at $s$:
	%DIFDELCMD < 								\begin{alignat*}{3}
	%DIFDELCMD < 									&\val{Q}{s}\val{Z}{s} && &\quad \stackrel{?}{=}\quad  && &\val{H}{s^3} - \val{G}{s} - \gamma \val{G}{\omega^m s} - \gamma^2 \val{G}{\omega^{2m} s} \\
	%DIFDELCMD < 									&\val{Q_1}{s}(s^k-1) && &\quad \stackrel{?}{=}\quad && &(\alpha - \val{g}{s})\val{z}{\omega s} - (\alpha - \val{f}{s})\val{z}{s}
	%DIFDELCMD < 									+\beta \val{Z_2}{s}(\val{A^\ast}{\omega s}-\val{A^\ast}{s}-\val{\delta_A^\ast}{s}). \\
	%DIFDELCMD < 									&\val{Q_2}{s}\val{Z_2}{s} && &\quad \stackrel{?}{=}\quad && &(\val{A^\ast}{\omega s} - \val{A^\ast}{s}) + \beta (\val{t^\ast}{s} - \val{t}{s} - \val{\delta}{s}) \\
	%DIFDELCMD < 									& && & && &\qquad + \beta^2 (\val{\op^\ast}{s} - 1)(\val{V^\ast}{\omega s}-\val{V^\ast}{s}) \\
	%DIFDELCMD < 									&\val{Z_1}{s}\val{Z_2}{s} && &\quad \stackrel{?}{=}\quad && &s^k - 1 \\
	%DIFDELCMD < 								\end{alignat*}
	%DIFDELCMD < 								\item $\verifier$ checks evaluation proofs:
	%DIFDELCMD < 								\begin{alignat*}{3}
	%DIFDELCMD < 									&\kzgverify(\gone{P_1(X)},\val{P_1}{s},s, \Pi_s) && &\quad\stackrel{?}{=}\quad && &1 \\
	%DIFDELCMD < 									&\kzgverify(\gone{P_2(X)},\val{P_2}{\omega s},\Pi_{\omega s}) && &\quad\stackrel{?}{=}\quad && &1 \\
	%DIFDELCMD < 									&\kzgverify(\gone{H(X)},\val{H}{s^3},\Pi_{s^3}) && &\quad\stackrel{?}{=}\quad && &1 \\
	%DIFDELCMD < 									&\kzgverify(\gone{G(X)},(\val{G}{\omega^m s},\val{G}{\omega^{2m} s}),
	%DIFDELCMD < 									(\omega^m s, \omega^{2m} s), \Pi_{\{\omega^m s, \omega^{2m} s\}}) && &\quad\stackrel{?}{=}\quad && &1 \\
	%DIFDELCMD < 									&\kzgverify(\gone{A^\ast(X)}, \val{A^\ast}{\omega}, \omega, \Pi_\omega) && &\quad\stackrel{?}{=}\quad && &1
	%DIFDELCMD < 								\end{alignat*}
	%DIFDELCMD < 								\item $\verifier$ enforces range checks: Invoke the lookup argument $\varPi_{\mathsf{lookup}}$ to check that
	%DIFDELCMD < 								polynomials $\delta_A^\ast(X)$, $\delta_T^\ast(X)$ and $t^\ast(X)$ encode values in $[0,N]$.
	%DIFDELCMD < 								\item $\verifier$ outputs: The verifier outputs accept (1) if all the preceeding checks pass, else it outputs reject (0).
	%DIFDELCMD < 							\end{enumerate}
	%DIFDELCMD < 						}
	%DIFDELCMD < 					\end{subfigure}
	%DIFDELCMD < 					\caption{Argument of Knowledge for Verifiable RAM}
	%DIFDELCMD < 					\label{fig:aok-vram}
	%DIFDELCMD < 				\end{figure}
	%DIFDELCMD < 			%DIFDELCMD < \end{comment}
	%DIFDELCMD < 			%%%
	\DIFdelend \DIFaddbegin \begin{comment}
	\begin{enumerate}[leftmargin=2em]
	\item $\prover\rightarrow\verifier$: For the transcript $\tr^\ast=(\vec{t^\ast},\vec{\op^\ast},\vec{A^\ast},\vec{V^\ast})$,
	the prover computes encoding polynomials $\wt{\tr^\ast}=({t^\ast}(X),\op^\ast(X), A^\ast(X),V^\ast(X))$. Next, the prover
	computes sets $I_1$ and $I_2$ as in Section \ref{subsec:encoded-relations}, and then computes polynomials
	$Z_b(X)=\prod_{i\in I_b}(X-\omega^i)$ for $b=1,2$.
	The prover also interpolates polynomials $\delta^\ast_A(X)$ and $\delta^\ast_T(X)$ satisfying
	$\delta^\ast_A(\omega^i) = A^\ast_{i+1} - A^\ast_i$ for $i\in I_1$ and $\delta^\ast_T(\omega^i)=t^\ast_{i+1}-t^\ast_i$ for $i\not\in I_1$.
	The prover sends all the above polynomials to the verifier.
	\item $\verifier$ computes: The verifier checks relations (C1)-(C6) in Lemma \ref{lem:addr-ordered-transcript}. The range constraints
	in (C7) can be verified using polynomial protocols in sub-vector lookup arguments such as Caulk+ ~\cite{EPRINT:PosKat22}, CQ
	~\cite{EPRINT:EagFioGab22}.
	\end{enumerate}
	
	
	\subsection{Succinct Argument for Verifiable RAM}\label{subsec:succ-args}
	Polynomial protocols can be compiled into succinct arguments of knowledge using an extractable polynomial commitment scheme. At a
	high level the compilation involves the prover sending ``commitments'' to the message polynomials, whereas the verifier checks
	the polynomial identities probabilistically by requesting evaluation proofs of the polynomials at random points.
	We complile the polynomial protocol for RAM to a succinct argument of knowledge in the Algebraic Group Model (AGM)
	using $\kzg$ as the extractable polynomial commitment scheme. We also leverage homomorphism of $\kzg$ scheme
	to avoid sending commitments and evaluations for polynomials which are linear combinations of previously committed polynomials.
	We now present an argument for verifiable RAM, which combines the polynomial protocols in this section, and instantiates
	them using $\kzg$ commitments. We make standard optimisations of batching several identity checks into one where applicable.
	Let $\srs=\{\gone{\tau^i},\gtwo{\tau^i}\}_{i=0}^d$
	denote the $\kzg$ setup parameters for degree $d\geq k$ for the bilinear group $\mathsf{BG}=(\Gone,\Gtwo,\gone{1},\gtwo{1},e)$. Let
	$\F=\F_p$ be the finite field where $p$ is the order of the groups.
	
	
	\begin{figure}[t]
	\begin{subfigure}{\linewidth}
	\centering
	{\footnotesize
	\begin{enumerate}[leftmargin=1em]
	\item $\prover\rightarrow\verifier$: Send polynomial commitments $\gone{a(X)}$, $\gone{v(X)}$, $\gone{a'(X)}$, $\gone{v'(X)}$,
	$\gone{\bar{\op}(X)}$, $\gone{\bar{a}(X)}, \gone{\bar{v}(X)}$, $\gone{\op(X)}$, $\gone{A(X)}$, $\gone{V(X)}$, $\gone{t^\ast(X)}$,
	$\gone{\op^\ast(X)}$, $\gone{A^\ast(X)}$, $\gone{V^\ast(X)}$, $\gone{\delta_A^\ast(X)}$, $\gone{\delta_T^\ast(X)}$.
	\item $\verifier\rightarrow\prover$: Send $\alpha,\beta,\gamma\gets \F$.
	\item $\prover$ computes: Compute sets $I_1=\{i\in [k]: A^\ast_i\neq A^\ast_{i+1}\}$, and $I_2=[k]\setminus I_1$. Next, compute
	polynomials:
	\begin{align*}
	&Z_1(X)=\prod_{i\in I_1}(X-\omega^i),\quad Z_2(X)=\prod_{i\in I_2}(X-\omega^i), \\
	&H(X) = \begin{bmatrix} 1 & \gamma & \gamma^2 \end{bmatrix}
	\begin{bmatrix}
	a(X) & v(X) & 0 \\
	a'(X) & v'(X) & 0 \\
	\bar{a}(X) & \bar{v}(X) & \bar{\op}(X)
	\end{bmatrix}
	\begin{bmatrix}
	1 \\
	\beta \\
	\beta^2
	\end{bmatrix}, \\
	&G(X) = A(X) + \beta V(X) + \beta^2 \op(X), \\
	&Q(X) = \big(H(X^3) - G(X) - \gamma G(\omega^m X) - \gamma^2 G(\omega^{2m} X)\big)/Z(X), \\
	&f(X) = G(X) + \beta^3 t(X),\, g(X) = A^{\ast}(X) + \beta V^{\ast}(X) + \beta^2 \op^{\ast}(X) + \beta^3 t^{\ast}(X), \\
	&z(X) = \sum_{i=1}^k \lambda_i(X)\prod_{j=1}^{i-1} (\alpha - g(\omega^j))/(\alpha - f(\omega^j)), \\
	&Q_1(X) = \frac{1}{\mathbb{Z}_\setH(X)}\Big( (\alpha - g(X))z(\omega X) - (\alpha - f(X))z(X) \\
	&\qquad\qquad + \beta Z_2(X)(A^\ast(\omega X) - A^\ast(X) - \delta_A^\ast(X))\Big), \\
	&Q_2(X) = \frac{1}{Z_2(X)}\Big((A^\ast(\omega X) - A^\ast(X))
	+ \beta(t^\ast(\omega X) - t^\ast(X)-\delta_T^\ast(X)) \\
	&\qquad\qquad + \beta^2 (\op^\ast(X) - 1)(V^\ast(\omega X) - V^\ast(X))\Big)
	\end{align*}
	\item $\prover\rightarrow \verifier$: Send $\gone{Z_1(X)}$, $\gone{Z_2(X)}$, $\gone{Q(X)}$, $\gone{z(X)}$,
	$\gone{Q(X)}$, $\gone{Q_1(X)}$, $\gone{Q_2(X)}$.
	\item $\verifier\rightarrow\prover$: $s\gets \F$.
	\item $\prover\rightarrow\verifier$: Send evaluations $\val{Z_1}{s}=Z_1(s)$, $\val{Z_2}{s}=Z_2(s)$,
	$\val{G}{s}=G(s)$, $\val{Q}{s}=Q(s)$, $\val{Z}{s}=Z(s)$, $\val{A^\ast}{s}=A^\ast(s)$, $\val{V^\ast}{s}=V^\ast(s)$,
	$\val{\op^\ast}{s}=\op^\ast(s)$, $\val{t}{s}=t(s)$, $\val{t^\ast}{s}=t^\ast(s)$, $\val{z}{s}=z(s)$, $\val{Q_1}{s}=Q_1(s)$,
	$\val{Q_2}{s}=Q_2(s)$, $\val{H}{s^3}=H(s^3)$, $\val{G}{\omega^m s}=G(\omega^m s)$, $\val{\delta^\ast_A}{s}=\delta_A^\ast(s)$,
	$\val{\delta_T^\ast}{s}=\delta_T^\ast(s)$
	$\val{G}{\omega^{2m} s}=G(\omega^{2m} s)$,
	$\val{A^\ast}{\omega s}=A^\ast(\omega s)$, $\val{V^\ast}{\omega s}=V^\ast(s)$, $\val{t^\ast}{\omega s}=t^\ast(\omega s)$,
	$\val{z}{\omega s}=z(\omega s)$, $\val{A^\ast}{\omega}=A^\ast(\omega)$.
	
	\item $\verifier\rightarrow\prover$: Send $r \gets \F$.
	\item $\prover$ computes: Compute batched $\kzg$ proofs:
	\begin{alignat*}{3}
	&P_1 && &\quad=\quad  && &Z_1+r Z_2 + r^2 G + r^3 Q + r^4 Z + r^5 A^{\ast} + r^6 V^{\ast} + r^7 \op^{\ast} \\
	& && & && &\quad  + r^8 t + r^9 t^{\ast} + r^{10} z + r^{11} Q_1 + r^{12} Q_2 + r^{13} \delta_A^{\ast} + r^{14} \delta_T^{\ast} \\
	&P_2 && &\quad=\quad && &A^{\ast} + r V^{\ast} + r^2 z \\
	&\Pi_s && &\quad=\quad && &\kzgprove(P_1, s) \\
	&\Pi_{\omega s} && &\quad=\quad && &\kzgprove(P_2, \omega s) \\
	&\Pi_{s^3} && &\quad=\quad && &\kzgprove(H, s^3) \\
	&\Pi_{\{\omega^m s, \omega^{2m} s\}} && &\quad=\quad && &\kzgprove(G, (\omega^m s, \omega^{2m} s))
	\end{alignat*}
	\item $\prover\rightarrow \verifier$: Send $\Pi_s,\Pi_{\omega s}, \Pi_{s^3},\Pi_{\{\omega^m s, \omega^{2m} s\}}$ and
	$\val{H}{s^3},\val{G}{\omega^m s}, \val{G}{\omega^{2m} s}$.
	
	\item $\verifier$ computes: Compute commitments and evaluations for linear combinations.
	\begin{align*}
	\gone{G(X)} &= \gone{A(X)}+\beta\gone{V(X)}+\beta^2\gone{\op(X)}, \\
	\gone{H(X)} &= \begin{bmatrix} 1 & \gamma & \gamma^2 \end{bmatrix}
	\begin{bmatrix}
	\gone{a(X)} & \gone{v(X)} & \gone{0} \\
	\gone{a'(X)} & \gone{v'(X)} & \gone{0} \\
	\gone{\bar{a}(X)} & \gone{\bar{v}(X)} & \gone{\bar{\op}(X)}
	\end{bmatrix}
	\begin{bmatrix}
	1 \\
	\beta \\
	\beta^2
	\end{bmatrix}, \\
	\gone{P_1(X)} &= \gone{Z_1(X)}+r \gone{Z_2(X)} + r^2 \gone{G(X)} + r^3 \gone{Q(X)} + r^4 \gone{Z(X)} + r^5 \gone{A^{\ast}(X)} \\
	&\qquad + r^6 \gone{V^{\ast}(X)} + r^7 \gone{\op^{\ast}(X)} + r^8 \gone{t(X)} + r^9 \gone{t^{\ast}(X)} + r^{10} \gone{z(X)} \\
	&\qquad + r^{11} \gone{Q_1(X)} + r^{12} \gone{Q_2(X)} + r^{13} \gone{\delta_A^{\ast}(X)} + r^{14} \gone{\delta_T^{\ast}(X)}, \\
	\gone{P_2(X)} &= \gone{A^\ast(X)} + r\gone{V^\ast(X)} + r^2\gone{z(X)},\\
	\val{P_1}{s} &= \val{Z_1}{s}+r \val{Z_2}{s} + r^2 \val{G}{s} + r^3 \val{Q}{s} + r^4 \val{Z}{s} + r^5 \val{A^{\ast}}{s} \\
	&\qquad + r^6 \val{V^{\ast}}{s} + r^7 \val{\op^{\ast}}{s} + r^8 \val{t}{s} + r^9 \val{t^{\ast}}{s} + r^{10} \val{z}{s} \\
	&\qquad + r^{11} \val{Q_1}{s} + r^{12} \val{Q_2}{s} + r^{13} \val{\delta_A^{\ast}}{s} + r^{14} \val{\delta_T^{\ast}}{s}, \\
	\val{P_2}{\omega s} &= \val{A^\ast}{s} + r\val{V^\ast}{s} + r^2\val{z}{s}. \\
	\val{g}{s} &= \val{A^\ast}{s} + \beta \val{V^\ast}{s} + \beta^2 \val{\op^\ast}{s} + \beta^3 \val{t^\ast}{s}, \\
	\val{f}{s} &= \val{G}{s} + \beta^3 \val{t}{s}.
	\end{align*}
	
	\item $\verifier$ checks polynomial identities at $s$:
	\begin{alignat*}{3}
	&\val{Q}{s}\val{Z}{s} && &\quad \stackrel{?}{=}\quad  && &\val{H}{s^3} - \val{G}{s} - \gamma \val{G}{\omega^m s} - \gamma^2 \val{G}{\omega^{2m} s} \\
	&\val{Q_1}{s}(s^k-1) && &\quad \stackrel{?}{=}\quad && &(\alpha - \val{g}{s})\val{z}{\omega s} - (\alpha - \val{f}{s})\val{z}{s}
	+\beta \val{Z_2}{s}(\val{A^\ast}{\omega s}-\val{A^\ast}{s}-\val{\delta_A^\ast}{s}). \\
	&\val{Q_2}{s}\val{Z_2}{s} && &\quad \stackrel{?}{=}\quad && &(\val{A^\ast}{\omega s} - \val{A^\ast}{s}) + \beta (\val{t^\ast}{s} - \val{t}{s} - \val{\delta}{s}) \\
	& && & && &\qquad + \beta^2 (\val{\op^\ast}{s} - 1)(\val{V^\ast}{\omega s}-\val{V^\ast}{s}) \\
	&\val{Z_1}{s}\val{Z_2}{s} && &\quad \stackrel{?}{=}\quad && &s^k - 1 \\
	\end{alignat*}
	\item $\verifier$ checks evaluation proofs:
	\begin{alignat*}{3}
	&\kzgverify(\gone{P_1(X)},\val{P_1}{s},s, \Pi_s) && &\quad\stackrel{?}{=}\quad && &1 \\
	&\kzgverify(\gone{P_2(X)},\val{P_2}{\omega s},\Pi_{\omega s}) && &\quad\stackrel{?}{=}\quad && &1 \\
	&\kzgverify(\gone{H(X)},\val{H}{s^3},\Pi_{s^3}) && &\quad\stackrel{?}{=}\quad && &1 \\
	&\kzgverify(\gone{G(X)},(\val{G}{\omega^m s},\val{G}{\omega^{2m} s}),
	(\omega^m s, \omega^{2m} s), \Pi_{\{\omega^m s, \omega^{2m} s\}}) && &\quad\stackrel{?}{=}\quad && &1 \\
	&\kzgverify(\gone{A^\ast(X)}, \val{A^\ast}{\omega}, \omega, \Pi_\omega) && &\quad\stackrel{?}{=}\quad && &1
	\end{alignat*}
	\item $\verifier$ enforces range checks: Invoke the lookup argument $\varPi_{\mathsf{lookup}}$ to check that
	polynomials $\delta_A^\ast(X)$, $\delta_T^\ast(X)$ and $t^\ast(X)$ encode values in $[0,N]$.
	\item $\verifier$ outputs: The verifier outputs accept (1) if all the preceeding checks pass, else it outputs reject (0).
	\end{enumerate}
	}
	\end{subfigure}
	\caption{Argument of Knowledge for Verifiable RAM}
	\label{fig:aok-vram}
	\end{figure}
	\end{comment}
	\DIFaddend 
	
	\DIFdelbegin %DIFDELCMD < \begin{comment}%DIFDELCMD < 
	%DIFDELCMD < 				Let $\LLconcat$ denote the language consisting of polynomial tuples $(a(X)$, $b(X)$, $c(X)$, $v(X))$ satisfying the identities
	%DIFDELCMD < 				in Lemma ~\ref{lem:vec-concatenation}. To check membership of $(T,\vec{o},T',\tr)$ in the language $\LTr$, let $\wt{T}=(a(X),v(X))$, $\wt{T'}=(a'(X),v'(X))$,
	%DIFDELCMD < 				$\wt{O}=(\bar{\op}(X),\bar{a}(X),\bar{v}(X))$ and $\wt{\tr}=(t(X),o(X),A(X),V(X))$ be the polynomial
	%DIFDELCMD < 				encodings of $T,T',\vec{o}$ and $\tr$ respectively.
	%DIFDELCMD < 				From the construction of time ordered transcript $\tr$ outlined
	%DIFDELCMD < 				in Section \ref{subsec:transcripts}, we see that the equivalent constraints on the encodings are given by:
	%DIFDELCMD < 				\begin{align*}\label{eq:tot-poly-constraints}
	%DIFDELCMD < 					t(X) &= \sum_{i=1}^k i\lambda_i(X) \quad (= \mathsf{Enc}(\setind_k)) \\
	%DIFDELCMD < 					(0, \bar{\op}(X), 0, o(X)) &\in \LLconcat \\
	%DIFDELCMD < 					(a(X), \bar{a}(X), a'(X), A(X)) &\in \LLconcat \\
	%DIFDELCMD < 					(v(X), \bar{v}(X), v'(X), V(X)) &\in \LLconcat
	%DIFDELCMD < 				\end{align*}
	%DIFDELCMD < 				
	%DIFDELCMD < 				We can probabilistically combine the different polynomial identities into one and obtain the following:
	%DIFDELCMD < 				\begin{lemma}
	%DIFDELCMD < 					Let the polynomial $Z(X)=\prod_{i\in [m]}(X-\omega^i)$ and $\rho=\omega^m$ be as before.
	%DIFDELCMD < 					Then, we have $(T,\vec{o},T',\tr)\in \LTr$ if and only if the encoding polynomials as defined above satisfy
	%DIFDELCMD < 					the identity $G_\gamma(X) = 0  \text{ mod } Z(X)$ and $\evalH{t}=(1,\ldots,k)$ with overwhelming
	%DIFDELCMD < 					probability over the choice of $\gamma\gets \F$. Here the polynomial $G_\gamma(X)$ is defined as:
	%DIFDELCMD < 					\begin{multline*}
	%DIFDELCMD < 						G_\gamma(X) = o(X) + \gamma(\bar{\op}(X^3) - o(\rho X)) + \gamma^2 o(\rho^2 X) \\
	%DIFDELCMD < 						+ \gamma^3(a(X^3) - A(X)) + \gamma^4(\bar{a}(X^3) - A(\rho X)) + \gamma^5(a'(X^3) - A(\rho^2 X)) \\
	%DIFDELCMD < 						+ \gamma^6(v(X^3) - V(X)) + \gamma^7(\bar{v}(X^3) - V(\rho X)) + \gamma^8(v'(X^3) - V(\rho^2 X))
	%DIFDELCMD < 					\end{multline*}
	%DIFDELCMD < 				\end{lemma}
	%DIFDELCMD < 				
	%DIFDELCMD < 				Next, we consider the language $\Lperm$ consisting of pairs $(\tr, \tr^\ast)$ of $k$-length transcripts such
	%DIFDELCMD < 				that $\tr^\ast=\sigma(\tr)$ for some permutation $\sigma:[k]\rightarrow [k]$. We now describe constraints
	%DIFDELCMD < 				to check the same using polynomial encodings of the transcripts. Let encodings $\wt{\tr},\wt{\tr}^\ast$ be
	%DIFDELCMD < 				given by polynomials as below:
	%DIFDELCMD < 				\begin{align*}\label{eq:tr-encodings}
	%DIFDELCMD < 					\wt{\tr} &= (t(X),\op(X),A(X),V(X)) \\
	%DIFDELCMD < 					\wt{\tr}^\ast &= (t^\ast(X), \op^\ast(X), A^\ast(X), V^\ast(X))
	%DIFDELCMD < 				\end{align*}
	%DIFDELCMD < 				We need to show that for some $\sigma:[k]\rightarrow [k]$, $\evalH{p^\ast}=\sigma(\evalH{p})$ for $p\in \{t,\op,A,V\}$.
	%DIFDELCMD < 				Again, for $\gamma\gets \F$, with overwhelming probability it is equivalent to establishing that polynomial
	%DIFDELCMD < 				$f^\ast(X)=t^\ast(X)+\gamma \op^\ast(X) + \gamma^2 A^\ast(X) + \gamma^3 V^\ast(X)$ encodes a permutation of
	%DIFDELCMD < 				vector encoded by $f(X)=t(X)+\gamma \op(X) + \gamma^2 A(X) + \gamma^3 V(X)$. We recall the following variation
	%DIFDELCMD < 				of the grand product argument to show that two polynomials encode vectors which are permutations of each other.
	%DIFDELCMD < 			%DIFDELCMD < \end{comment}
	%DIFDELCMD < 			%%%
	\DIFdelend \DIFaddbegin \begin{comment}
	Let $\LLconcat$ denote the language consisting of polynomial tuples $(a(X)$, $b(X)$, $c(X)$, $v(X))$ satisfying the identities
	in Lemma ~\ref{lem:vec-concatenation}. To check membership of $(T,\vec{o},T',\tr)$ in the language $\LTr$, let $\wt{T}=(a(X),v(X))$, $\wt{T'}=(a'(X),v'(X))$,
	$\wt{O}=(\bar{\op}(X),\bar{a}(X),\bar{v}(X))$ and $\wt{\tr}=(t(X),o(X),A(X),V(X))$ be the polynomial
	encodings of $T,T',\vec{o}$ and $\tr$ respectively.
	From the construction of time ordered transcript $\tr$ outlined
	in Section \ref{subsec:transcripts}, we see that the equivalent constraints on the encodings are given by:
	\begin{align*}\label{eq:tot-poly-constraints}
	t(X) &= \sum_{i=1}^k i\lambda_i(X) \quad (= \mathsf{Enc}(\setind_k)) \\
	(0, \bar{\op}(X), 0, o(X)) &\in \LLconcat \\
	(a(X), \bar{a}(X), a'(X), A(X)) &\in \LLconcat \\
	(v(X), \bar{v}(X), v'(X), V(X)) &\in \LLconcat
	\end{align*}
	
	We can probabilistically combine the different polynomial identities into one and obtain the following:
	\begin{lemma}
	Let the polynomial $Z(X)=\prod_{i\in [m]}(X-\omega^i)$ and $\rho=\omega^m$ be as before.
	Then, we have $(T,\vec{o},T',\tr)\in \LTr$ if and only if the encoding polynomials as defined above satisfy
	the identity $G_\gamma(X) = 0  \text{ mod } Z(X)$ and $\evalH{t}=(1,\ldots,k)$ with overwhelming
	probability over the choice of $\gamma\gets \F$. Here the polynomial $G_\gamma(X)$ is defined as:
	\begin{multline*}
	G_\gamma(X) = o(X) + \gamma(\bar{\op}(X^3) - o(\rho X)) + \gamma^2 o(\rho^2 X) \\
	+ \gamma^3(a(X^3) - A(X)) + \gamma^4(\bar{a}(X^3) - A(\rho X)) + \gamma^5(a'(X^3) - A(\rho^2 X)) \\
	+ \gamma^6(v(X^3) - V(X)) + \gamma^7(\bar{v}(X^3) - V(\rho X)) + \gamma^8(v'(X^3) - V(\rho^2 X))
	\end{multline*}
	\end{lemma}
	
	Next, we consider the language $\Lperm$ consisting of pairs $(\tr, \tr^\ast)$ of $k$-length transcripts such
	that $\tr^\ast=\sigma(\tr)$ for some permutation $\sigma:[k]\rightarrow [k]$. We now describe constraints
	to check the same using polynomial encodings of the transcripts. Let encodings $\wt{\tr},\wt{\tr}^\ast$ be
	given by polynomials as below:
	\begin{align*}\label{eq:tr-encodings}
	\wt{\tr} &= (t(X),\op(X),A(X),V(X)) \\
	\wt{\tr}^\ast &= (t^\ast(X), \op^\ast(X), A^\ast(X), V^\ast(X))
	\end{align*}
	We need to show that for some $\sigma:[k]\rightarrow [k]$, $\evalH{p^\ast}=\sigma(\evalH{p})$ for $p\in \{t,\op,A,V\}$.
	Again, for $\gamma\gets \F$, with overwhelming probability it is equivalent to establishing that polynomial
	$f^\ast(X)=t^\ast(X)+\gamma \op^\ast(X) + \gamma^2 A^\ast(X) + \gamma^3 V^\ast(X)$ encodes a permutation of
	vector encoded by $f(X)=t(X)+\gamma \op(X) + \gamma^2 A(X) + \gamma^3 V(X)$. We recall the following variation
	of the grand product argument to show that two polynomials encode vectors which are permutations of each other.
	\end{comment}
	\DIFaddend 
%	
%	
%	
%	
%	
%	
%	
%	
%	
%	
%	
	\section{Improved Batching-Efficient RAM}\label{sec:batch-efficient-ram}
	%DIF < \input{batching-efficient-ram}
	%DIF > 	\input{batching-efficient-ram-revised}
	
	
	\DIFdelbegin \DIFdel{The construction in the previous section results in prover complexity which is quasi-linear in both the
		size of the RAM and the number of operations.
		Our goal in this section is to achieve prover complexity which is }%DIFDELCMD < {\em %%%
	\DIFdel{sublinear}%DIFDELCMD < } %%%
	\DIFdel{in the size of the RAM.
		In what follows, let $N$ denote the size of the RAM (upper-case to signify it's large) and $m$ denote the number
		of operations in a batch. We will use a vectors in $\F^N$ to denote the ``large'' RAMs, where index column is implicitly
		assumed to be $(1,\ldots,N)$.
		Let $\vec{T},\vec{T'}\in \F^N$ denote the initial and final RAM states, and let $\vec{o}$ be
		a sequence of $m$ operations which updates $\vec{T}$ to $\vec{T}'$. Let $\vec{a}\in \F^m$ denote the vector
		of RAM indices referenced by the operations in $\vec{o}$, i.e, $a_i$ is the index referenced by $i^{th}$ operation.
		To prove the transformation of $\vec{T}$ to $\vec{T}'$ via operation sequence $\vec{o}$, we proceed as follows:
	}%DIFDELCMD < \begin{itemize}[leftmargin=2em, label=-]
	%DIFDELCMD < 				\item %%%
	\DIFdel{We isolate sub-tables $S=(\vec{a},\vec{v})$ and $S'=(\vec{a},\vec{a'})$ of $T$ and $T'$ consisting of
		rows corresponding to indices in $\vec{a}$. This requires proving $\vec{v}=\vecT[\vec{a}]$ and $\vec{v'}=
		\vecT'[\vec{a}]$, which we show using }%DIFDELCMD < {\em %%%
	\DIFdelend %DIF > \usepackage{amssymb}%! Author = nitinsingh
	%DIF > ! Date = 12/04/24
	\DIFaddbegin \DIFadd{We now detail the steps required to realize batching efficient RAM outlined in the technical overview.
	}
	
	\subsection{\DIFadd{Committed Index Lookup}}\label{subsec:committed-index-lookup}
	\DIFadd{In this section, we ``lift'' any committed sub-vector argument to a }\DIFaddend committed index lookup \DIFdelbegin %DIFDELCMD < } %%%
	\DIFdel{discussed in Section ~\ref{subsec:committed-index-lookup}.
	}%DIFDELCMD < 
	
	%DIFDELCMD < 				\item %%%
	\DIFdel{On the isolated sub-table $S$ and $S'$ of size $m$, we use the standard memory checking arguments (c.f. argument
		presented in Section \ref{sec:poly-proto-ram-app}) to prove that sequence $\vec{o}$ correctly updates $S$ to $S'$ with
		prover complexity of $\wt{O}(m)$.
	}%DIFDELCMD < 
	
	%DIFDELCMD < 				\item %%%
	\DIFdel{Finally, we show that the RAMs $T$ and $T'$ are identical outside indices in $\vec{a}$. We call them $\vec{a}$-identical
		and describe the protocol for proving the same in Section ~\ref{subsec:proximity-ram}.
	}%DIFDELCMD < \end{itemize}
	%DIFDELCMD < 			
	
	%DIFDELCMD < 			\begin{figure}[htbp]
	%DIFDELCMD < 				\centering
	%DIFDELCMD < 				\includegraphics[width=0.4\textwidth]{RAM-Lookup}
	%DIFDELCMD < 				%%%
	%DIFDELCMD < \caption{%
	{%DIFAUXCMD
		\DIFdelFL{Illustrating different steps of sub-linear lookup protocol between large RAMs $\vecT$ and $\vecT'$.}}
	%DIFAUXCMD
	%DIFDELCMD < \label{fig:blueprint}
	%DIFDELCMD < 			\end{figure}
	%DIFDELCMD < 			
	
	%DIFDELCMD < 			%%%
	\DIFdel{The blueprint for the above approach is illustrated in Figure ~\ref{fig:blueprint}.
		The efficiency of the above approach relies crucially on the efficiency of committed index lookup used to reduce the size of the RAMs for quasi-linear memory checking methods. It is tempting to use the recent lookup arguments
		in ~\mbox{%DIFAUXCMD
			\cite{CCS:ZBKMNS22,EPRINT:PosKat22,EPRINT:EagFioGab22} }\hskip0pt%DIFAUXCMD
		to prove the correctness of the first step with prover complexity dependent only on $m$.
		However, employing them directly is difficult; their table-independent efficiency relies on
		table specific expensive pre-computation, which does not help when the table is updatable. This is the problem we solve
		in Section ~\ref{sec:update-protocol}, where we modify the prover algorithm for the lookup arguments to remain efficient
		with access to pre-computed parameters for an ``approximate'' table.
	}%DIFDELCMD < \begin{comment}%DIFDELCMD <  %repetition
	%DIFDELCMD < 				In particular, we show that lookup protocols in
	%DIFDELCMD < 				~\cite{CCS:ZBKMNS22,EPRINT:PosKat22,EPRINT:EagFioGab22} can be used to show $m$ lookups from a table $\vecT'$, given pre-computed parameters
	%DIFDELCMD < 				for a table $\vecT$ with additional overhead of $O((m+\delta)\log^2 (m+\delta))$, where $\delta$ denotes the hamming distance
	%DIFDELCMD < 				of tables $\vecT$ and $\vecT'$. By optimally deferring the $O(N\log N)$ re-computation till we
	%DIFDELCMD < 				accumulate $\delta \approx \sqrt{mN}$ updates, we achieve an amortized prover overhead $O(\sqrt{mN})$ over the lookup protocols for non-updatable tables.
	%DIFDELCMD < 				This modification described in ~\ref{sec:update-protocol} applies to all the aforementioned lookup protocols.
	%DIFDELCMD < 			%DIFDELCMD < \end{comment}
	%DIFDELCMD < 			
	
	%DIFDELCMD < 			\noindent{\bf Additional Notation}%%%
	\DIFdel{:
		%DIF < Before proceeding, we introduce the subgroup $\setN=\{\xi,\ldots,\xi^N\}$ consisting of $N^{th}$ roots of unity,
		%DIF < over which we encode vectors in $\F^N$ as polynomials of degree less than $N$. Let $\{\mu_i(X)\}_{i=1}^N$ be the associated
		%DIF < lagrange basis polynomials over the set $\setN$. 
		We introduce the set $\setV$ consisting of $m^{th}$ roots of unity
		$\nu,\ldots,\nu^m$ with associated lagrange polynomials as $\{\tau_i(X)\}_{i=1}^m$. For $\vec{f}\in \F^N$, let $\enc{f}{\setN}$ denote the polynomial encoding of $\vec{f}$ over $\setN$ given by $\sum_{i=1}^N f_i\mu_i(X)$. Similarly,
		for $\vec{g}\in \F^m$, let $\enc{g}{\setV}$ denote its polynomial encoding over $\setV$ given by $\sum_{i=1}^m g_i\tau_i(X)$. For vectors $\vec{t}$, we sometimes use the index notation $\vec{t}[a_i]$ to denote $a_i^{th}$ element of the vector . For vectors
		$\vec{t}$ and $\vec{a}$ we use the notation $\vec{t}[\,\vec{a}\,]$ to denote the vector $\vec{v}$ such that $v_i=\vec{t}[\,a_i\,]$ for all $i$.
		%DIF < \chaya{above can be pruned. some of it is already in prelims.}
	}%DIFDELCMD < 
	
	%DIFDELCMD < 			
	%DIFDELCMD < 			%%%
	\subsection{\DIFdel{Committed Index Lookup}}%DIFAUXCMD
	\addtocounter{subsection}{-1}%DIFAUXCMD
	%DIFDELCMD < \label{subsec:committed-index-lookup}
	%DIFDELCMD < 			%%%
	\DIFdel{Let $m,N\in \N$ be fixed parameters with $m < N$ and let $\srs$ denote a $\kzg$ setup of degree $d\geq N$
		over bi-linear group $(\F$, $\Gone$, $\Gtwo$, $\GT$, $e$, $\gone{1}$, $\gtwo{1}$, $[1]_t)$.
		Recall that the committed index
		lookup relation in Definition ~\ref{defn:comm-index-lookup} involves the prover showing knowledge of vectors $\vecT\in \F^N$, $\vec{a}\in \F^m$ and $\vec{v}\in \F^m$ corresponding to public commitments $c_T, c_a$ and $c_v$ such that they
		satisfy $v_i = \vecT[\,a_i\,] = T_{a_i}$.
		We present a polynomial protocol for the same, which is an adaptation of the lookup protocol from Caulk+ ~\mbox{%DIFAUXCMD
			\cite{EPRINT:PosKat22}}\hskip0pt%DIFAUXCMD
		. However, here we do not aim for zero-knowledge. Let $T(X)=\enc{t}{\setN}$, $a(X)=\enc{a}{\setV}$ and $v(X)=\enc{v}{\setV}$ denote the polynomials encoding the vectors
		$\vec{t},\vec{a}$ and $\vec{v}$ respectively. The verifier knows commitments to these polynomials at the start of the protocol.
		Now $v_i = \vec{t}[a_i]$ for $i\in [m]$ is equivalent to $v(\nu^i) = T(\xi^{a(\nu^i)})$ for $i\in [m]$. To
		obtain a polynomial protocol, the prover interpolates a polynomial $h(X)=\sum_{i=1}^m \xi^{a_i}\tau_i(X)$, which satisfies
		$h(\nu^i)=\xi^{a(\nu^i)}$. To show that
		polynomial $h$ correctly ``exponentiates'' evaluations of $a(X)$, we consider the
		inverting polynomial $\ell(X)=\sum_{i=1}^N i\mu_i(X)$ which behaves like ``log'' over $\setN$ by evaluating to $i$ on $\xi^i$. Now, we see
		that all constraints are encoded as polynomial identities below:
	}\begin{displaymath}
		\DIFdel{\begin{aligned}
				\ell(h(X)) &= a(X) \quad \text{mod } Z_{\setV}\\  % & \quad\text{ encodes } & \quad \forall i\in [m]:& h(\nu^i) = \xi^{a(\nu^i)}  \\
				T(h(X)) &= v(X) \quad \text{mod } Z_\setV \\ % \quad\text{ encodes } & \quad \forall i\in [m]:& v_i = \vec{t}[a_i] \\
				Z_{\setN}(h(X)) &= 0 \qquad \text{mod } Z_\setV  %&\quad\text{ encodes } & \quad \forall i \in [m]:& h(\nu^i)\in \setN
			\end{aligned}
			%DIFDELCMD < \label{eq:comm-index-lookup}%%%
	}\end{displaymath}%DIFAUXCMD
	%DIFDELCMD < 			%%%
	\DIFdel{The last polynomial identity ensures that evaluations of $h$ on $\setV$ lie in $\setN$ (the set of roots of $\vpolyN$). Since the polynomial $\ell$ is one-one
		over $\setN$, the first equation implies $h(\nu^i)=\xi^{a_i}$ for all $i\in [m]$. The desired relation $v_i=T_{a_i}$ now follows from }\DIFdelend \DIFaddbegin \DIFadd{argument, where
		the latter makes a black-box use of }\DIFaddend the \DIFdelbegin \DIFdel{second identity.
		The above formulation involves composition with polynomials $\ell,T$ and $\vpolyN$ of degree $O(N)$, which is inefficient. We use the trick from
		\mbox{%DIFAUXCMD
			\cite{EPRINT:PosKat22}}\hskip0pt%DIFAUXCMD
		, where we work with low-degree restrictions of $O(N)$-degree polynomials such as $T, \ell$ over the set
		$\setN_I=\{{h(\nu^i)}: i\in [m]\}=\{\xi^{a_i}:i\in I\}\subseteq \setN$, where $I=\{a_i: i\in [m]\}$. The prover
		commits to the polynomials $Z_I(X)=\prod_{i\in I}(X-\xi^i)$, $h(X)$ and low degree ($<m$) restrictions $T_I, \ell_I$ of $T$ and $\ell$
		on the $\setN_I$ respectively. The polynomial protocol then checks the following:
	}\begin{displaymath}
		\begin{alignedat}{3}
			\DIFdel{T(X) - T_I(X) }&\DIFdel{= 0 \quad \text{ mod } Z_I ,}&\DIFdel{\quad}& \DIFdel{T_I(h(X)) }&\DIFdel{= v(X) \quad \text{ mod } Z_{\setV} }\\
			\DIFdel{\ell(X) - \ell_I(X) }&\DIFdel{= 0 \quad \text{ mod } Z_I ,}&\DIFdel{\quad}& \DIFdel{\ell_I(h(X)) }&\DIFdel{= a(X) \quad \text{ mod } Z_{\setV} }\\
			\DIFdel{Z_{\setN}(X) }&\DIFdel{= 0 \quad \text{ mod } Z_I ,}&\DIFdel{\quad}& \DIFdel{Z_I(h(X)) }&\DIFdel{= 0 \quad \text{ mod } Z_{\setV}
		}\end{alignedat}
		\DIFdel{%DIFDELCMD < \label{eq:poly-comm-index}%%%
	}\end{displaymath}%DIFAUXCMD
	%DIFDELCMD < 			%%%
	\DIFdel{It must be noted that the above identities imply the earlier polynomial identities in \eqref{eq:comm-index-lookup}. This is so because evaluations
		of $h$ on $\setV$ are roots of $Z_I$, which implies $T_I(h(\nu^i))=T(h(\nu^i))$, $\ell_I(h(\nu^i))=\ell(h(\nu^i))$ and $\vpolyN(h(\nu^i))=0$ over $\setV$.
		While the identities on the left still involve a degree $N$ polynomial, we can use the $\srs$ to check the polynomial
		identity at the point $\tau$ encoded in the $\srs$. For example, we can evaluate the encoded quotient $\gtwo{Q(X)} =$
		$\gtwo{\frac{(T(X) - T_I(X)}{Z_I(X)}}$ using the relation:
	}\begin{displaymath}
		\DIFdel{\gtwo{\frac{T(X)-T_I(X)}{Z_I(X)}} = \sum_{i\in I}\frac{1}{Z_I'(\xi^i)}\gtwo{\frac{T(X)-t_i}{X-\xi^i}}
	}\end{displaymath}%DIFAUXCMD
	%DIFDELCMD < 			%%%
	\DIFdel{By pre-computing }\DIFdelend \DIFaddbegin \DIFadd{former. We use the trick of random linear combination of vectors to infer
		indexed lookup relation among them from sub-vector relation over the aggregated vectors. Similar use of random
		linear combinations has been made in }\DIFaddend the \DIFdelbegin \DIFdel{$\kzg$ proofs $W_1^i=\gtwo{\frac{T(X)-t_i}{X-\xi^i}}$ for all $i\in [N]$, the encoded quotient can be
		evaluated using $O(m)$ $\Gtwo$-operations and $O(m\log^2 m)$ $\F$-operations.
		The identity is then checked using a
		real pairing check
	}\begin{displaymath}\DIFdel{e(\gone{T(X)}-\gone{T_I(X)},\gtwo{1})=e(\gone{Z_I(X)},\gtwo{Q(X)}).}\end{displaymath}%DIFAUXCMD
	%DIFDELCMD < 			%%%
	\DIFdel{Similarly, we also pre-compute the encoded
		quotients $W_2^i=\gtwo{\frac{\ell(X) - i}{X-\xi^i}}$ and $W_3^i=\gtwo{\frac{\vpolyN(X)}{X-\xi^i}}$ for all $i\in [N]$.
		The quotients can be computed in time $O(N\log N)$ using the techniques in ~\mbox{%DIFAUXCMD
			\cite{EPRINT:FeiKho23}}\hskip0pt%DIFAUXCMD
		. Using $\kzg$ commitment
		scheme the polynomial relations over $Z_\setV$ can be checked in a standard manner
		by having the prover send evaluation proofs for the committed polynomials at a random point chosen by the verifier. The total prover effort incurred is $O(m^2)$ group and field operations.
		Thus, we have:
	}%DIFDELCMD < \begin{lemma}\label{lem:comm-index-lookup}
	%DIFDELCMD < 				%%%
	\DIFdel{Assuming $\kzg$ is extractable polynomial commitment scheme, there exists a succinct argument of knowledge for
		the relation $\RLOOK$ with prover complexity of $O(m^2)$, given access to pre-computed parameters of size $O(N)$.
	}%DIFDELCMD < \end{lemma}
	%DIFDELCMD < 			%%%
	\DIFdelend \DIFaddbegin \DIFadd{context of proving permutations in literature (e.g. ~\mbox{%DIFAUXCMD
			\cite{SCN:GOTV22}}\hskip0pt%DIFAUXCMD
		).
	}\DIFaddend 
	
	\DIFdelbegin \subsubsection{\DIFdel{Committed Index Lookup: Generic Transformation}}%DIFAUXCMD
	\addtocounter{subsubsection}{-1}%DIFAUXCMD
	%DIFDELCMD < \label{subsubsec:generic-transformation}
	%DIFDELCMD < 			%%%
	\DIFdel{The protocol
		for committed index lookup using Caulk+ ~\mbox{%DIFAUXCMD
			\cite{EPRINT:PosKat22} }\hskip0pt%DIFAUXCMD
		can become prohibitive for higher values of
		$m$ due to the quadratic dependence on it. Here we describe a generic transformation, which realizes a committed index lookup
		argument from any sub-vector argument such as ~\mbox{%DIFAUXCMD
			\cite{CCS:ZBKMNS22,EPRINT:PosKat22,EPRINT:ZGKMR22,EPRINT:EagFioGab22} }\hskip0pt%DIFAUXCMD
		using a homomorphic
		vector commitment scheme. We recall that $\vec{a}\in \F^m$ is called a sub-vector of $\vec{b}\in \F^n$ if each element of $\vec{a}$
		also occurs in $\vec{b}$. We use $\vec{a}\leq \vec{b}$ to denote $\vec{a}$ is a sub-vector of $\vec{b}$.
		The transformation follows from the observation in the following lemma:
	}\DIFdelend \begin{lemma}\label{lem:generic-transformation}
		Let $\vec{t}\in \F^n$ and let \DIFdelbegin \DIFdel{$\vec{a},\vec{v}\in \F^m$ for some positive integers }\DIFdelend \DIFaddbegin \DIFadd{$\vec{a},\vec{v}\in \F^m$ for some positive integers }\DIFaddend $m,n$. Let \DIFdelbegin \DIFdel{$\vec{I}_n$ denote the vector }\DIFdelend \DIFaddbegin \DIFadd{$\vec{I}_n$ denote the vector }\DIFaddend $(1,\ldots,n)$.
		Then for $\gamma\gets \F$, \DIFdelbegin \DIFdel{$\vec{a}\leq \vec{I}_n$, $\vec{v}\leq \vec{t}$ and $(\vec{v}+\gamma \vec{a})\leq (\vec{t} + \gamma \vec{I}_n)$ }\DIFdelend \DIFaddbegin \DIFadd{$(\vec{v}+\gamma \vec{a})\preceq (\vec{t} + \gamma \vec{I}_n)$ }\DIFaddend implies
		$\vec{v}=\vec{t}[\,\vec{a}\,]$ except with probability \DIFdelbegin \DIFdel{$O(n/|\F|)$.
		}\DIFdelend \DIFaddbegin \DIFadd{$mn/|\F|$.
		}\DIFaddend \end{lemma}
	\DIFdelbegin \DIFdel{The proof of the Lemma appears in Section ~\ref{sec:generic-transformation-app}.
		Using Lemma ~\ref{lem:generic-transformation}, allows construction of argument for proving }\DIFdelend \DIFaddbegin \begin{proof}
		\DIFadd{We define vectors of linear polynomials $\vec{p}=(p_1,\ldots,p_m)$ and $\vec{q}=(q_1,\ldots,q_n)$ where
			$p_i(X) = v_i + a_i X$, $i\in [m]$ and $q_i(X) = t_i + i X$, $i\in [n]$. Now, we see that }\DIFaddend $\vec{v}=\vec{t}[\,\vec{a}\,]$
		\DIFdelbegin \DIFdel{using three instantiations
			of a }\DIFdelend \DIFaddbegin \DIFadd{if and only if $\vec{p}\preceq \vec{q}$. For $\gamma\in F$, let $\vec{p}_\gamma$ and $\vec{q}_\gamma$ denote the vectors
			$(p_1(\gamma),\ldots,p_m(\gamma))$ and $(q_1(\gamma),\ldots,q_n(\gamma))$ respectively. It is obvious that $\vec{p}\preceq \vec{q}$
			implies $\vec{p}_\gamma\preceq \vec{q}_\gamma$ for all $\gamma\in \F$. Using Schwartz-Zippel Lemma, it can also be seen that
			$\Pr_{{}_{\gamma\gets \F}}[\vec{p}\npreceq \vec{q} \,|\, \vec{p}_\gamma\preceq \vec{q}_\gamma]\leq mn/|\F|$. The bound
			follows from the observation that the event occurs only when $\gamma$ is a common root of at least one pair of linear polynomials
			$\{(p_i(X),q_j(X)): i\in [m], j\in [n]\}$.
	}\end{proof}
	\DIFadd{In Figure ~\ref{fig:committed-index-lookup}, we invoke Lemma ~\ref{lem:generic-transformation} to construct a
		committed index lookup argument using a committed }\DIFaddend sub-vector \DIFdelbegin \DIFdel{argument.
		We require homomorphism of the commitment scheme to enable the verifier to compute commitments for $\vec{v}+\gamma \vec{a}$ and $\vec{t} + \gamma \vec{I}_n$ for the final instantiation of sub-vector }\DIFdelend argument \DIFaddbegin \DIFadd{$(\subvecprover,\subvecverifier)$. We formally
		state the following lemma, whose proof essentially follows from Lemma ~\ref{lem:generic-transformation}.
	}
	
	\begin{lemma}\label{lem:proto-committed-index-lookup}
		\DIFadd{Assuming that $(\subvecprover,\subvecverifier)$ is an argument of knowledge for the relation $\RSVEC$ in the AGM, the interactive protocol
			in Figure ~\ref{fig:committed-index-lookup} is an argument of knowledge for the relation $\RLOOK$ in the AGM.
	}\end{lemma}
	
	\begin{figure}[htbp]
		\begin{mdframed}
			{
				%DIF > <<<<<<< HEAD
				{\bf \DIFaddFL{Common Input}}\DIFaddFL{: $\srs,c_t, c_a, c_v$, $c_I=\gone{I(X)}$ where $I(X)=\enc{I}{\setN}$ encodes the vector $\vec{I}=(1,\ldots,N)\in \F^N$. }\\
				%DIF > =======
				%DIF >     {\bf Common Input}: $\srs,c_t, c_a, c_v$, $c_I=\gone{I(X)}$ where $I(X)=\enc{\vec{I}_N}{\setN}$
				%DIF >     with $\vec{I}_N=(1,\ldots,N)$. \\
				%DIF > >>>>>>> f162aeee62ad0161c590d91a0cd6435404bbcc83
				{\bf \DIFaddFL{Prover's Input}}\DIFaddFL{: Vectors $\vec{t}\in \F^N$, $\vec{a},\vec{v}\in\F^m$.
				}\begin{enumerate}[leftmargin=1em, label=\arabic*.]
					\item \DIFaddFL{$\verifier$ sends $\gamma\gets \F$.
					}\item \DIFaddFL{$\prover$ and $\verifier$ compute: $\tilde{c}_t=\gamma c_I+c_t$, $\tilde{c}_v=\gamma c_a + c_v$.
					}\item \DIFaddFL{$\prover$ computes: $\tilde{\vec{t}}=\gamma\vec{I}_N + \vec{t}$, $\tilde{\vec{v}}=\gamma\vec{a} + \vec{v}$}\DIFaddendFL .
					\DIFdelbeginFL \DIFdelFL{In particular, we also benchmark committed index lookup
						protocol using lookup argument in ~\mbox{%DIFAUXCMD
							\cite{EPRINT:EagFioGab22}}\hskip0pt%DIFAUXCMD
						, which incurs prover complexity of $O(m\log m)$.
						~
					}\DIFdelendFL \DIFaddbeginFL \item \DIFaddFL{$\prover$ and $\verifier$ run sub-vector argument $(\subvecprover,\subvecverifier)$ with
						$(\srs,\tilde{c}_t,\tilde{c}_v)$ as the common input and $(\tilde{\vec{t}},\tilde{\vec{v}})$ as $\subvecprover$'s input.
					}\item \DIFaddFL{$\verifier$ outputs
						$b\gets \argoutput{\subvecprover}{\subvecverifier}{\tilde{\vec{t}},\tilde{\vec{v}}}{\srs,\tilde{c}_t,\tilde{c}_v}$.
				}\end{enumerate}
			}
		\end{mdframed}
		\caption{\DIFaddFL{Committed Index Lookup Argument}}
		\label{fig:committed-index-lookup}
	\end{figure}
	\DIFaddend 
%	
	\subsection{Almost Identical RAM States}\label{subsec:proximity-ram}
	For a vector $\vec{a}\in [N]^m$, let $\uniq{a}=\{a_i: i\in [m]\}$ denote the subset of unique values in $\vec{a}$. We call two
	RAM states $\vecT, \vecT'\in \F^N$ to be $\vec{a}$-{\em identical} if $\vecT[i]=\vecT'[i]$ for all $i\not\in\uniq{a}$. As before,
	let \DIFdelbegin \DIFdel{$T(X),T'(X)$ }\DIFdelend \DIFaddbegin \DIFadd{$T(X),T^*(X)$ }\DIFaddend and $a(X)$ be polynomials encoding the vectors $\vecT,\vecT'$ (over $\setN$) and $\vec{a}$ (over $\setV$). \DIFdelbegin \DIFdel{Given
		commitments }\DIFdelend \DIFaddbegin \DIFadd{Let
	}\DIFaddend $c_T, c_T'$ and $c_a$ \DIFaddbegin \DIFadd{be the commitments to vectors $\vecT, \vecT'$ and $\vec{a}$ respectively in the group $\Gone$. The }\DIFaddend polynomial protocol to prove that
	\DIFdelbegin \DIFdel{committed vectors }\DIFdelend $\vecT,\vecT'\in \F^N$ and $\vec{a}\in \F^m$ are \DIFdelbegin \DIFdel{such that $\vecT,\vecT'$ are }\DIFdelend $\vec{a}$-identical \DIFdelbegin \DIFdel{involves }\DIFdelend \DIFaddbegin \DIFadd{requires }\DIFaddend proving the relation
	\DIFdelbegin \DIFdel{$Z_I(X)(T(X) - T'(X)) = 0$ over }\DIFdelend \DIFaddbegin \DIFadd{$Z_I(X)(T(X) - T^*(X)) = 0$ over the set }\DIFaddend $Z_\setN$ where
	$I=\uniq{a}$ and $Z_I(X)=\prod_{i\in I}(X-\xi^i)$ \DIFdelbegin \DIFdel{denotes }\DIFdelend \DIFaddbegin \DIFadd{is }\DIFaddend the vanishing polynomial for the set $\setN_I=\{\xi^i: i\in I\}$.
	\DIFdelbegin \DIFdel{The }\DIFdelend %DIF > \moumita{ Prev: To proceed, the honest prover commits to polynomial $Z_I$ and proves (i) $Z_I(T - T') = 0 \text{ mod } Z_\setN$ and (ii) the zeroes
	%DIF > of $Z_I$ form a subset of $\setN_I$ as defined. }
	\DIFaddbegin \DIFadd{To proceed, the honest }\DIFaddend prover commits to polynomial \DIFdelbegin \DIFdel{$Z_I$ }\DIFdelend \DIFaddbegin \DIFadd{$Z_I(X)$ }\DIFaddend and proves (i) \DIFdelbegin \DIFdel{$Z_I(T - T') = 0 \text{ mod } Z_\setN$ }\DIFdelend \DIFaddbegin \DIFadd{$Z_I(X)\cdot (T(X) - T^*(X)) = 0 \text{ mod } Z_\setN$ }\DIFaddend and (ii) \DIFdelbegin \DIFdel{$Z_I$ is the vanishing
		polynomial of the set $\setN_I$ }\DIFdelend \DIFaddbegin \DIFadd{the zeroes of $Z_I(X)$ form a subset of zeroes of $\setN_I(X)$ }\DIFaddend as defined.
	\DIFaddbegin \DIFadd{Together, the two conditions imply that $T(\xi^i)=T^*(\xi^i)$ for $i\not\in \uniq{a}$.
	}\DIFaddend To prove the first relation, the prover computes the polynomial \DIFdelbegin \DIFdel{$Q(X)$ }\DIFdelend \DIFaddbegin \DIFadd{$D(X)$ }\DIFaddend as below:
	\begin{align}\label{eq:poly-q}
		D(X) &= \DIFdelbegin \DIFdel{\frac{(T(X)-T'(X))\cdot Z_I(X)}{Z_\setN(X)} }\DIFdelend \DIFaddbegin \DIFadd{\frac{(T(X)-T^*(X))\cdot Z_I(X)}{Z_\setN(X)} }\DIFaddend \nonumber \\
		&= \sum_{i\in I}\DIFdelbegin \DIFdel{\frac{(t_i - t_i')\mu_i(X)}{Z_\setN(X)} }\DIFdelend \DIFaddbegin \DIFadd{\frac{(T(\xi^i) - T^*(\xi^i))\mu_i(X)}{Z_\setN(X)} }\DIFaddend Z_I(X) \nonumber \\
		\DIFdelbegin \intertext{\DIFdel{Substituting, $\Delta_i=t_i-t_i'$, $\mu_i(X)=\vpolyN(X)/(\vpolyN'(\xi^i)(X-\xi^i))$ }}
		%DIFAUXCMD
		\DIFdelend \DIFaddbegin \intertext{ \DIFadd{Substituting, $\Delta_i=T(\xi^i) - T^*(\xi^i)$, $\mu_i(X)=\vpolyN(X)/(\vpolyN'(\xi^i)(X-\xi^i))$ }}
		\DIFaddend &=\sum_{i\in I}\frac{\Delta_i}{Z_\setN'(\xi^i)}\left(\frac{Z_I(X)}{X-\xi^i}\right) = \sum_{i\in I}\frac{\Delta_i Z_I'(\xi^i)}{Z_\setN'(\xi^i)}\kappa_i(X)
	\end{align}
	In the above, the summation only runs over indices in $I$, as $\Delta i = 0$ for $i\not\in I$. In the final equality, we use
	$\kappa_i(X) = Z_I(X)/(Z_I'(\xi^i)(X-\xi^i))$ for $i\in I$ which we recognize as the lagrange basis polynomials for the set
	$\{\xi^i: i\in I\}$. Thus, Equation \eqref{eq:poly-q} implies that $D(X)$ is at most degree $|I|-1$ polynomial, with
	$D(\xi^i)=\Delta_i Z_I'(\xi^i)/\vpolyN'(\xi^i)$ for $i\in I$.
	The prover can therefore interpolate $D(X)$ (in power basis)
	in $O(|I|\log^2 |I|)$ $\F$-operations and compute $\gone{D(X)}$ in $O(|I|)$ $\Gone$-operations. The prover sends the
	commitment $\gone{D(X)}$ to the verifier. Finally, the verifier can
	check the identity \DIFdelbegin \DIFdel{$Z_I(T - T') = D\cdot Z_\setN$ }\DIFdelend \DIFaddbegin \DIFadd{$Z_I(X) \cdot  (T(X) - T^*(X)) = D(X) \cdot Z_\setN(X)$ %DIF > \moumita{Prev: $Z_I(T - T^*) = D\cdot Z_\setN$ . Alternative: $Z_I(T - T') = D\cdot Z_\setN$} 
	}\DIFaddend by a pairing check. For this, since the tables are committed in $\Gone$, prover will need to send $\elttwo{Z_I(X)}$.
	
	Next, the prover needs to show that \DIFaddbegin \DIFadd{zeroes of $Z_I$ are indeed in the set $\setN_I=\{\xi^i: i\in I\}=\{\xi^{a_i}:i\in [m]\}$.
		Clearly, it suffices to show that }\DIFaddend $Z_I(X)$ \DIFdelbegin \DIFdel{is indeed the vanishing polynomial of $\setN_I$. We again use the }\DIFdelend \DIFaddbegin \DIFadd{divides the polynomial $\prod_{i\in [m]}(X - \xi^{a_i})$. To obtain a
		polynomial protocol, the prover commits to an auxiliary }\DIFaddend polynomial $h(X)=\sum_{i=1}^m \xi^{a_i}\tau_i(X)$
	which interpolates the vector\\ \DIFdelbegin \DIFdel{$(\xi^{a_1},\ldots,\xi^{a_m})$}\DIFdelend \DIFaddbegin \DIFadd{$\vec{h}=(\xi^{a_1},\ldots,\xi^{a_m})$}\DIFaddend . The correctness of \DIFdelbegin \DIFdel{the }\DIFdelend $h$ polynomial can be
	established by \DIFdelbegin \DIFdel{checking the polynomial identities in the last two rows of Equation
		~\eqref{eq:poly-comm-index}.The aforementioned identities show that $Z_I(h(X)) = 0$ over $Z_\setV$ which shows that $Z_I$ vanishes over
		entire vector interpolated by $h$ over $\setV$. To assert that $Z_I$ has no additional roots}\DIFdelend \DIFaddbegin \DIFadd{showing that the interpolated vector $\vec{h}$ satisfies committed index lookup relation
		$\vec{h}=\vec{T}_{\exp}[\,\vec{a}\,]$ where $\vec{T}_{\exp}=(\xi^1,\ldots,\xi^N)$. Moreover, we notice that the polynomial
		interpolating the table $\vec{T}_{\exp}$ is particularly simple, i.e, $T_{\exp}(X)=X$, and thus the setup need not be
		augmented with table-specific parameters for $\vec{T}_{\exp}$.
		Finally, it remains to show that $Z_I(X)$ divides $K(X) = \prod_{i=1}^m (X - h(\nu^i))$. To do so}\DIFaddend , the prover commits to
	\DIFdelbegin \DIFdel{the product polynomial
		$K(X)=\prod_{i=1}^m (X - h(\nu^i))$ }\DIFdelend \DIFaddbegin \DIFadd{$K(X)$ }\DIFaddend and the quotient polynomial $q(X)=K(X)/Z_I(X)$. The verifier checks the polynomial identities
	at $\alpha$, i.e $K(\alpha)=q(\alpha)Z_I(\alpha)$ and $K(\alpha)=\prod_{i=1}^m(\alpha - h(\nu^i))$.
	The former is easily accomplished
	using evaluation proofs for $K,q$ and $Z_I$ at $\alpha$\DIFaddbegin \DIFadd{.
	}\DIFaddend For checking the latter, the prover commits to another polynomial
	$u(X)$ satisfying $u(\nu^i)=\prod_{j=1}^{i-1}\big((\alpha - h(\nu^j))/(1 + \beta\tau_1(\nu^j))\big)$ for $i\in [m]$
	where $\beta=K(\alpha) - 1$.
	The verifier ensures the correctness of $u(X)$ by \DIFaddbegin \DIFadd{checking:
	}\DIFaddend \begin{equation}
		\begin{aligned}
			\tau_1(X)(u(X) - 1) &= 0 \text{ mod } Z_{\setV} \\
			u(\nu X)(1+\beta \tau_1(X))-u(X)(\alpha - h(X)) &= 0 \text{ mod } Z_\setV.
		\end{aligned}
		\label{eq:kh-check}
	\end{equation}
	We prove that the above constraints imply that $K(\alpha)=\prod_{i\in [m]}(\alpha - h(\nu^i))$ \DIFaddbegin \DIFadd{in Lemma ~\ref{lem:kh-check}}\DIFaddend .
	Note that in this protocol we require commitment to the polynomial $Z_I$ in both $\Gone$ and $\Gtwo$,
	and thus another pairing check is required to show that the $Z_I(X)$ committed in $\Gone$
	\DIFdelbegin \DIFdel{(whose well formation is shown in the protocol as described above) is the }\DIFdelend \DIFaddbegin \DIFadd{is the }\DIFaddend same as the $Z_I(X)$ committed in $\Gtwo$ \DIFdelbegin \DIFdel{,
	}\DIFdelend \DIFaddbegin \DIFadd{(}\DIFaddend used for the real pairing check\DIFdelbegin \DIFdel{.
	}\DIFdelend \DIFaddbegin \DIFadd{).
		The complete protocol for checking that RAMs $\vecT$ and $\vecT'$ are identical outside indices in $\vec{a}$
		is given in Figure ~\ref{fig:a-identical}.
	}
	
	\DIFaddend \begin{lemma}\label{lem:kh-check}
		There exists a polynomial $u(X)\in \F[X]$ satisfying the identities in Equation ~\eqref{eq:kh-check}
		if and only if $K(\alpha)=1+\beta=\prod_{i\in [m]} (\alpha - h(\nu^i))$.
	\end{lemma}
	\begin{proof}
		Assume that the identitites hold for some polynomial $u(X)$.
		The first identity implies $u(\nu)=1$. From the second identity, we conclude that for all $i\in [m]$, we have
		$u(\nu^{i+1})=u(\nu^i)\cdot ((\alpha - h(\nu^i))/(1+\beta \tau_1(\nu^i)))$, and thus:
		$$1 = u(\nu^{m+1})/u(\nu) = \prod_{i\in [m]}\left(\frac{\alpha - h(\nu^i)}{1+\beta \tau_1(\nu^i)}\right).$$
		We observe that the product of denominators in the above equation is simply $1+\beta$ as $\tau_1(\nu^i)$
		is $0$ for all $i\neq 1$, and thus $1+\beta = \prod_{i=1}^m (\alpha - h(\nu^i))$. In the other direction,
		it is easy to check that $u(X)$ as defined for an honest prover, satisfies the identities in Equation ~\ref{eq:kh-check}.
	\end{proof}
	
	\DIFdelbegin %DIFDELCMD < \noindent{\em Remark}%%%
	\DIFdel{: Note that we incur $O(m^2)$ cost if we show the correctness of polynomial $h$ using techniques of Section ~\ref{subsec:committed-index-lookup}.
		However, we can use more $O(m\log m)$ complexity committed index lookup obtained from ~\mbox{%DIFAUXCMD
			\cite{EPRINT:EagFioGab22}}\hskip0pt%DIFAUXCMD
		, which allows us to show correctness of
	}\DIFdelend %DIF > %% complete protocol listing %%%
	\DIFaddbegin \begin{figure}[tb!]
		\scriptsize
		\begin{mdframed}
			
			{\bf \DIFaddFL{Common Input}}\DIFaddFL{: $\srs$, $c_T, c_T', c_a$.
			}
			
			{\bf \DIFaddFL{Prover's Input}}\DIFaddFL{: Vectors $\vecT,\vecT'\in \F^N$, $\vec{a}\in \F^m$. Polynomials $T(X),T^*(X)$ and
				$a(X)$ encoding $\vecT,\vecT'$ and $\vec{a}$ respectively.}\\
			
			{\bf \DIFaddFL{Round 1}}\DIFaddFL{: Prover commits to auxiliary polynomials
			}\begin{enumerate}[leftmargin=1em, label=\arabic*.]
				\item \DIFaddFL{$\prover$ computes:
				}\begin{itemize}[leftmargin=1em, label=-]
					\item \DIFaddFL{$I=\uniq{a}$, $Z_I(X)=\prod_{i\in I}(X-\xi^i)$.
					}\item \DIFaddFL{$D(X)=Z_I(X)(T(X) - T^*(X))/\vpolyN(X)$.
					}\item \DIFaddendFL $h(X)$ \DIFdelbeginFL \DIFdelFL{by proving that it interpolates vector $\vec{h}$ }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{such that $h(\nu^i)=\xi^{a_i}$ for $i\in [m]$.
					}\item \DIFaddFL{$K(X)=\prod_{i=1}^m (X - h(\nu^i))$, $q(X)=K(X)/Z_I(X)$.
				}\end{itemize}
				\item \DIFaddFL{$\prover$ sends $c_z = \gone{Z_I(X)}$, $c_z'=\gtwo{Z_I(X)}$, $c_d=\gone{D(X)}$, $c_h=\gone{h(X)}$, $c_k=\gone{K(X)}$,
					$c_q=\gone{q(X)}$.
				}\item \DIFaddFL{$\verifier$ sends $\alpha\gets \F$.
			}\end{enumerate}
			
			{\bf \DIFaddFL{Round 2}}\DIFaddFL{: Prover commits to polynomial $u(X)$.
			}\begin{enumerate}[leftmargin=1em, label=\arabic*.]
				\item \DIFaddFL{$\prover$ sets $\beta=K(\alpha)-1$ and interpolates $u(X)$ }\DIFaddendFL on $\setV$ \DIFdelbeginFL \DIFdelFL{obtained as }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{such that
					$u(\nu^i)=\prod_{j=1}^{i-1}\big((\alpha - h(\nu^j))/(1 + \beta\tau_1(\nu^j))\big)$ for $i\in [m]$.
				}\item \DIFaddFL{$\prover$ sends $c_u=\gone{u(X)}$.
				}\item \DIFaddFL{$\verifier$ sends $r\gets \F$.
			}\end{enumerate}
			
			
			{\bf \DIFaddFL{Round 3}}\DIFaddFL{: Prover batches checks in Eq ~\eqref{eq:kh-check}.
			}\begin{enumerate}[leftmargin=1em, label=\arabic*.]
				\item \DIFaddFL{$\prover$ computes:
				}\begin{align*}
					\DIFaddFL{Q(X)=\big(}&\DIFaddFL{u(\nu X)(1+\beta \tau_1(X)) - u(X)(\alpha - h(X)) }\\
					&\DIFaddFL{\quad + r\tau_1(X)(u(X)-1)\big)/Z_{\setV}(X)
				}\end{align*}
				\item \DIFaddFL{$\prover$ sends $c_Q = \gone{Q(X)}$.
				}\item \DIFaddFL{$\verifier$ sends $s\gets \F$.
			}\end{enumerate}
			
			{\bf \DIFaddFL{Round 4}}\DIFaddFL{: Prover sends evaluations.
			}\begin{enumerate}[leftmargin=1em, label=\arabic*.]
				\item \DIFaddFL{$\prover$ sends $\val{\alpha}{z}=Z_I(\alpha)$, $\val{\alpha}{q}=q(\alpha)$, $\val{\alpha}{K}=K(\alpha)$,
					$\val{s}{Q}=Q(s)$, $\val{s}{u}=u(s)$, $\val{\nu s}{u}=u(\nu s)$, $\val{s}{h}=h(s)$.
				}\item \DIFaddFL{$\verifier$ sends $r_\alpha, r_s\gets \F$.
			}\end{enumerate}
			
			{\bf \DIFaddFL{Round 5}}\DIFaddFL{: Prover batches evaluation proofs.
			}\begin{enumerate}[leftmargin=1em, label=\arabic*.]
				\item \DIFaddFL{Compute:
				}\begin{itemize}[leftmargin=1em, label=-]
					\item \DIFaddFL{$p_\alpha(X)=Z_I(X) + r_\alpha q(X) + r_\alpha^2 K(X)$.
					}\item \DIFaddFL{$p_s(X) = Q(X) + r_s u(X) + r_s^2 h(X)$.
					}\item \DIFaddFL{$\Pi_\alpha = \kzgprove(\srs,p_\alpha,\alpha)$.
					}\item \DIFaddFL{$\Pi_s = \kzgprove(\srs,p_s,s)$, $\Pi_{\nu s}$ = $\kzgprove(\srs,u,\nu s)$.
				}\end{itemize}
				\item \DIFaddFL{Send $\Pi_\alpha, \Pi_s, \Pi_{\nu s}$.
			}\end{enumerate}
			
			{\bf \DIFaddFL{Round 6}}\DIFaddFL{: Verifier checks identities.
			}\begin{enumerate}[leftmargin=1em, label=\arabic*.]
				\item \DIFaddFL{$\verifier$ computes $\gone{p_\alpha}=c_z + r_\alpha c_q + r_\alpha^2$, $\gone{p_z}$ = $c_Q + r_s c_u + r_s^2 c_h$.
				}\item \DIFaddFL{$\verifier$ checks:
				}\begin{itemize}[leftmargin=1em, label=-]
					\item \DIFaddFL{$\val{\alpha}{z}\cdot \val{\alpha}{q}=\val{\alpha}{K}$.
					}\item \DIFaddFL{$\val{\nu s}{u}(1+\beta \tau_1(s)) - \val{s}{u}(\alpha - \val{s}{h})$\\ $+ r\tau_1(s)(\val{s}{u}-1)=\val{s}{Q}Z_\setV(s)$.
					}\item \DIFaddFL{$e(c_T - c_T', c_z')=e(c_d, \gtwo{\vpolyN(X)})$.
					}\item \DIFaddFL{$e(\gone{1},c_z')=e(c_z, \gtwo{1})$.
					}\item \DIFaddFL{$\kzgverify(\srs$ ,
						$\gone{p_\alpha}$, $\val{\alpha}{z}+r_\alpha \val{\alpha}{q} + r_\alpha^2 \val{\alpha}{K}$, $\alpha$, $\Pi_\alpha)$.
					}\item \DIFaddFL{$\kzgverify(\srs$,
						$\gone{p_z}$,  $\val{s}{Q}+r_s \val{s}{u} + r_s^2 \val{s}{K}$, $s$, $\Pi_s)$.
					}\item \DIFaddFL{$\kzgverify(\srs,c_u, \val{\nu s}{u}, \nu s, \Pi_{\nu s})$.
				}\end{itemize}
			\end{enumerate}
			
			{\bf \DIFaddFL{Round 7}}\DIFaddFL{: Check correctness of polynomial $h$.
			}\begin{enumerate}[leftmargin=1em, label=\arabic*.]
				\item \DIFaddFL{$\prover$ and $\verifier$ execute }\DIFaddendFL committed index lookup \DIFdelbeginFL \DIFdelFL{using vector $\vec{a}$ on the vector $\vecT_{exp}=(\xi,\xi^2,\ldots,\xi^N)$
					i.e., $\vec{h}=\vecT_{exp}[\, \vec{a}\,]$.
				}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{argument (Fig ~\ref{fig:committed-index-lookup})
					to check $(\gone{X},c_a,c_h)\in \RLOOK$.
				}\item \DIFaddFL{$\verifier$ accepts if the above argument accepts and all the preceding checks succeed.
			}\end{enumerate}
		\end{mdframed}
		\caption{\DIFaddFL{Argument for showing RAMs are identical outside small set of indices.}}
		\label{fig:a-identical}
	\end{figure}
	\DIFaddend 
%	
	\subsection{Batching-Efficient RAM: Combined Protocol}\label{subsec:all-together}
	We put the entire protocol together now. Let $\setind$ denote the set of indices $\{1,\ldots,N\}$, and \DIFdelbegin \DIFdel{$\mathcal{I}_N$
	}\DIFdelend \DIFaddbegin \DIFadd{$\vec{I}_N$
	}\DIFaddend denote the vector $(1,\ldots,N)$. We formally define the committed RAM relation for which we present an argument of
	knowledge in this section.
	\begin{definition}\label{defn:committed-ram}
		We define the {\em committed ram} relation
		$\CRAM$ to consist of tuples $((c_T, c_T', c_\op, c_a, c_w),(\vecT, \vecT',\vec{\op},\vec{a},\vec{w}))$
		such that:
		\begin{itemize}[leftmargin=1em]
			\item \DIFdelbegin \DIFdel{$(T,\vec{o},T')\in \LRAM{I}{N}{m}$ for $T=(\setind_N,\vecT)$, $T'=(\setind_N,\vecT')$ and }\DIFdelend \\ \DIFaddbegin \DIFadd{$(\vecT,\vec{o},\vecT')\in \LRAM{I}{N}{m}$ for }\DIFaddend $\vec{o}=(o_1,\ldots,o_m)$ where \DIFaddbegin \DIFadd{we have }\DIFaddend $o_i=(\op_i, a_i, w_i)\in \RAMOp{I}$ for all $i\in [m]$\DIFdelbegin \DIFdel{.
			}\DIFdelend \DIFaddbegin \DIFadd{~(here we implicitly view vectors $\vecT$ and $\vecT'$ as RAMs with index column $\setind_N$). 
			}
			
			%DIF >     for $T=(\setind_N,\vecT)$, $T'=(\setind_N,\vecT')$ and $\vec{o}=(o_1,\ldots,o_m)$
			
			\DIFaddend \item $c_T=\KZGcommit(\srs, T(X))$, \DIFdelbegin \DIFdel{$c_T'=\KZGcommit(\srs, T'(X))$}\DIFdelend \\ \DIFaddbegin \DIFadd{$c_T'=\KZGcommit(\srs, T^*(X))$}\DIFaddend , $c_\op = \KZGcommit(\srs,\op(X))$,  $c_a=\KZGcommit(\srs, a(X))$,
			$c_w = \KZGcommit(\srs, w(X))$ where polynomials \DIFdelbegin \DIFdel{$T(X), T'(X)$ }\DIFdelend \DIFaddbegin \DIFadd{$T(X), T^*(X)$ }\DIFaddend encode vectors $\vecT, \vecT'$ over $\setN$, while $\op(X), a(X)$ and
			$w(X)$ encode vectors $\vec{\op}=(\op_1,\ldots,\op_m)$, $\vec{a}$ and $\vec{w}$ over $\setV$.
		\end{itemize}
	\end{definition}
	As outlined in the blueprint, the prover first commits to ``smaller'' RAMs \DIFdelbegin \DIFdel{$S=(\vec{a},\vec{v})$ and $S'=(\vec{a},\vec{v}')$
	}\DIFdelend \DIFaddbegin \DIFadd{$\Sbf=(\vec{a},\vec{v})$ and $\Sbf'=(\vec{a},\vec{v}')$
	}\DIFaddend where $\vec{v}=\vecT[\vec{a}]$ and $\vec{v}'=\vecT'[\vec{a}]$. The prover commits to \DIFdelbegin \DIFdel{$S$ and $S'$ }\DIFdelend \DIFaddbegin \DIFadd{$\Sbf$ and $\Sbf'$ }\DIFaddend by sending commitments
	$c_v$ and $c_v'$ to $\vec{v}$ and $\vec{v}'$. Then the prover and verifier execute the committed index lookup protocol to
	prove:
	\begin{equation}
		(c_T, c_a, c_v)\in \RLOOK\, \wedge\, (c_T', c_a, c_v')\in \RLOOK
	\end{equation}
	The verifier uses a random challenge $\chi\gets \F$ to reduce two instances of $\RLOOK$ to one instance
	$(c_T + \chi c_T', c_a, c_v + \chi c_v')\in \RLOOK$. Then, we show that
	RAMs $\vecT$ and $\vecT'$ are $\vec{a}$-identical using the protocol in \DIFdelbegin \DIFdel{Section }\DIFdelend \DIFaddbegin \DIFadd{Figure ~\ref{fig:a-identical}, described
		in Section ~}\DIFaddend \ref{subsec:proximity-ram}.
	All that remains is to prove is that the operation sequence $\vec{o}$ is consistent with small RAMs \DIFdelbegin \DIFdel{$S$ and $S'$}\DIFdelend \DIFaddbegin \DIFadd{$\Sbf$ and $\Sbf'$}\DIFaddend .
	We check this using the argument in \DIFdelbegin \DIFdel{Section ~\ref{sec:poly-proto-ram}}\DIFdelend \DIFaddbegin \DIFadd{Appendix ~\ref{sec:poly-proto-ram-app}, which is obtained by compiling the
		polynomial protocol for RAM in Appendix ~\ref{sec:poly-proto-ram} into an argument of knowledge in the AGM}\DIFaddend .
	Specifically, the prover and the verifier set
	$c_S = (c_a, c_v)$, $c_S'=(c_a, c_v')$ and $c_o = (c_\op, c_a, c_w)$, and execute the argument of knowledge for
	showing $(c_S, c_o, c_S')\in \CLRAM$ (see Definition ~\DIFdelbegin \DIFdel{\ref{defn:committed-ram}}\DIFdelend \DIFaddbegin \DIFadd{\ref{defn:committed-vram}}\DIFaddend ). We provide the complete protocol
	listing in Figure ~\ref{fig:complete-listing}. The protocol in Figure ~\ref{fig:complete-listing} assumes pre-computed parameters
	for the tables \DIFdelbegin \DIFdel{$T$ and $T'$. In the next section, we discuss how to efficiently maintain table-dependent }\DIFdelend \DIFaddbegin \DIFadd{$\vecT$ and $\vecT'$. The maintenance of these pre-computed }\DIFaddend parameters in the \DIFdelbegin \DIFdel{setting
		requiring updates
		to the table}\DIFdelend \DIFaddbegin \DIFadd{presence of updates
		is detailed in Section ~\ref{sec:update-protocol}}\DIFaddend .
	
	\begin{theorem}\label{thm:committed-ram}
		The protocol in Figure~\ref{fig:complete-listing} \DIFdelbegin \DIFdel{(continued in Figures~\ref{fig:complete-listing-2} and
			~\ref{fig:complete-listing-3}) }\DIFdelend is a succinct argument of knowledge for the relation \DIFdelbegin \DIFdel{$\CLRAM$ }\DIFdelend \DIFaddbegin \DIFadd{$\CRAM$ }\DIFaddend in
		the AGM, under the $Q$-DLOG assumption for the bilinear group $(\F,\Gone,\Gtwo,\GT,e,g_1,g_2)$.
	\end{theorem}
	
	%DIF < %% complete protocol listing %%%
	\begin{figure}[tb!]
		\scriptsize
		\begin{mdframed}
			
			
			\underline{Setup $(1^\secp,N,m, \vecT, \vecT')$}:
			\begin{itemize}[leftmargin=1em]
				\item $\srs = (\{\gone{\tau^i}\}_{i=0}^N, \{\gtwo{\tau^i}\}_{i=0}^N)$ for $\tau\gets \F$.
				\item \DIFdelbeginFL \DIFdelFL{$W_2^i=\gtwo{(\ell(X) - i)/(X-\xi^i)}$}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{$W_2^i=\gtwo{\vpolyN(X)/(X-\xi^i)}$}\DIFaddendFL , $i\in [N]$ (needed by prover)\DIFaddbeginFL \DIFaddFL{.
				}\DIFaddendFL \item \DIFdelbeginFL \DIFdelFL{$W_3^i=\gtwo{\vpolyN(X)/(X-\xi^i)}$, $i\in [N]$(needed by prover)
				}%DIFDELCMD < \item %%%
				\item%DIFAUXCMD
				\DIFdelFL{$\gone{\ell(X)}, \gone{\vpolyN(X)}, \gtwo{\vpolyN(X)}$(known by both )
				}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{$\gone{\vpolyN(X)}, \gtwo{\vpolyN(X)}$ (known to both $\prover$ and $\verifier$).
				}\DIFaddendFL \end{itemize}
			
			\underline{Precompute $(\vecT, \vecT')$}:
			\begin{itemize}[leftmargin=1em]
				\item $W_1^i=\gtwo{(T(X)-T(\xi^i))/(X-\xi^i)}$, $i\in [N]$,
				\item \DIFdelbeginFL \DIFdelFL{${W_1^i}'=\gtwo{(T'(X) - T'(\xi^i))/(X-\xi^i)}$}\DIFdelendFL\\ \DIFaddbeginFL \DIFaddFL{${W_1^i}'=\gtwo{(T^*(X) - T^*(\xi^i))/(X-\xi^i)}$}\DIFaddendFL , $i\in [N]$.
			\end{itemize}
			
			{\bf Common Input}: $\srs$, $c_T, c_T', c_\op, c_a, c_w\in \Gone$.\\
			{\bf Prover's Input}: Vectors $\vecT,\vecT',\vec{\op},\vec{a},\vec{w}$ and their encoding polynomials.\\
			
			{\bf Round 1}: Commit to sub RAMs.
			\begin{enumerate}[leftmargin=1em, label=\arabic*.]
				\item $\prover$ computes $\vec{v}=\vecT[\,\vec{a}\,]$, $\vec{v}'=\vecT'[\,\vec{a}\,]$ and the encoding
				polynomials $v(X)$ and \DIFdelbeginFL \DIFdelFL{$v'(X)$}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{$v^*(X)$}\DIFaddendFL .
				\item $\prover$ sends $c_v = \gone{v(X)}$, \DIFdelbeginFL \DIFdelFL{$c_v'=\gone{v'(X)}$}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{$c_v'=\gone{v^*(X)}$}\DIFaddendFL .
				\item $\verifier$ sends $\chi\gets \F$.
			\end{enumerate}
			
			{\bf Round 2}: Execute committed index lookup.
			\begin{enumerate}[leftmargin=1em, label=\arabic*.]
				\item $\prover$ and $\verifier$ compute \DIFdelbeginFL \DIFdelFL{$C_T=c_T + \chi c_T'$, $C_V=c_v + \chi c_v'$}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{$\hat{c}_T=c_T + \chi c_T'$, $\hat{c}_v=c_v + \chi c_v'$}\DIFaddendFL .
				\item $\prover$ computes \DIFdelbeginFL \DIFdelFL{$P(X) = T(X) + \chi T'(X)$, $V(X)=v(X) + \chi v'(X)$}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{$\hat{\vec{T}} = \vecT + \chi \vecT'$, $\hat{\vec{v}}= \vec{v} + \chi \vec{v}'$}\DIFaddendFL .
				\item $\prover$ \DIFdelbeginFL \DIFdelFL{computes $I=\{a_i: i\in [m]\}$, $\setN_I=\{\xi^i: i\in I\}$.
				}%DIFDELCMD < \item %%%
				\item%DIFAUXCMD
				\DIFdelFL{$\prover$ computes polynomials:
				}%DIFDELCMD < \begin{itemize}[leftmargin=1em, label=-]
				%DIFDELCMD < 							\item %%%
				\item%DIFAUXCMD
				\DIFdelFL{Vanishing polynomial $Z_I(X)$ of $\setN_I$.
				}%DIFDELCMD < \item %%%
				\item%DIFAUXCMD
				\DIFdelFL{Polynomial $h(X)=\sum_{i\in [m]}\xi^{a_i}\tau_i(X)$.
				}%DIFDELCMD < \item %%%
				\item%DIFAUXCMD
				\DIFdelFL{Restrictions $P_I(X),\ell_I(X)$ of $P(X),\ell(X)$ on set $I$.
				}%DIFDELCMD < \item %%%
				\item%DIFAUXCMD
				\DIFdelFL{$K(X)=\prod_{i\in [m]}(X-\xi^{a_i})$, $q(X)=K(X)/Z_I(X)$
				}%DIFDELCMD < \item %%%
				\item%DIFAUXCMD
				\DIFdelFL{$D(X)= \sum_{i\in I}\frac{\Delta_i Z_I'(\xi^i)}{Z_\setN'(\xi^i)}\kappa_i(X)$ by interpolation as described in section 5.2
				}%DIFDELCMD < \end{itemize}
				%DIFDELCMD < 						
				
				%DIFDELCMD < 						\item %%%
				\item%DIFAUXCMD
				\DIFdelFL{$\prover$ sends $c_p = \gone{P_I(X)}$,
					$c_z=\gone{Z_I(X)}$, $c_{z2}=\gtwo{Z_I(X)}$, $c_h=\gone{h(X)}$, $c_l = \gone{\ell_I(X)}$,
					$c_k = \gone{K(X)}$, $c_q = \gone{q(X)}$, $c_d=\gone{D(X)}$
				}%DIFDELCMD < \item %%%
				\item%DIFAUXCMD
				\DIFdelendFL \DIFaddbeginFL \DIFaddFL{and }\DIFaddendFL $\verifier$ \DIFdelbeginFL \DIFdelFL{sends $\gamma\gets \F$}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{execute committed index lookup argument in Fig ~\ref{fig:committed-index-lookup},
					with $(\hat{c}_T,c_a,\hat{c}_v)$ as the common input and $(\hat{\vec{T}}, \vec{a},\hat{\vec{v}})$ as prover's input}\DIFaddendFL .
			\end{enumerate}
			
			{\bf Round 3}: \DIFdelbeginFL \DIFdelFL{Prover send aggregated quotients}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{Prove RAMs are $\vec{a}$-identical}\DIFaddendFL .
			\begin{enumerate}[leftmargin=1em, label=\arabic*.]
				\item $\prover$ \DIFdelbeginFL \DIFdelFL{computes $g(X)=P_I(X) + \gamma \ell_I(X) + \gamma^2 Z_I(X)$.
				}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{and $\verifier$ execute argument in Fig ~\ref{fig:a-identical} with common input
					$(c_T,c_T',c_a)$ and prover's input as $(\vecT,\vecT',\vec{a})$.
			}\end{enumerate}
			
			{\bf \DIFaddFL{Round 4}}\DIFaddFL{: Prove sub RAMs are memory-consistent with update.
			}\begin{enumerate}[leftmargin=1em, label=\arabic*.]
				\DIFaddendFL \item $\prover$ \DIFdelbeginFL \DIFdelFL{computes $Q(X) = (g(h(X)) - v(X) -\gamma a(X))/Z_\setV(X)$.
				}%DIFDELCMD < \item %%%
				\item%DIFAUXCMD
				\DIFdelFL{$\prover$ computes: $W = \sum_{i\in [m]} \frac{1}{Z_I'(\xi^i)} (W_1^i + \chi {W_1^i}' + \gamma W_2^i + \gamma^2 W_3^i)$.
				}%DIFDELCMD < \item %%%
				\item%DIFAUXCMD
				\DIFdelFL{$\prover$ sends $W\in \Gtwo$, $c_Q=\gone{Q(X)}$.
				}%DIFDELCMD < \item %%%
				\item%DIFAUXCMD
				\DIFdelendFL \DIFaddbeginFL \DIFaddFL{and }\DIFaddendFL $\verifier$ \DIFdelbeginFL \DIFdelFL{computes $c_g = c_p + \gamma c_l + \gamma^2 c_z$, $C_G = C_T + \gamma\gone{\ell(X)}+\gamma^2\gone{\vpolyN(X)}$}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{execute argument in Fig ~\ref{fig:covering-protocol}
					to check $(c_S, c_o, c_S')\in \CLRAM$ with $c_S = (c_a, c_v)$, $c_S'=(c_a, c_v')$ and
					$c_o=(c_\op,c_a,c_w)$}\DIFaddendFL .
				\item $\verifier$ \DIFdelbeginFL \DIFdelFL{checks: $e(C_G - c_g, \gtwo{1})=e(c_z, W)$.
				}%DIFDELCMD < \item %%%
				\item%DIFAUXCMD
				\DIFdelFL{$\verifier$ checks: $e(c_T-c_{T'}, c_{z2})=e(c_d, \gtwo{\vpolyN(X)})$
				}%DIFDELCMD < \item %%%
				\item%DIFAUXCMD
				\DIFdelFL{$\verifier$ checks: $e(c_z, [1]_2)=e([1]_1, c_{z2})$
				}%DIFDELCMD < \item %%%
				\item%DIFAUXCMD
				\DIFdelFL{$\verifier$ sends $\alpha\gets \F$.
				}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{accepts if all sub-protocols accept.
				}\DIFaddendFL \end{enumerate}
			\DIFdelbeginFL %DIFDELCMD < 
			
			%DIFDELCMD < 					%%%
			\DIFdelFL{Continued in Figure ~\ref{fig:complete-listing-2}
			}\DIFdelendFL \end{mdframed}
		\caption{\DIFdelbeginFL \DIFdelFL{Batching-Efficient }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{Our batching-efficient }\DIFaddendFL RAM \DIFdelbeginFL \DIFdelFL{Protocol}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{protocol}\DIFaddendFL }
		\label{fig:complete-listing}
	\end{figure}
%	
	\section{Fast Lookups from Approximate Pre-Processing}\label{sec:update-protocol}
	%									\input{update-protocol}
	
	
	
	\DIFdelbegin \DIFdel{The recent progress and interest in lookup arguments has been stellar, as exemplified by the series of works
		both in uni-variate setting ~\mbox{%DIFAUXCMD
			\cite{CCS:ZBKMNS22,EPRINT:PosKat22,EPRINT:ZGKMR22,EPRINT:EagFioGab22} }\hskip0pt%DIFAUXCMD
		and multi-variate
		setting ~\mbox{%DIFAUXCMD
			\cite{lasso}}\hskip0pt%DIFAUXCMD
		. However, the excellent online efficiency of current constructions relies on expensive
		$\wt{O}(|T|)$ table-specific  pre-computation }\DIFdelend \DIFaddbegin \DIFadd{In this section, we provide details of the algorithm to construct lookup argument }\DIFaddend for a table \DIFdelbegin \DIFdel{$T$,
		or on tables exhibiting tensor structure as in ~\mbox{%DIFAUXCMD
			\cite{lasso}}\hskip0pt%DIFAUXCMD
		.
		This limits their application in settings where such assumptions are not viable, for example when tables model account balances in
		a layer 2 (L2) blockchain network. We make the first attempt in this direction. Our key idea is to extend the utiltiy of }\DIFdelend \DIFaddbegin \DIFadd{$\vecT$,
		using }\DIFaddend pre-computed parameters \DIFdelbegin \DIFdel{for a table  $\vecT$, to proving lookups from tables $\vecT'\neq \vecT$. Essentially, we show that for $\delta=\Delta(\vecT, \vecT')$,
		an argument for $m$ lookups from $\vecT'$ incurs an additional prover overhead of $(m+\delta)\log^2(m+\delta)$. We note that overhead is additive
		in $\delta$ and that too only }%DIFDELCMD < {\em %%%
	\DIFdel{quasi}%DIFDELCMD < } %%%
	\DIFdel{linear. Our competitive overhead rests on
		several innovative applications of algebraic
		algorithms, which are summarised in Section ~\ref{subsec:comp-algebra-app}.
	}%DIFDELCMD < 
	
	%DIFDELCMD < 						\noindent{\bf Naive approaches are inadequate}%%%
	\DIFdel{: We note that the aforementioned constructions of lookup arguments require encoded quotients
	}\DIFdelend \DIFaddbegin \DIFadd{of a table  which is a small hamming distance away. The dependence on
		pre-computed parameters in several recent lookup arguments such as ~\mbox{%DIFAUXCMD
			\cite{CCS:ZBKMNS22,EPRINT:PosKat22,EPRINT:ZGKMR22,EPRINT:EagFioGab22}
		}\hskip0pt%DIFAUXCMD
		stems from the need to compute an encoded quotient }\DIFaddend of the form\DIFdelbegin \DIFdel{$\gany{(T(X)-T(\xi^i))/(X-\xi^i)}$ for upto $m$ values of $i$ during the proof generation. While constructions ~\mbox{%DIFAUXCMD
			\cite{CCS:ZBKMNS22,EPRINT:PosKat22}
		}\hskip0pt%DIFAUXCMD
		consider quotients encoded in the group $\Gtwo$, the protocol in ~\mbox{%DIFAUXCMD
			\cite{EPRINT:EagFioGab22} }\hskip0pt%DIFAUXCMD
		encodes them in $\Gone$. We use a generic $[\,\cdot\,]_g$ to
		account for protocol-specific choices. We also see that even a small change to the table requires one to update all the quotients (the polynomial $T(X)$ is
		common to all quotients). Updating the $|\vecT|$ quotients for each batch is clearly infeasible. One could consider delaying the updation of the quotients, till
		the time they are actually required in a proof, which happens when the corresponding index in the table is involved in lookup. However, each of the $m$ quotients
		is now potentially ``lagging'' by }\DIFdelend \DIFaddbegin \DIFadd{:
	}\begin{equation}\DIFadd{\label{eq:enc-quotient}
			\gany{Q} = \sum_{i\in I}c_i\gany{\frac{T(X) - T(\xi^i)}{X-\xi^i}}
	}\end{equation}
	\DIFadd{for some $O(m)$ sized set $I$. The quotient in Equation ~\eqref{eq:enc-quotient} can be computed in $O(m)$ cost
		when the quotients $\gany{(T(X)-T(\xi^i))/(X-\xi^i)}$ are available for all $i\in [N]$. In this Section
		we exhibit an algorithm which computes the above with $O((m+\delta)\log^2(m+\delta))$ cost, given access to
		similar quotients for a table at hamming distance }\DIFaddend $\delta$ \DIFdelbegin \DIFdel{updates, so we would need $\Omega(m\delta)$ group operations to refresh all of them. This gives us multiplicative degradation
		with $\delta$, and is clearly unsustainable for reasonable values of $\delta$. We abandon the idea of computing individual encoded quotients, and instead attempt
		to directly compute the aggregate encoding $\gany{(T(X)-T_I(X))/Z_I(X)}$, which as seen earlier is given by the summation below:
	}\begin{displaymath}\DIFdel{%DIFDELCMD < \label{eq:encoded-quotient}%%%
			\gany{\frac{T(X)-T_I(X)}{Z_I(X)}} = \sum_{i\in I}\frac{1}{Z_I'(\xi^i)}\gany{\frac{T(X)-T(\xi^i)}{X-\xi^i}}
	}\end{displaymath}%DIFAUXCMD
	%DIFDELCMD < 						%%%
	\DIFdelend \DIFaddbegin \DIFadd{from $\vecT$. }\DIFaddend We now describe our approach.
	%Recall that our lookup protocol in section 5.1 involves certain precomputations by the prover namely $W_1^i, W_2^i, W_3^i$. $W_2^i$ and $W_3^i$ do not depend on the table. However, $W_1^i$ depends on the lookup table and their values will change even if the table changes by a small amount. It is expensive to recompute all the $W_1^i$ for every small change in the table and this will affect the efficiency of our lookup protocol in the long run.\\\\
	%In this section, we show how to achieve efficient lookups even when the table is changing frequently, as long as the cumulative change in the table is small. \\
	%In particular, we show how the prover can compute $[Q(X)]_2=\gtwo{\frac{T(X)-T_I(X)}{Z_I(X)}}$ without computing all the $W_1^i$(thus minimizing the overhead).\\
	%The overhead(as long as the table doesn't change too much) will be much lower than the time needed for the lookup and so is very practical.
	
	\subsection{Base + Cache approach}\label{subsec:base-cache}
	The key idea we employ is to express the current table $\vecT\in \F^N$ as $\vecTbase + \vecTcache$, where $\vecTbase$ is the table for which we assume that
	the encoded quotients are available (via the $O(N\log N)$ computation), and $\vecTcache$ captures the changes to the table since. We will periodically update (say
	after $s$ batch updates) $\vecTbase$ to current table state, and re-compute all the quotients (we call it the {\em offline} phase).
	We will revisit the question on choosing $s$ optimally later. Let $I\subseteq [N]$ denote the set of indices in the current batch of $m$ lookups. The {\em online}
	phase of our proof generation involves computing the sum in Equation \DIFdelbegin \DIFdel{\eqref{eq:encoded-quotient} }\DIFdelend \DIFaddbegin \DIFadd{\eqref{eq:enc-quotient} }\DIFaddend for the table $\vecT$.
	%that we do not compute $W_1^i$ after each change of the table. Instead, this expensive computation will be done periodically for all $i \in [N]$ after say $s \in \mathbb{Z}$ batches.
	%Let current table $\vecT$ can be represented as $\vecTbase + \vecTcache$ where the vector
	%$\vecTbase$ denotes the base table (with respect to which $W_1^i$ was last computed for all $i \in [N]$) and the vector $\vecTcache$ corresponds to the changes
	%that have happened to the base table since the last rebasing (rebasing denotes computation of all $W_1^i$)\\\\
	%Thus, there is an \textbf{online} phase which happens after every batch (which includes computation of $[Q(X)]_2$ among other things) and an \textbf{offline} phase which consists of the rebasing(this is all prover computations) \\\\
	\DIFdelbegin %DIFDELCMD < \begin{comment}%DIFDELCMD < 
	%DIFDELCMD < 							\noindent{\bf Offline Phase}: This computation is executed once after every $s$ rounds. Here, the prover updates the base vector $\vecTbase$ with the changes in the cache vector
	%DIFDELCMD < 							$\vecTcache$ by setting $\vecTbase := \vecTbase + \vecTcache$ and simultaneously clears the cache vector by setting
	%DIFDELCMD < 							$\vecTcache = 0$.\\
	%DIFDELCMD < 							It computes the commitment of $T_b$ as well\\
	%DIFDELCMD < 							It also re-computes the $\mathsf{KZG}$ opening proofs $[W_1^i(X)]_2$ for $i\in [N]$ where
	%DIFDELCMD < 							$W_1^i(X) = (\Tbasepoly{X} - t_i)/(X-\xi^i)$. Here, $t_i=\Tbasepoly{\xi^i}$ are the coordinates
	%DIFDELCMD < 							of the updated base vector $\vecTbase$.\\
	%DIFDELCMD < 							As mentioned in section 5.1 this can be done in $O(N\log N)$ group and field operations.\\\\
	%DIFDELCMD < 							\noindent{\bf Online Phase}:
	%DIFDELCMD < 							The online phase happens for every batch because the purpose of this phase is to ensure that all the things needed for the current execution of the lookup protocol are available. We show how the prover computes the next table $T'$ from the current table $T$ and the new Cache vector from the old cache vector (by an inductive argument this suffices)
	%DIFDELCMD < 							\begin{enumerate}[leftmargin=1em]
	%DIFDELCMD < 								\item Prover has the $T$ for the current round and the commitment $[T(X)]_1$ as well(because these are just the $T'$, $[T'(x)]_1$ of the last round)
	%DIFDELCMD < 								\item The $\vecTcache$ and $\vecTcache(X)$ is also updated to the start of the current round (contains information till previous round:$\vecT=\vecTbase+\vecTcache$)
	%DIFDELCMD < 								\item The prover updates the cache using the current batch: $\vec{T'}_{\mathsf{ch}}[i] = \vec{T}_{\mathsf{ch}}[i] + \Delta_i$ for $i\in I$ in $O(m)$ $\F$ operations
	%DIFDELCMD < 								\item Here $\Delta_i$ for all $i \in I$ is the change that will happen to $\vecT$ \textbf{during the current round}
	%DIFDELCMD < 								\item Prover computes the commitment to the new cache polynomial:
	%DIFDELCMD < 								$$[\vec{T'}_{\mathsf{ch}}(X)]_1=[\vec{T}_{\mathsf{ch}}(X)]_1+\sum_{i\in I}\Delta_i[\mu_i(X)]_1$$ in
	%DIFDELCMD < 								$O(m)$ $\Gone$ operations.
	%DIFDELCMD < 								\item Prover also gets ${T}'$ as ${T'}[i]=T_b[i]+ \vec{T'_{\text{ch}}}[i]$ using the $T_b$ and the latest cache
	%DIFDELCMD < 								\item Prover computes the commitment to the new table $\vecT'$: $[T'(X)]_1=[\Tbasepoly{X}]_1+[\vec{T'}_{\mathsf{ch}}(X)]_1$
	%DIFDELCMD < 								
	%DIFDELCMD < 								\item In addition, the other things (apart from $[Q(X)]_2$) needed for the current round of the lookup protocol are also computed by the prover as described in the lookup protocol in section 5.1 as it is just naive computation
	%DIFDELCMD < 								
	%DIFDELCMD < 							\end{enumerate}
	%DIFDELCMD < 							\subsection{Computation of $[Q(X)]_2$}
	%DIFDELCMD < 							Clearly, it suffices to efficiently compute $[Q(X)]_2$ where $[Q(X)]_2=\gtwo{\frac{T(X)-T_I(X)}{Z_I(X)}}$. We have the information of $[\Tbasepoly{X}-\Tbasepoly{\xi^i}/(X-\xi^i)]_2$. For this, we have the following lemma:
	%DIFDELCMD < 						%DIFDELCMD < \end{comment}
	%DIFDELCMD < 						%%%
	\DIFdelend \DIFaddbegin \begin{comment}
	\noindent{\bf Offline Phase}: This computation is executed once after every $s$ rounds. Here, the prover updates the base vector $\vecTbase$ with the changes in the cache vector
	$\vecTcache$ by setting $\vecTbase := \vecTbase + \vecTcache$ and simultaneously clears the cache vector by setting
	$\vecTcache = 0$.\\
	It computes the commitment of $T_b$ as well\\
	It also re-computes the $\mathsf{KZG}$ opening proofs $[W_1^i(X)]_2$ for $i\in [N]$ where
	$W_1^i(X) = (\Tbasepoly{X} - t_i)/(X-\xi^i)$. Here, $t_i=\Tbasepoly{\xi^i}$ are the coordinates
	of the updated base vector $\vecTbase$.\\
	As mentioned in section 5.1 this can be done in $O(N\log N)$ group and field operations.\\\\
	\noindent{\bf Online Phase}:
	The online phase happens for every batch because the purpose of this phase is to ensure that all the things needed for the current execution of the lookup protocol are available. We show how the prover computes the next table $T'$ from the current table $T$ and the new Cache vector from the old cache vector (by an inductive argument this suffices)
	\begin{enumerate}[leftmargin=1em]
	\item Prover has the $T$ for the current round and the commitment $[T(X)]_1$ as well(because these are just the $T'$, $[T'(x)]_1$ of the last round)
	\item The $\vecTcache$ and $\vecTcache(X)$ is also updated to the start of the current round (contains information till previous round:$\vecT=\vecTbase+\vecTcache$)
	\item The prover updates the cache using the current batch: $\vec{T'}_{\mathsf{ch}}[i] = \vec{T}_{\mathsf{ch}}[i] + \Delta_i$ for $i\in I$ in $O(m)$ $\F$ operations
	\item Here $\Delta_i$ for all $i \in I$ is the change that will happen to $\vecT$ \textbf{during the current round}
	\item Prover computes the commitment to the new cache polynomial:
	$$[\vec{T'}_{\mathsf{ch}}(X)]_1=[\vec{T}_{\mathsf{ch}}(X)]_1+\sum_{i\in I}\Delta_i[\mu_i(X)]_1$$ in
	$O(m)$ $\Gone$ operations.
	\item Prover also gets ${T}'$ as ${T'}[i]=T_b[i]+ \vec{T'_{\text{ch}}}[i]$ using the $T_b$ and the latest cache
	\item Prover computes the commitment to the new table $\vecT'$: $[T'(X)]_1=[\Tbasepoly{X}]_1+[\vec{T'}_{\mathsf{ch}}(X)]_1$
	
	\item In addition, the other things (apart from $[Q(X)]_2$) needed for the current round of the lookup protocol are also computed by the prover as described in the lookup protocol in section 5.1 as it is just naive computation
	
	\end{enumerate}
	\subsection{Computation of $[Q(X)]_2$}
	Clearly, it suffices to efficiently compute $[Q(X)]_2$ where $[Q(X)]_2=\gtwo{\frac{T(X)-T_I(X)}{Z_I(X)}}$. We have the information of $[\Tbasepoly{X}-\Tbasepoly{\xi^i}/(X-\xi^i)]_2$. For this, we have the following lemma:
	\end{comment}
	\DIFaddend The following Theorem determines the efficiency of the online phase of our prover.
	\begin{theorem}\label{thm:approx-setup}
		Let $N,\xi$ be as defined previously. \DIFdelbegin \DIFdel{Suppose we are given
		}\DIFdelend \DIFaddbegin \DIFadd{Given
		}\DIFaddend $\kzg$ proofs $\{W_i\}_{i=1}^N$ with $W_i=\gany{\Tbasepoly{X} - \Tbasepoly{\xi^i}/(X-\xi^i)}$, where
		$\Tbasepoly{X}=\enc{T_{\mathsf{b}}}{\setN}$ encodes a vector $\vecTbase\in \F^N$\DIFdelbegin \DIFdel{.
			Let $I \subset [N]$, $\setN_I=\{\xi^i:i\in I\}$, $Z_I(X)$ denote the vanishing polynomial of $\setN_I$ and
			$T_I(X)$ be the restriction of polynomial $T(X)$ on $\setN_I$.
			Then, }\DIFdelend \DIFaddbegin \DIFadd{, for any $I\subseteq [N]$, }\DIFaddend there exists an algorithm to compute \DIFdelbegin \DIFdel{$\kzg$ multi-opening proof
			$\gany{Q(X)}=\gany{(T(X) - T_I(X)/Z_I(X)}$ for encoding }\DIFdelend \DIFaddbegin \DIFadd{$\gany{Q}$ as given in Equation ~\eqref{eq:enc-quotient}
			for polynomial }\DIFaddend $T(X)=\enc{T}{\setN}$ \DIFdelbegin \DIFdel{of }\DIFdelend \DIFaddbegin \DIFadd{encoding the }\DIFaddend vector $\vecT\in \F^N$ using $O((\delta + |I|) \log^2 (\delta + |I|))$ $\F$-operations
		and $O(\delta + |I|)$ $\mathbb{G}$-operations. Here, $\delta$ denotes the hamming distance
		between vectors $\vecTbase$ and $\vecT$.
	\end{theorem}
	\begin{proof}
		Let $\vecT=\vecTbase+\vecTcache$ and thus $T(X)=\Tbasepoly{X}+\Tcachepoly{X}$.
		Define $K=I\cup \{j\in [N]: \vecTcache[\,j\,]\neq 0\}$ as a set which captures the indices where the current table $\vecT$ differs from the base $\vecTbase$,
		where we explicitly also include the lookup indices $I$ in $K$. For $j\in K$, let $\vecTcache[j]=\Delta t_j$. Then $\Tcachepoly{X}=\sum_{j\in K}\Delta t_j\mu_j(X)$.
		%By definition of $K$, $|K|\leq \delta +|I|$. So, we need to bound $\Gtwo$ operations by $O(|K|)$ and field operations by $O(|K| \log^2|K|)$\\
		%First of all note that:
		We write the quotient $Q(X)$ as:
		{
			\DIFdelbegin %DIFDELCMD < \small
			%DIFDELCMD < 								%%%
			\DIFdelend \begin{equation*}
				\DIFdelbegin %DIFDELCMD < \begin{aligned}
				%DIFDELCMD < 										Q(X) = \sum_{i\in \setind}\frac{1}{z_I'(\xi^i)}\left(\frac{\Tbasepoly{X} - \Tbasepoly{\xi^i}}{X-\xi^i}\right)
				%DIFDELCMD < 										+ \sum_{i\in \setind}\frac{1}{z_I'(\xi^i)}\left(\frac{\Tcachepoly{X} - \Tcachepoly{\xi^i}}{X-\xi^i}\right)
				%DIFDELCMD < 									\end{aligned}%%%
				\DIFdelend \DIFaddbegin \begin{aligned}
					Q(X) = \sum_{i\in \setind}c_i\left(\frac{\Tbasepoly{X} - \Tbasepoly{\xi^i}}{X-\xi^i}\right)
					+ \sum_{i\in \setind}c_i\left(\frac{\Tcachepoly{X} - \Tcachepoly{\xi^i}}{X-\xi^i}\right)
				\end{aligned}\DIFaddend 
				%\label{eq:Q2-upd}
			\end{equation*}
		}
		\DIFdelbegin %DIFDELCMD < 
		
		%DIFDELCMD < 							%%%
		\DIFdelend From above, we have $\gany{Q(x)}=\gany{\Qbasepoly{x}}+\gany{\Qcachepoly{x}}$ where
		\begin{gather*}
			\Qbasepoly{X}=\sum_{i\in \setind}\DIFaddbegin \DIFadd{c_i }\DIFaddend (\DIFdelbegin \DIFdel{Z_I'(\xi^i))^{-1} (}\DIFdelend \Tbasepoly{X}-\Tbasepoly{\xi^i})/(X-\xi^i) \\
			\Qcachepoly{X}=\sum_{i\in \setind}\DIFaddbegin \DIFadd{c_i}\DIFaddend (\DIFdelbegin \DIFdel{Z_I'(\xi^i))^{-1} (}\DIFdelend \Tcachepoly{X}-\Tcachepoly{\xi^i})/(X-\xi^i)
		\end{gather*}
		We can compute
		$\gany{\Qbasepoly{X}}$ from the pre-computed KZG openings of $\Tbasepoly{X}$ at points $\xi^i,i\in I$ using $O(|I|)$ group operations and
		$O(|I|\log^2 |I|)$ field operations. Therefore, it suffices to compute $\gany{\Qcachepoly{X}}$ efficiently.
		%\textbf{Thus, it suffices to describe the computation for $\elttwo{\Qcachepoly{X}}$. }\\
		Using $\Tcachepoly{X}=\sum_{j\in K}\Delta t_j\mu_j(X)$
		\DIFdelbegin \DIFdel{and setting $c_i=(1/z_I'(\xi^i))$ for $i\in I$,
		}\DIFdelend we write $\Qcachepoly{X}$ as linear combination of table-independent polynomials:
		\begin{align*}
			\Qcachepoly{X} &= \sum_{i\in \setind} c_i\sum_{j\in K} \Delta t_j\frac{\mu_j(X)-\mu_j(\xi^i)}{X-\xi^i} \\
			&= \sum_{i\in \setind} c_i\Delta t_i\frac{\mu_i(X) - 1}{X-\xi^i} + \sum_{i\in \setind}\sum_{j\in K\setminus\{i\}}c_i\Delta t_j\frac{\mu_j(X)}{X-\xi^i}
		\end{align*}
		Now, we can write $\gany{\Qcachepoly{X}}=\elany{\Qcachepolyone{X}} + \elany{\Qcachepolytwo{X}}$ where:
		{
			\DIFdelbegin %DIFDELCMD < \small
			%DIFDELCMD < 								%%%
			\DIFdelend \begin{gather*}
				\Qcachepolyone{X}=\sum_{i\in \setind}c_i\Delta t_i\frac{\mu_i(X)-1}{X-\xi^i},\,
				\Qcachepolytwo{X}=\sum_{i\in \setind}\sum_{j\in K\setminus \{i\}} c_i\Delta t_j\frac{\mu_j(X)}{X-\xi^i}
			\end{gather*}
		}
		The term $\gany{\Qcachepolyone{X}}$ can be computed using $O(|I|)$ group operations by augmenting the setup with pre-computed
		$\kzg$ opening proofs of polynomials $\mu_i(X)$ at $\xi^i$ for $i\in [N]$. This adds $O(N)$ to the setup parameters, while the computation
		can be done in $O(N\log N)$ time with methods similar to existing pre-computed parameters. This eventually leaves us with $\elany{\Qcachepolytwo{X}}$.
		%That is by precomputing $[\frac{\mu_i(X)-1}{X-\xi^i}]_2$. This requires just $N$ more precomputations and can be done along with the other precomputations which are done in the lookup protocol\\
		Next, we synthesize the polynomial $\Qcachepolytwo{X}$ in a form that reduces group operations required to compute its encoding.
		%\textbf{Thus, it suffices to describe the computation for $\elttwo{\Qcachepolytwo{X}}$}:
		\begin{align}\label{eq:Qcachepoly2}
			&\Qcachepolytwo{X} = \sum_{i\in \setind}c_i\sum_{j\in K\setminus \{i\}} \Delta t_j\mu_j(X)/(X-\xi^i) \nonumber \\
			&\quad = \sum_{i\in\setind}c_i\sum_{j\in K\setminus \{i\}}\frac{\Delta t_j}{Z_{\nroots}'(\xi^j)} \frac{Z_{\nroots}(X)}{(X-\xi^i)(X-\xi^j)} \nonumber \\
			%\intertext{Above, we expanded $\mu_j(X)$. Now using $Z_\nroots'(\xi^j)=N\xi^{-j}$ and using partial fractions}
			&\quad = N^{-1}\sum_{i\in\setind}c_i\sum_{j\in K\setminus \{i\}}\frac{\xi^j\Delta t_j}{\xi^i-\xi^j}
			\left(\frac{Z_\nroots(X)}{X-\xi^i} - \frac{Z_\nroots(X)}{X-\xi^j}\right) \nonumber \\
			&\quad = N^{-1}\sum_{i\in\setind}\left(c_i\cdot \sum_{j\in K\setminus \{i\}} \frac{\xi^j\Delta t_j}{\xi^i-\xi^j}\right)\frac{Z_\nroots(X)}{X-\xi^i} \nonumber \\
			&\qquad + \DIFaddbegin \DIFadd{N^{-1} }\DIFaddend \sum_{j\in K}\left(\xi^j\Delta t_j\cdot \sum_{i\in \setind\setminus \{j\}}\frac{c_i}{\xi^j-\xi^i}\right)\frac{Z_{\nroots}(X)}{X-\xi^j}
		\end{align}
		In the first step, we substituted $\mu_j(X)$, while in the final step we re-arranged the summation to accumulate the scalar factor for
		each distinct polynomial of the form $\vpolyN(X)/(X-\xi^i)$. Define scalars $a_i$, $i\in I$ and $b_j$, $j\in K$ as below:
		%this last equality, the first term is just the first term of the distributive property in finite fields.\\
		%The second term is just the second term of the distributive property in finite fields except that the order of the sums is reversed. This follows from the following fact \\
		%\begin{fact}
		%    $\sum_{i \in I} \sum_{j \in K \setminus \{i\}} f(i,j)=\sum_{j \in K} \sum_{i \in I \setminus \{j\}} f(i,j) $
		%\end{fact}
		%In the above equation \eqref{eq:Qcachepoly2}, let us define:
		\begin{gather}\DIFdelbegin %DIFDELCMD < \label{eq:scalars}
			%DIFDELCMD < 								%%%
			\DIFdelend \DIFaddbegin \label{eq:scalars1}
			\DIFaddend a_i = \sum_{j\in K\setminus \{i\}}\frac{\xi^j\Delta t_j}{\xi^i-\xi^j}, i\in \setind\quad
			b_j=  \sum_{i\in \setind\setminus \{j\}}\frac{c_i}{\xi^j - \xi^i}, j\in K
			%W_3^i(X) = \frac{Z_\nroots(X)}{X-\xi^i}, \text{ for } i\in [N]
		\end{gather}
		Now, \DIFdelbegin \DIFdel{recalling that $W_3^j=\gany{\vpolyN(X)/(X-\xi^i)}$}\DIFdelend \\ \DIFaddbegin \DIFadd{define
			$W_3^j:=\gany{\vpolyN(X)/(X-\xi^j)}$. We see that $W_3^j$ is just the KZG opening proof of the polynomial $\vpolyN(X)$ evaluated at $\xi^j$ for $j \in [N]$. These can be precomputed one time and it adds $O(N)$ to the setup parameters and the computation can be done in $O(N\log N)$ time.}\\ \DIFadd{Now}\DIFaddend , we see that   $\elany{\Qcachepolytwo{X}}$ can be written as linear combination of $O(|K|+|I|)$ group elements.
		\begin{equation}\label{eq:Qcachepoly2commit}
			\gany{\Qcachepolytwo{X}} = N^{-1}\left(\sum_{i\in\setind}(c_ia_i)\cdot W_3^i + \sum_{j\in K}(\xi^j \Delta t_j b_j)\cdot W_3^j\right)
		\end{equation}
		Now\DIFdelbegin \DIFdel{$c_i=(z_I(\xi^i))^{-1}$ for all $i \in \setind$ can be determined in $O(|I|\log^2 |I|)$ field operations by evaluating $Z_{\setind}'(X)$ on $H_{\setind}$}\DIFdelend \DIFaddbegin \DIFadd{, $c_i$ are known constants depending on the specific lookup scheme}\DIFaddend . So, given $\{a_i\}_{i\in I}, \{b_j\}_{j\in K}$, $\gany{\Qcachepolytwo{X}}$ can be computed in $O(|\setind|+|K|)$ group operations.
		While we have diligently reduced the group operations, we still seem to need $O(|I||K|)=O(m\delta)$ field operations. We clearly need better than
		naive way of computing the scalars in \DIFdelbegin \DIFdel{\eqref{eq:scalars} }\DIFdelend \DIFaddbegin \DIFadd{\eqref{eq:scalars1} }\DIFaddend to obtain additive overhead in $\delta$. This is what we consider next.
		\DIFdelbegin \DIFdel{Routine calculation shows that we can write $a_i$ as:
		}\begin{displaymath}
			\DIFdel{a_i = -\Delta T + \Delta t_i + \xi^i\sum_{j\in K\setminus\{i\}}\frac{\Delta t_j}{\xi^i-\xi^j}, i\in I
		}\end{displaymath}%DIFAUXCMD
		%DIFDELCMD < 							%%%
		\DIFdel{where in the above, we have $\Delta T=\sum_{j\in K}\Delta t_j$.
		}\DIFdelend %DIF > Routine calculation shows that we can write $a_i$ as:
		%DIF > \begin{equation*}
		%DIF >  a_i = -\Delta T + \Delta t_i + \xi^i\sum_{j\in K\setminus\{i\}}\frac{\Delta t_j}{\xi^i-\xi^j}, i\in I
		%DIF >   \end{equation*}
		%DIF >  where in the above, we have $\Delta T=\sum_{j\in K}\Delta t_j$.
		%a_i = -\sum_{j\in K\setminus \{i\}}\Delta t_j + \xi^i\sum_{j\in K\setminus \{i\}}\frac{\Delta t_j}{\xi^i-\xi^j} $$
		%This is because:
		%$$a_i+\sum_{j\in K\setminus \{i\}}\Delta t_j= \sum_{j \in K \setminus \{i\}}\frac{\xi^j\Delta t_j}{\xi^i-\xi^j}+\Delta t_j$$
		%$$=\sum_{j \in K \setminus \{i\}}\frac{\xi^i\Delta t_j}{\xi^i-\xi^j} = \xi^i\sum_{j\in K\setminus \{i\}}\frac{\Delta t_j}{\xi^i-\xi^j}$$
		%Now, define $\Delta T=\sum_{j\in K}\Delta t_j$\\
		\DIFdelbegin %DIFDELCMD < 
		
		%DIFDELCMD < 							%%%
		\DIFdelend %Here computing $\Delta T$ is a one time computation (per batch). It can be computed from the knowledge of $T_{\text{ch}}$ in the online phase.
		%We have:
		%$$  $$
		%Suppose we get $\sum_{j\in K\setminus\{i\}}\frac{\Delta t_j}{\xi^i-\xi_j}$ for all $i \in I$ efficiently. Then $a_i$ for all $i \in I$ can be obtained in $O(|I|)$ field operations. \\
		%\textbf{Thus, to get $a_i$ for all $i \in I$ it suffices to describe the computation of $e_i=\sum_{j\in K\setminus\{i\}}\frac{\Delta t_j}{\xi^i-\xi^j}$ for all $i \in I$}\\\\
		\DIFdelbegin \DIFdel{Above implies that to compute $a_i, i\in I$ efficiently, it is sufficient to efficiently
			compute $e_i=\sum_{j\in K\setminus\{i\}}\frac{\Delta t_j}{\xi^i-\xi^j}$ for all $i \in I$}\DIFdelend \DIFaddbegin \DIFadd{Let $d_j:=\xi^j \Delta t_j$. Then we have from Eq ~\eqref{eq:scalars1}:}\\
		\begin{gather}\label{eq:scalars}
			\DIFadd{a_i = \sum_{j\in K\setminus \{i\}}\frac{d_j}{\xi^i-\xi^j}, i\in \setind\quad
				b_j=  \sum_{i\in \setind\setminus \{j\}}\frac{c_i}{\xi^j - \xi^i}, j\in K
				%DIF > W_3^i(X) = \frac{Z_\nroots(X)}{X-\xi^i}, \text{ for } i\in [N]
		}\end{gather}
		\DIFadd{So, to compute $a_i$ and $b_j$, it suffices to compute }{\em \DIFadd{reciprocal sums}} \DIFadd{efficiently}\DIFaddend . Our next lemma claims that
		such \DIFdelbegin %DIFDELCMD < {\em %%%
		\DIFdel{reciprocal sums }%DIFDELCMD < } %%%
		\DIFdelend \DIFaddbegin \DIFadd{reciprocal sums }\DIFaddend can be computed efficiently.
		\DIFdelbegin \DIFdel{The computation of $b_j,j\in K$ can also be reduced a similar computation.
			We defer this reduction and }\DIFdelend \DIFaddbegin \DIFadd{We defer }\DIFaddend the full proof of Lemma ~\ref{lem:sum-computation} to the Appendix, but illustrate the key ideas in the proof.
		%Our requirement is now to bound the number of field operations for $e_i$ and for $b_j$. For this, we invoke the following lemma with the proof in the appendix.
		\begin{lemma}\label{lem:sum-computation}
			Let $I\subset K\subset [N]$ and let \DIFdelbegin \DIFdel{$e_i$ }\DIFdelend \DIFaddbegin \DIFadd{$a_i$ }\DIFaddend for all $i \in \setind$ and $b_j$ for all $j \in K$ be as described above.
			Then, \DIFdelbegin \DIFdel{$e_i$ }\DIFdelend \DIFaddbegin \DIFadd{$a_i$ }\DIFaddend for all $i \in I$ and $b_j$ for all $j \in K$ can be computed in $O(|K|\log^2|K|)\, \mathbb{F}$ operations\DIFaddbegin \DIFadd{.
			}\DIFaddend \end{lemma}
		\begin{proof}[Proof-Sketch]
			\DIFaddbegin \DIFadd{We sketch the proof here for $a_i$.
			}\DIFaddend First, we mention that the special case of the lemma when \DIFdelbegin \DIFdel{$\Delta t_j=1$ }\DIFdelend \DIFaddbegin \DIFadd{$d_j=1$ }\DIFaddend for all $j\in K$ admits an efficient computation due to the following identity
			proved in Lemma ~\ref{lem:sumtoder}.
			\begin{equation*}
				\frac{Z_K''(\xi^i)}{Z_K'(\xi^i)} = 2\sum_{j\in K\setminus \{i\}}\frac{1}{\xi^i-\xi^j}
			\end{equation*}
			for $Z_K(X)=\prod_{i\in K}(X-\xi^i)$. The polynomial $Z_K$ can be computed in $O(|K|\log^2|K|)$ and subsequent evaluations of its first two
			derivatives can also be evaluated on the set $\{\xi^i: i\in I\}$ with the same complexity. However, to deal with arbitrary values of \DIFdelbegin \DIFdel{$\Delta t_j$ }\DIFdelend \DIFaddbegin \DIFadd{$d_j$ }\DIFaddend we
			need more ingenuity. We will {\em imagine} \DIFdelbegin \DIFdel{$\Delta t_j$ }\DIFdelend \DIFaddbegin \DIFadd{$d_j$ }\DIFaddend to be $p(\xi^j)$ for some polynomial $p(X)$. Moreover, we demand that $p(\xi^j)=0$ for $j\not\in K$.
			We will not compute such a polynomial $p$, as it has degree $O(N)$, but view it as an ``oracle'' which we can hopefully query at the points we need.
			Then it can be seen that \DIFdelbegin \DIFdel{$e_i=g_i(\xi^i) - r_i(\xi^i)$ }\DIFdelend \DIFaddbegin \DIFadd{$a_i=g_i(\xi^i) - r_i(\xi^i)$ }\DIFaddend for rational functions $g_i(X)$ and $r_i(X)$ defined by:
			\begin{align}\label{eq:rat-fun-f}
				g_i(X) =\sum_{j\in [N]\setminus i}\frac{p(X)}{X-\xi^j},\quad
				r_i(X) =\sum_{j\in [N]\setminus i} \frac{p(X)-p(\xi^j)}{X-\xi^j}
			\end{align}
			Now, $g_i(\xi^i)$ for $i\in I$ turns out to be (using the special case above):
			$$p(\xi^i)\sum_{j\in K\setminus \{i\}} 1/(\xi^i-\xi^j)=\DIFdelbegin \DIFdel{\Delta t}\DIFdelend \DIFaddbegin \DIFadd{d}\DIFaddend _i(Z_K''(\xi^i)/Z_K'(\xi^i))/2$$
				Defining $u(X,Y)=(p(X)-p(Y))/(X-Y)$, we can write $r_i(\xi^i)$ as:
				\begin{equation}
					r_i(X) = \sum_{j\in [N]}u(X,\xi^j) - u(X,\xi^i)
				\end{equation}
				\DIFdelbegin \DIFdel{Using the fact }\DIFdelend \DIFaddbegin \DIFadd{Observe }\DIFaddend that $u(X,X)=p'(X)$ \DIFaddbegin \DIFadd{and so $u(X,X)$ }\DIFaddend gives the formal derivative of polynomial $p(X)$\DIFdelbegin \DIFdel{, we }\DIFdelend \DIFaddbegin \DIFadd{. We }\DIFaddend get
				$r_i(\xi^i)=r(\xi^i)-p'(\xi^i)$ for all $i\in I$, where $r(X)=\sum_{j\in [N]}u(X,\xi^j)$. Fortunately,
				$r(X)$ is simply $Nu(X,0)=N(p(X) - p(0))/X$, a fact that follows from \DIFdelbegin \DIFdel{univariate }\DIFdelend \DIFaddbegin \DIFadd{uni-variate }\DIFaddend sum-check. The problem
				thus reduces to being able to compute \DIFdelbegin \DIFdel{derivates }\DIFdelend \DIFaddbegin \DIFadd{derivatives }\DIFaddend $p'(\xi^i)$ for $i\in I$ and the value $p(0)$. Before
				concluding the proof-sketch, we briefly highlight the structure of the polynomial $p(X)$. Since $p(X)$
				vanishes for $p(\xi^i)$ for $i\not\in K$, it can we written as the product $\widehat{Z}_K(X)q(X)$ where
				$\widehat{Z}_K$ is the vanishing polynomial of ``complementary'' roots of unity $\{\xi^i: i\not\in K\}$
				and $q$ is a low-degree ($<K$) polynomial. Assuming we can interpolate $q(X)$, we can write:
				\[ p'(\xi^i) = \widehat{Z}_K(\xi^i)q'(\xi^i) + \widehat{Z}_K'(\xi^i)q(\xi^i) \DIFdelbegin \DIFdel{. }\DIFdelend \]
				In the above expression, we require evaluations of high-degree polynomials $\widehat{Z}_K(X)$ and $\widehat{Z}_K'(X)$
				at $\xi^i$, $i\in I$. This is discussed in Lemma ~\ref{lem:zk-hat} and other related lemmas in \DIFdelbegin \DIFdel{Section }\DIFdelend \DIFaddbegin \DIFadd{Appendix }\DIFaddend ~\ref{subsec:sub-results},
				and motivates the at times tedious algebra there. We conclude the proof-sketch, deferring the missing details to the full
				proof in \DIFdelbegin \DIFdel{Section }\DIFdelend \DIFaddbegin \DIFadd{Appendix }\DIFaddend ~\ref{subsec:sum-computation}.
			\end{proof}
			
			From the Lemma ~\ref{lem:sum-computation}, we conclude that the scalars $a_i,i\in I$ and $b_j, j\in K$ can be computed in
			time $O(|K|\log^2 |K|)$, which proves the bound in Theorem ~\ref{thm:approx-setup}.
			%have shown that the field operations needed to get $e_i$ and $b_j$ and thus $\elttwo{\Qcachepolytwo{X}}$ is $O(|K|\log^2|K|)$. \\
			
			%This completes the proof of Lemma \ref{lem:approx-setup}.
		\end{proof}
		
		\subsection{Amortized Sublinear Batching}\label{sec:amortization}
		We now return to the question of how frequently should we run the offline phase to compute full parameters.
		For concrete analysis, let $s$ be the period after which the rebasing takes place; i.e., after $s$ batches of $m$ operations
		each, we set the base table $\vecTbase$ to the current table, setting $\vecTcache=\vec{0}$. At this point we also compute all
		encoded quotients for $\vecTbase$ using the $O(N\log N)$ algorithm of ~\cite{EPRINT:FeiKho23}. Consider $\delta\leq ms$ as
		the upper-bound on $\delta$, and distributing the cost of re-basing, the amortized overhead for the batch of $m$ operations is:
		$O(ms \log^2(ms)+\frac{N\log N}{s})$ $\mathbb{F}$-operations and $O(ms +\frac{N\log N}{s})$ $\mathbb{G}$-operations. Ignoring
		the logarithmic factors, the cost is minimized by setting $s\approx \sqrt{N/m}$, resulting in amortized prover overhead of
		$\wt{O}(\sqrt{mN})$. We note that the above analysis considers the worst case scenario, where each update affects a distinct
		position in the table. In settings, where frequency of updates is non-uniform accross positions in the table (e.g, in the
		blockchain example, if bulk of transactions come from small number of clients), we may be able to defer the offline phase even longer.
		Same is also true for settings where updates to the table are infrequent.
		
%		
%		
		\section{Experiments}\label{sec:experiments}
		%DIF < \input{experiments}
		%DIF > 	\input{experiments}
		\DIFaddbegin 
		
		
		
		\DIFaddend 
		

		
		In this section we \DIFdelbegin \DIFdel{report on the experimental }\DIFdelend \DIFaddbegin \DIFadd{present a concrete }\DIFaddend evaluation of our \DIFdelbegin \DIFdel{batching-efficient }\DIFdelend \DIFaddbegin \DIFadd{batching efficient }\DIFaddend RAM \DIFaddbegin \DIFadd{and compare it to the
			prior works on batching efficient RAM ~\mbox{%DIFAUXCMD
				\cite{USENIX:OWWB20,CCS:CFHKKO22}}\hskip0pt%DIFAUXCMD
		}\DIFaddend .
		\DIFdelbegin \DIFdel{In addition, we }\DIFdelend \DIFaddbegin \DIFadd{We }\DIFaddend also \DIFdelbegin \DIFdel{report on }\DIFdelend \DIFaddbegin \DIFadd{separately benchmark }\DIFaddend the \DIFdelbegin \DIFdel{overhead (as a function }\DIFdelend \DIFaddbegin \DIFadd{effectiveness }\DIFaddend of \DIFdelbegin \DIFdel{changes since last parameter computation)
			incurred by }\DIFdelend our \DIFdelbegin \DIFdel{updatable lookup arguments }\DIFdelend \DIFaddbegin \DIFadd{approach of computing encoded quotients presented
			in Section ~\ref{sec:update-protocol} }\DIFaddend over the \DIFdelbegin \DIFdel{vanilla lookup arguments with read-only table}\DIFdelend \DIFaddbegin \DIFadd{na\"{i}ve approach}\DIFaddend .
		\DIFdelbegin \DIFdel{We
			benchmark this overhead over vanilla implementations of Caulk+ ~\mbox{%DIFAUXCMD
				\cite{EPRINT:PosKat22} }\hskip0pt%DIFAUXCMD
			and
			CQ ~\mbox{%DIFAUXCMD
				\cite{EPRINT:EagFioGab22}}\hskip0pt%DIFAUXCMD
			, both of which we also implement (as their non zero-knowledge variants)
			for our evaluation. }\DIFdelend %DIF > on the experimental evaluation of our batching-efficient RAM.
		%DIF > In addition, we also report on the overhead (as a function of changes since last parameter computation)
		%DIF > incurred by our updatable lookup arguments over the vanilla lookup arguments with read-only table. We
		%DIF > benchmark this overhead over vanilla implementations of Caulk+ ~\cite{EPRINT:PosKat22} and
		%DIF > CQ ~\cite{EPRINT:EagFioGab22}, both of which we also implement (as their non zero-knowledge variants)
		%DIF > for our evaluation.
		Our implementation is built on top of existing implementation ~\cite{caulk-implementation}
		of lookup argument Caulk ~\cite{CCS:ZBKMNS22}. We make our implementation available at
		\url{https://anonymous.4open.science/r/updatableRAM/}\DIFdelbegin \DIFdel{for anonymous submission}\DIFdelend . 
		\DIFdelbegin \DIFdel{We later plan to
			make the implementation available as open-source project.
		}\DIFdelend %DIF > for anonymous submission. 
		%DIF > We later plan to
		%DIF > make the implementation available as open-source project.
		\DIFaddbegin 
		
		\noindent{\bf Experimental Test-bed.} \DIFaddend All the benchmarks were run single-threaded on a commodity configuration featuring a
		$2.1GHz$ Intel-I5 processor, $16$ GB memory running Ubuntu Linux 22.04. The implementation was compiled using {\tt --release}
		flag in Rust. \DIFdelbegin \DIFdel{Generating table specific parameters }\DIFdelend \DIFaddbegin \DIFadd{Our protocol was instantiated }\DIFaddend for \DIFdelbegin \DIFdel{table of size $1$ million takes around $12,000$
			seconds for }\DIFdelend \DIFaddbegin \DIFadd{BLS12-381 curve, using the scheme }\DIFaddend CQ \DIFdelbegin \DIFdel{based instantiation.
		}%DIFDELCMD < 
		
		%DIFDELCMD < 			%%%
		\DIFdel{We first provide overall running time of batching-efficient RAM primitive instantiated with Caulk+
			and CQ lookup arguments. The degradation in running time is plotted }\DIFdelend \DIFaddbegin \DIFadd{~\mbox{%DIFAUXCMD
				\cite{EPRINT:EagFioGab22}
			}\hskip0pt%DIFAUXCMD
		}\DIFaddend as \DIFdelbegin \DIFdel{function of accumulated updates
			since }\DIFdelend the \DIFdelbegin \DIFdel{last computation of full table-specific parameters in }\DIFdelend \DIFaddbegin \DIFadd{underlying sub-vector argument.
		}
		
		\noindent{\bf Online Proof Generation.} \DIFadd{In }\DIFaddend Figure ~\ref{fig:batch-ram-proving-time}\DIFdelbegin \DIFdel{.
			We choose batch size of $m=128$ for Caulk+ based instantiation (to avoid prohibitive performance)}\DIFdelend , \DIFdelbegin \DIFdel{while }\DIFdelend we \DIFdelbegin \DIFdel{consider $m=1024$ for CQ based instantiation. Our overhead remains barely noticable against
		}\DIFdelend \DIFaddbegin \DIFadd{benchmark }\DIFaddend the \DIFdelbegin \DIFdel{quadratic costs incurred by Caulk+ base protocol till about $2^{15}$ accumulated changes, equivalent }\DIFdelend \DIFaddbegin \DIFadd{time
			to prove a batch }\DIFaddend of \DIFdelbegin \DIFdel{$128$ lookups }\DIFdelend \DIFaddbegin \DIFadd{$m=2^{10}$ updates on a table }\DIFaddend of size \DIFdelbegin \DIFdel{$512$}\DIFdelend \DIFaddbegin \DIFadd{$N=2^{20}$}\DIFaddend . \DIFdelbegin \DIFdel{While }\DIFdelend \DIFaddbegin \DIFadd{Here, }\DIFaddend the \DIFdelbegin \DIFdel{performance overhead compared to base CQ protocol is much more apparent}\DIFdelend \DIFaddbegin \DIFadd{values on the $x$-axis denote
			the hamming distance between the table on which the updates are being proved and the table whose pre-processed
			parameters are used for the proof generation. Naturally}\DIFaddend , \DIFdelbegin \DIFdel{overall }\DIFdelend we \DIFdelbegin \DIFdel{continue }\DIFdelend \DIFaddbegin \DIFadd{expect the proving time }\DIFaddend to \DIFdelbegin \DIFdel{achieve online }\DIFdelend \DIFaddbegin \DIFadd{increase as the table
			becomes more distant from the one used to generate pre-processed parameters. Our }\DIFaddend proof generation time \DIFdelbegin \DIFdel{of }\DIFdelend \DIFaddbegin \DIFadd{stays
		}\DIFaddend under a minute \DIFdelbegin \DIFdel{all }\DIFdelend \DIFaddbegin \DIFadd{till }\DIFaddend the \DIFdelbegin \DIFdel{way upto
			$\approx 2\times 10^5$ accumulated changes, equivalent of roughly $200$ batch updates (assuming the worst-case
			scenario). In real application scenarios, we can expect better degradation curve, as its unlikely that updates
			to the }\DIFdelend \DIFaddbegin \DIFadd{two }\DIFaddend table \DIFdelbegin \DIFdel{occur uniformly across all }\DIFdelend \DIFaddbegin \DIFadd{differ in almost $2\times 10^5$ }\DIFaddend positions.
		
				\begin{figure}[H]
			\centering
			\DIFdelbeginFL %DIFDELCMD < \begin{tikzpicture}
			%DIFDELCMD < 					\begin{axis}[
			%DIFDELCMD < 						ybar,
			%DIFDELCMD < 						ylabel={Number of Constraints},
			%DIFDELCMD < 						ymin = 3,
			%DIFDELCMD < 						ymax = 8.5,
			%DIFDELCMD < 						ytick = {4, 5, 6, 7, 8},
			%DIFDELCMD < 						xlabel={},
			%DIFDELCMD < 						xtick=data,
			%DIFDELCMD < 						symbolic x coords= {A, B, C, D},
			%DIFDELCMD < 						xticklabels = {{Our Work}, ~\cite{CCS:CFHKKO22}, ~\cite{USENIX:OWWB20}, MT-20},
			%DIFDELCMD < 						yticklabels = {$10^4$, $10^5$, $10^6$, $10^7$, $10^8$},
			%DIFDELCMD < 						xlabel near ticks,
			%DIFDELCMD < 						ylabel near ticks,
			%DIFDELCMD < 						nodes near coords,
			%DIFDELCMD < 						nodes near coords align={vertical},
			%DIFDELCMD < 						]
			%DIFDELCMD < 						\addplot coordinates {(A,4.7) (B,6.8) (C,7.2) (D,7.5)};
			%DIFDELCMD < 						\legend{Batching RAMs}
			%DIFDELCMD < 					\end{axis}
			%DIFDELCMD < 					
			
			%DIFDELCMD < 				\end{tikzpicture}
			%DIFDELCMD < 				%%%
			%DIFDELCMD < \caption{%
			{%DIFAUXCMD
				\DIFdelFL{Comparison of Constraints incurred by different approaches for batching RAMs. MT-20
					refers to Merkle-tree with depth 20.}}
			%DIFAUXCMD
			\DIFdelendFL \DIFaddbeginFL \begin{tikzpicture}
				\begin{axis}[
					ybar,
					ylabel={Number of Constraints},
					ymin = 3,
					ymax = 8.5,
					ytick = {4, 5, 6, 7, 8},
					xlabel={},
					xtick=data,
					symbolic x coords= {A, B, C},
					xticklabels = {CFHKKO22 ~\cite{CCS:CFHKKO22}, OWWB20 ~\cite{USENIX:OWWB20}, MT-20},
					yticklabels = {$10^4$, $10^5$, $10^6$, $10^7$, $10^8$},
					xlabel near ticks,
					ylabel near ticks,
					nodes near coords,
					nodes near coords align={vertical},
					]
					\addplot coordinates {(A,6.8) (B,7.3) (C,7.2)};
					\legend{Batching Efficient RAMs}
				\end{axis}
				
			\end{tikzpicture}
			\caption{\DIFaddFL{Comparison of R1CS constraints incurred by existing approaches for batching efficient
					RAMs. MT20 refers to Merkle-tree with depth 20, instantiated using Poseidon hash.}}
			\DIFaddendFL \label{fig:batch-ram-constraints}
			\DIFaddbeginFL
			\DIFaddendFL \end{figure}
		
		\begin{figure}[H]
			\centering
			\DIFdelbeginFL %DIFDELCMD < \begin{tikzpicture}
			%DIFDELCMD < 					\begin{axis}[
			%DIFDELCMD < 						xmin=0,
			%DIFDELCMD < 						xmax=9,
			%DIFDELCMD < 						ymin=0,
			%DIFDELCMD < 						ymax=200,
			%DIFDELCMD < 						xlabel={Number of updated positions},
			%DIFDELCMD < 						ylabel={Prover Time (sec)},
			%DIFDELCMD < 						xtick={0, 1, 2, 3, 4, 5, 6, 7, 8, 9},
			%DIFDELCMD < 						ytick={20, 40, 60, 80, 100, 120, 140, 160, 180, 200},
			%DIFDELCMD < 						xticklabels={$2^{10}$, $2^{11}$, $2^{12}$, $2^{13}$, $2^{14}$, $2^{15}$, $2^{16}$, $2^{17}$, $2^{18}$, $2^{19}$},
			%DIFDELCMD < 						yticklabels={20, 40, 60, 80, 100, 120, 140, 160, 180, 200},
			%DIFDELCMD < 						legend pos=north west,
			%DIFDELCMD < 						ymajorgrids=true,
			%DIFDELCMD < 						grid style=dashed,
			%DIFDELCMD < 						xlabel near ticks,
			%DIFDELCMD < 						ylabel near ticks
			%DIFDELCMD < 						]
			%DIFDELCMD < 						\addlegendimage{color=blue, mark=square}
			%DIFDELCMD < 						\addlegendentry{CQ:$m=2^{10}$}
			%DIFDELCMD < 						\addlegendimage{color=red, mark=square}
			%DIFDELCMD < 						\addlegendentry{Caulk+:$m=2^{7}$}
			%DIFDELCMD < 						\addplot[
			%DIFDELCMD < 						color=blue,
			%DIFDELCMD < 						mark=square,
			%DIFDELCMD < 						]
			%DIFDELCMD < 						coordinates {
			%DIFDELCMD < 							(0,1.2)(1,1.5)(2,2.1)(3,3.3)(4,6.0)(5,11.1)(6,22.0)(7,46)(8,98)(9,208)
			%DIFDELCMD < 						};
			%DIFDELCMD < 						
			
			%DIFDELCMD < 						\addplot[
			%DIFDELCMD < 						color=red,
			%DIFDELCMD < 						mark=square,
			%DIFDELCMD < 						]
			%DIFDELCMD < 						coordinates {
			%DIFDELCMD < 							(0,16.2)(1,16.5)(2,17.1)(3,18.2)(4,20.4)(5,25.6)(6,36)(7,60)(8,108)(9,216)
			%DIFDELCMD < 						};
			%DIFDELCMD < 					\end{axis}
			%DIFDELCMD < 				\end{tikzpicture}
			%DIFDELCMD < 				%%%
			%DIFDELCMD < \caption{%
			{%DIFAUXCMD
				\DIFdelFL{Online Prover Performance of our Batching-Efficient RAM with updates}}
			%DIFAUXCMD
			\DIFdelendFL \DIFaddbeginFL \begin{tikzpicture}
				\begin{axis}[
					xmin=0,
					xmax=9,
					ymin=0,
					ymax=200,
					%		xlabel style={text width=0.5cm}, 
					xlabel= {Hamming distance $\delta = \Delta(\vecTbase,\vecT)$},
					ylabel={Prover Time (sec)},
					xtick={0, 1, 2, 3, 4, 5, 6, 7, 8, 9},
					ytick={20, 40, 60, 80, 100, 120, 140, 160, 180, 200},
					xticklabels={$2^{10}$, $2^{11}$, $2^{12}$, $2^{13}$, $2^{14}$, $2^{15}$, $2^{16}$, $2^{17}$, $2^{18}$, $2^{19}$},
					yticklabels={20, 40, 60, 80, 100, 120, 140, 160, 180, 200},
					legend pos=north west,
					ymajorgrids=true,
					grid style=dashed,
					xlabel near ticks,
					ylabel near ticks
					]
					\addlegendimage{color=blue, mark=square}
					\addlegendentry{CQ: $m=2^{10}$}
					\addlegendimage{color=red, mark=square}
					\addlegendentry{Caulk+: $m=2^{7}$}
					\addplot[
					color=blue,
					mark=square,
					]
					coordinates {
						(0,1.2)(1,1.5)(2,2.1)(3,3.3)(4,6.0)(5,11.1)(6,22.0)(7,46)(8,98)(9,208)
					};
					
					\addplot[
					color=red,
					mark=square,
					]
					coordinates {
						(0,16.2)(1,16.5)(2,17.1)(3,18.2)(4,20.4)(5,25.6)(6,36)(7,60)(8,108)(9,216)
					};
				\end{axis}
			\end{tikzpicture}
			\caption{\DIFaddFL{Online proving time of updates on table $\vecT$ given the pre-processing parameters for table $\vecTbase$, plotted against the Hamming distance $\delta$ between $\vecTbase$ and $\vecT$. Here $m$ denotes the batch size of updates, while the RAM size is $N=2^{20}$. The blue plot corresponds to our scheme instantiated with CQ as the sub-vector argument (in a black-box manner). The red plot corresponds to the non-black-box adaptation of Caulk+ described in Appendix~\ref{app:committed-index-lookup}.}}
			\DIFaddendFL \label{fig:batch-ram-proving-time}
			\DIFaddbeginFL \vspace*{-3mm}
			\DIFaddendFL \end{figure} 
		
		\DIFdelbegin %DIFDELCMD < 
		
		%DIFDELCMD < 			%%%
		\DIFdelend In \DIFdelbegin \DIFdel{Figure }\DIFdelend \DIFaddbegin \DIFadd{conjunction with offline
			pre-processing time from table }\DIFaddend ~\DIFdelbegin \DIFdel{\ref{fig:batch-ram-constraints}}\DIFdelend \DIFaddbegin \DIFadd{\ref{tbl:offline-proving-time}}\DIFaddend , \DIFdelbegin \DIFdel{we compare }\DIFdelend the \DIFdelbegin \DIFdel{R1CS constraints incurred by existing approaches }\DIFdelend \DIFaddbegin \DIFadd{graph in Figure ~\ref{fig:batch-ram-proving-time}
			determines how the offline parameter generation should be scheduled }\DIFaddend to \DIFdelbegin \DIFdel{batching friendly RAMs/accumulators}\DIFdelend \DIFaddbegin \DIFadd{achieve optimal performance on average}\DIFaddend .
		\DIFdelbegin \DIFdel{For our work, }\DIFdelend \DIFaddbegin 
		
		\noindent{\bf Offline Pre-Processing.} \DIFadd{In Table ~\ref{tbl:offline-proving-time} }\DIFaddend we \DIFdelbegin \DIFdel{assume
			that }\DIFdelend \DIFaddbegin \DIFadd{provide }\DIFaddend the \DIFdelbegin \DIFdel{memory checking on the RAM instance }\DIFdelend \DIFaddbegin \DIFadd{time to compute
			table-specific parameters as a function }\DIFaddend of \DIFaddbegin \DIFadd{table }\DIFaddend size\DIFdelbegin \DIFdel{$m$ }\DIFdelend \DIFaddbegin \DIFadd{. This }\DIFaddend is \DIFdelbegin \DIFdel{implemented }\DIFdelend \DIFaddbegin \DIFadd{the most computationally intensive step in our
			scheme involving FFTs over group polynomials }\DIFaddend as \DIFaddbegin \DIFadd{the main bottle-neck. We believe offline pre-processing can be
			made }\DIFaddend an \DIFdelbegin \DIFdel{arithmetic circuit using a Bene's
			routing network ~\mbox{%DIFAUXCMD
				\cite{benes}}\hskip0pt%DIFAUXCMD
			, though we also provide a purely polynomial protocol }\DIFdelend \DIFaddbegin \DIFadd{order of magnitude more efficient by leveraging parallel implementation }\DIFaddend for the \DIFdelbegin \DIFdel{same in Section ~\ref{sec:poly-proto-ram-app}}\DIFdelend \DIFaddbegin \DIFadd{FFTs}\DIFaddend .
		\DIFaddbegin 
		
		\begin{table}[htbp]
			\begin{tabular}{|l|c|c|c|c|c|c|}
				\hline
				%DIF > \rowcolor{gray}
				\cellcolor{lightgray} \DIFaddFL{Table-Size ($N$) }& \DIFaddFL{$2^{10}$ }& \DIFaddFL{$2^{12}$ }& \DIFaddFL{$2^{14}$ }& \DIFaddFL{$2^{16}$ }& \DIFaddFL{$2^{18}$ }& \DIFaddFL{$2^{20}$ }\\ \hline
				\cellcolor{lightgray} \DIFaddFL{Time (s) }& \DIFaddFL{$7$ }& \DIFaddFL{$29$ }& \DIFaddFL{$135$ }& \DIFaddFL{$620$ }& \DIFaddFL{$2766$ }& \DIFaddFL{$12000$ }\\ \hline
			\end{tabular}
			\caption{\DIFaddFL{Pre-processing time for tables of different sizes.}}
			\label{tbl:offline-proving-time}
			\vspace*{-5mm}
		\end{table}
		
		
		\noindent{\bf Proof Size and Verification Costs.} \DIFaddend Our \DIFdelbegin \DIFdel{construction is relatively ``circuit-free'' }\DIFdelend \DIFaddbegin \DIFadd{proof sizes }\DIFaddend and \DIFdelbegin \DIFdel{relies on }\DIFdelend \DIFaddbegin \DIFadd{verification costs are independent of }\DIFaddend the
		\DIFdelbegin \DIFdel{efficiency }\DIFdelend \DIFaddbegin \DIFadd{size }\DIFaddend of \DIFdelbegin \DIFdel{lookup arguments coupled with
			our novel modifications to make them update resilient}\DIFdelend \DIFaddbegin \DIFadd{the table and the number of updates in a batch}\DIFaddend . \DIFdelbegin %DIFDELCMD < 
		
		%DIFDELCMD < 			%%%
		\DIFdel{Finally}\DIFdelend \DIFaddbegin \DIFadd{For the instantiation of our scheme with BLS12-381 curve}\DIFaddend ,
		we \DIFaddbegin \DIFadd{incur a proof size of $4.4KB$ while the verification takes around $15ms$.
		}
		
		\noindent{\bf Fast Update Bechmarks.} \DIFadd{We }\DIFaddend also \DIFdelbegin \DIFdel{compare }\DIFdelend \DIFaddbegin \DIFadd{benchmark }\DIFaddend the \DIFdelbegin \DIFdel{efficacy }\DIFdelend \DIFaddbegin \DIFadd{efficiency }\DIFaddend of \DIFdelbegin \DIFdel{our quasi-linear }\DIFdelend \DIFaddbegin \DIFadd{the }\DIFaddend algorithm to compute
		scalar coefficients in Lemma ~\ref{lem:sum-computation}, required to assemble the encoded quotients from pre-computed quotients.
		This algorithm is implemented and tested in the {\tt fastupdate} module of the referenced repository.
		In Table ~\ref{tbl:sum-computation-compare}, we compare it against the naive computation of the quotients.
		In the table, we vary the sizes of set $I$ in Lemma ~\ref{lem:sum-computation} in the set $\{2^i:7\leq i\leq 10\}$
		and set $K=2^7|I|$\DIFaddbegin \DIFadd{. %DIF > i.e, we compare the two methods to sustain $\approx 100$ online proofs.
			We clearly see $5\times-20\times$ advantage over the naive computation.
		}
		
		\noindent{\bf Continuity.} \DIFadd{To maintain }{\em \DIFadd{continuity}} \DIFadd{of the system~(this is particularly important for applications such as rollups)}\DIFaddend , \DIFdelbegin \DIFdel{i}\DIFdelend \DIFaddbegin \DIFadd{we must carefully
			align the online prover performance curve with the cost of offline computation}\DIFaddend .  \DIFdelbegin \DIFdel{e}\DIFdelend \DIFaddbegin \DIFadd{As an example}\DIFaddend , \DIFaddbegin \DIFadd{suppose $\vec{T}_0$
			is the initial pre-processed table at time $t=0$. We generate proofs using pre-computed parameters for $\vec{T}_0$
			till the time $t=t_1$, when the table state $\vec{T}_1$ is at hamming distance $2^{17}$ from $\vec{T}_0$. At this point,
			from Figure ~\ref{fig:batch-ram-proving-time}, online proof generation takes around $40s$ for batch of $1000$ updates.
			At $t=t_1$, }\DIFaddend we \DIFdelbegin \DIFdel{compare }\DIFdelend \DIFaddbegin \DIFadd{also start an offline parameter computation for }\DIFaddend the \DIFdelbegin \DIFdel{two methods }\DIFdelend \DIFaddbegin \DIFadd{table $\vec{T}_1$, while continuing }\DIFaddend to \DIFdelbegin \DIFdel{sustain $\approx 100$ }\DIFdelend \DIFaddbegin \DIFadd{generate
		}\DIFaddend online proofs \DIFaddbegin \DIFadd{using parameters for $\vec{T}_0$}\DIFaddend . We \DIFdelbegin \DIFdel{clearly see
			$5\times-20\times$ advantage over }\DIFdelend \DIFaddbegin \DIFadd{can generate }\DIFaddend the \DIFdelbegin \DIFdel{naive computation}\DIFdelend \DIFaddbegin \DIFadd{next $2^7$ batches of updates at an average of
			approximately $12000/128\approx 94s$ each, thus finishing with a table state $\vec{T}_2$ at hamming distance at most
			$2^{18}$ from $\vec{T}_0$ at $t_2=t_1+12000$}\DIFaddend . \DIFaddbegin \DIFadd{At this point, we should have the pre-computed parameters for $\vec{T}_1$,
			which is at update distance of $2^{17}$ from $\vec{T}_2$, and thus online proof generation can switch to parameters
			for the table $\vec{T}_1$. This alignment gives us a proving time of $94s$ per batch of $1000$, while ensuring system
			is live at all times. Clearly, a faster offline pre-computation using parallel implementation would allow us to stay
			at the cheaper end of online proving performance.
		}\DIFaddend 
		
		
		\begin{table}[htbp]
			\centering
			\begin{tabularx}{0.45\textwidth}{@{}XXXX@{}}
				\toprule
				$|I|$ & $|K|$ & Lemma ~\ref{lem:sum-computation} & Naive \\ \midrule
				$2^7$ & $2^{14}$ & 3.3s  & 12s \\
				$2^8$ & $2^{15}$ & 7.7s  & 48s \\
				$2^9$ & $2^{16}$ & 16.8s & 198s \\
				$2^{10}$ & $2^{17}$ & 39.2s & 839s \\
				\bottomrule
			\end{tabularx}
			\caption{Comparison of Lemma ~\ref{lem:sum-computation} and naive computation for calculating
				scalar coefficients for encoded quotients.}
			\label{tbl:sum-computation-compare}
			\DIFaddbeginFL \vspace*{-5mm}
			\DIFaddendFL \end{table}
		
		\DIFaddbegin \noindent{\bf Comparison with Prior Work}\DIFadd{. The proof generation in the prior batching efficient RAM constructions
			using RSA accumulators ~\mbox{%DIFAUXCMD
				\cite{USENIX:OWWB20,CCS:CFHKKO22} }\hskip0pt%DIFAUXCMD
			involves two key steps (i) generation of a SNARK
			proof showing knowledge of witness for a relation modeled as arithmetic circuit/R1CS and (ii) computing the witness
			for the proof generation. A platform agnostic metric to express the cost of the first step is the number of R1CS
			constraints needed to encode the relation, which is also the metric used in ~\mbox{%DIFAUXCMD
				\cite{CCS:CFHKKO22} }\hskip0pt%DIFAUXCMD
			for comparison.
			Using the R1CS constraints reported in ~\mbox{%DIFAUXCMD
				\cite{USENIX:OWWB20,CCS:CFHKKO22} }\hskip0pt%DIFAUXCMD
			(see Figure ~\ref{fig:batch-ram-constraints}),
			we benchmark single-threaded proof generation
			using the }\textsf{\DIFadd{Groth16}} \DIFadd{protocol (used in prior works) for R1CS of equivalent size on our test-bed. We use publicly
			available benchmarking suite in ~\mbox{%DIFAUXCMD
				\cite{ark-groth-16} }\hskip0pt%DIFAUXCMD
			for }\textsf{\DIFadd{Groth16}} \DIFadd{benchmarks. Since the
			the prior works only report performance of parallel implementation of the second step which is common to both (with
			degree of parallelism not explicitly mentioned), we will use the reported parallel performance to estimate the
			overall proving time with this caveat. 
		}
		
		\DIFadd{Even without the benefit of parallelization, our average proof generation
			time of $\approx 90s$ for batch size of $2^{10}$ and RAM size of $2^{20}$ is $3-5\times$ faster than the prior works
			for the same setting. The proof sizes and verification complexity are constant for prior work and our work.
			Concrete proof sizes are smaller in ~\mbox{%DIFAUXCMD
				\cite{USENIX:OWWB20,CCS:CFHKKO22} }\hskip0pt%DIFAUXCMD
			owing to their usage of }\textsf{\DIFadd{Groth16}}
		\DIFadd{proving backend, while our verification times are competitive with ~\mbox{%DIFAUXCMD
				\cite{USENIX:OWWB20} }\hskip0pt%DIFAUXCMD
			and substantially
			less than ~\mbox{%DIFAUXCMD
				\cite{CCS:CFHKKO22}}\hskip0pt%DIFAUXCMD
			. One way to reduce proof size in our scheme would be to use a SNARK to prove
			memory-checking on smaller sub-RAMs, instead of explicit polynomial protocol that we employ for the same. For
			completeness, we also include the batching efficient RAM using Merkle tree (with Poseidon hash) in the performance
			comparison in Table ~\ref{tbl:performance-comparison} which was considered in prior works. Note that the Merkle
			tree-based approach is faster than that of ~\mbox{%DIFAUXCMD
				\cite{USENIX:OWWB20} }\hskip0pt%DIFAUXCMD
			for batch size of $2^{10}$ (the break-even
			point reported to be batch size of $\approx 1200$).
		}
		
		\begin{table}[htbp]
			\centering
			\begin{tabularx}{0.49\textwidth}{@{}XXXX@{}}
				\toprule
				\DIFaddFL{Scheme }& \DIFaddFL{$\prover$ (s) }& \DIFaddFL{$\verifier$ (ms) }& \DIFaddFL{$|\pi|$ (KB) }\\ \midrule
				\DIFaddFL{MT20 }& \DIFaddFL{$450$ }& \DIFaddFL{$7$  }& \DIFaddFL{$0.26$ }\\
				\DIFaddFL{OWWB20 ~\mbox{%DIFAUXCMD
						\cite{USENIX:OWWB20} }\hskip0pt%DIFAUXCMD
				}& \DIFaddFL{$550+43$ }& \DIFaddFL{$7$  }& \DIFaddFL{$0.26$ }\\
				\DIFaddFL{CFHKKO22 ~\mbox{%DIFAUXCMD
						\cite{CCS:CFHKKO22} }\hskip0pt%DIFAUXCMD
				}& \DIFaddFL{$226+43$ }& \DIFaddFL{$120$ }& \DIFaddFL{$1.3$ }\\
				\DIFaddFL{Our Work }& \DIFaddFL{$94$ }& \DIFaddFL{$15$ }& \DIFaddFL{$4.4$ }\\
				\bottomrule
			\end{tabularx}
			\caption{\DIFaddFL{Comparing performance of our batching-efficient RAM with prior works. $\prover$
					denotes proof generation time, $\verifier$ denotes verification time while $|\pi|$ denotes
					argument size. We mention proof generation time as $a+b$ for ~\mbox{%DIFAUXCMD
						\cite{USENIX:OWWB20,CCS:CFHKKO22}
					}\hskip0pt%DIFAUXCMD
					where $a$ denote proving time of }\textsf{\DIFaddFL{Groth16}} \DIFaddFL{and $b$ denotes witness generation time. The latter
					is reported in respective works for a parallel implementation.}}
			\label{tbl:performance-comparison}
			\vspace*{-5mm}
		\end{table}
		
		
		%DIF > Generating table specific parameters for table of size $1$ million takes around $12,000$
		%DIF > seconds for CQ based instantiation.
		
		%DIF > We first provide overall running time of batching-efficient RAM primitive instantiated with Caulk+
		%DIF > and CQ lookup arguments. The degradation in running time is plotted as function of accumulated updates
		%DIF > since the last computation of full table-specific parameters in Figure ~\ref{fig:batch-ram-proving-time}.
		%DIF > We choose batch size of $m=128$ for Caulk+ based instantiation (to avoid prohibitive performance),
		%DIF > while we consider $m=1024$ for CQ based instantiation. Our overhead remains barely noticable against
		%DIF > the quadratic costs incurred by Caulk+ base protocol till about $2^{15}$ accumulated changes, equivalent of
		%DIF > $128$ lookups of size $512$. While the performance overhead compared to base CQ protocol is much more apparent,
		%DIF > overall we continue to achieve online proof generation time of under a minute all the way upto
		%DIF > $\approx 2\times 10^5$ accumulated changes, equivalent of roughly $200$ batch updates (assuming the worst-case
		%DIF > scenario). In real application scenarios, we can expect better degradation curve, as its unlikely that updates
		%DIF > to the table occur uniformly across all positions.
		
		%DIF > In Figure ~\ref{fig:batch-ram-constraints}, we compare the
		%DIF > R1CS constraints incurred by existing approaches to batching friendly RAMs/accumulators. For our work, we assume
		%DIF > that the memory checking on the RAM instance of size $m$ is implemented as an arithmetic circuit using a Bene's
		%DIF > routing network ~\cite{benes}, though we also provide a purely polynomial protocol for the same in Section ~\ref{sec:poly-proto-ram-app}.
		%DIF > Our construction is relatively ``circuit-free'' and relies on the efficiency of lookup arguments coupled with
		%DIF > our novel modifications to make them update resilient.
		
		
		
		
		
		
		
		
		\DIFaddend \bibliographystyle{plain}
		\bibliography{cryptobib/abbrev3,cryptobib/crypto,main}
%		
		\appendix
		%DIF < \input{appendix}
		
		
		
		
		
		%										\appendix
		%										\input{app-prelims}
		\DIFaddbegin 
		
		
		
		
		\DIFaddend \section{More Preliminaries}
		
		\subsection{Polynomial Commitment Scheme}
		\label{sec:pcs_def}
		The notion of a polynomial commitment scheme (PCS) that allows the prover to open evaluations of the committed polynomial succinctly was introduced in~\cite{AC:KatZavGol10} who gave a construction under the trusted setup assumption.
		A polynomial commitment scheme over $\F$ is a tuple $\pc = 
		(\pcsetup,\pccommit,\pcopen,\pceval)$ where:
		
		\begin{itemize}
			\item $\pp \leftarrow \pcsetup(1^\secp,D) $. On input security parameter $\secp$, and an upper bound $D \in \mathbb{N}$ on the degree, $\pcsetup$ generates public parameters $\pp$.
			
			\item $(C,\mathbf{\tilde{c}}) \leftarrow \pccommit(\pp, f(X),d) $. On input the public parameters $\pp$, and a univariate polynomial $f(X) \in \F[X]$ with degree at most $d \leq D$, $\pccommit$ outputs a commitment to the polynomial $C$, and additionally an opening hint $\mathbf{\tilde{c}}$.
			
			\item $b \leftarrow \pcopen(\pp,f(X),d,C,\mathbf{\tilde{c}})$. On input the public parameters $\pp$, the commitment $C$ and the opening hint $\mathbf{\tilde{c}}$, a polynomial $f(X)$ of degree $d \leq D$, $\pcopen$ outputs a bit indicating accept or reject. 
			
			\item $b \leftarrow \pceval(\pp,C,d,x,v;f(X)) $. A public coin interactive protocol 
			$\langle P_{\mathsf{eval}}(f(X)), V_{\mathsf{eval}} \rangle(\pp,C,d,z,v)$ between a PPT prover and a PPT verifier. The parties have as common input public parameters $\pp$, commitment $C$, degree $d$, evaluation point $x$, and claimed evaluation $v$. The prover has, in addition, the opening $f(X)$ of $C$, with $\deg(f) \leq d$. At the end of the protocol, the verifier outputs $1$ indicating accepting the proof that $f(x)=v$, or outputs $0$ indicating rejecting the proof.
			
		\end{itemize}
		
		A polynomial commitment scheme must satisfy completeness, binding and extractability.
		
		\begin{definition}[Completeness]
			\label{def:pcs-comp}
			For all polynomials $f(X) \in \F[X]$ of degree $d \leq D$, for all $x \in \F$,
			\[
			\Pr \left( 
			\begin{matrix}
				%\pccheck(\pp,C,d,z,v,\pi) = 1 
				b=1
			\end{matrix}
			\,:\,
			\begin{matrix}
				\pp \leftarrow \pcsetup(1^\secp,D) \\
				(C,\mathbf{\tilde{c}}) \leftarrow \pccommit(\pp, f(X),d) \\
				v \leftarrow f(x) \\
				b \leftarrow \pceval(\pp,C,d,x,v;f(X))
				%\pi \leftarrow \pceval(\pp,f(X),d,z) \\
			\end{matrix}
			\right) = 1.
			\]
		\end{definition}
		
		\begin{definition}[Binding] 
			\label{def:pcs-binding-app}
			A polynomial commitment scheme $\pc$ is binding if for all PPT $\Adv$, the following probability is negligible in $\secp$:
			\[
			\Pr \left( 
			\begin{matrix}
				\pcopen(\pp,f_0,d,C,\mathbf{\tilde{c}_0}) =1 \wedge \\
				\pcopen(\pp,f_1,d,C,\mathbf{\tilde{c}_1}) = 1 \wedge \\
				f_0 \neq f_1
			\end{matrix}
			\,:\,
			\begin{matrix}
				\pp \leftarrow \pcsetup(1^\secp,D) \\
				(C,f_0,f_1,\mathbf{\tilde{c}_0}, \mathbf{\tilde{c}_1},d) \leftarrow \Adv(\pp) 
			\end{matrix}
			\right).
			\]
		\end{definition}
		
		\begin{definition}[Knowledge Soundness]
			\label{def:pcs-ext-app}
			For any PPT adversary $\Adv = (\Adv_{1},\Adv_{2})$, there exists a PPT algorithm $\ext$ such that the following probability is negligible in $\secp$:
			\[
			\Pr\left(
			\begin{matrix}
				%\pccheck(\pp,C,d,z,v,\pi) = 1 \wedge
				b = 1 \wedge \\
				\R_{\pceval}(\pp,C,x,v; \tilde{f},\mathbf{\tilde{c}}) = 0
			\end{matrix}
			\,:\,
			\begin{matrix}
				\pp \leftarrow \pcsetup(1^\secp,D) \\
				(C,d,x,v,\mathsf{st}) \leftarrow \Adv_{1}(\pp) \\
				(\tilde{f},\mathbf{\tilde{c}}) \leftarrow \ext^{\Adv_{2}}(\pp)\\
				b \leftarrow \langle \Adv_{2}(\mathsf{st}), V_{\mathsf{eval}} \rangle(\pp,C,d,x,v)
			\end{matrix}
			\right).
			\]
			where the relation $\R_{\pceval}$ is defined as follows:
			\begin{align*}
				\R_{\pceval} &= \{\left((\pp,C \in \mathbb{G},\; x \in \F, \; v \in \F);\; (f(X), \mathbf{\tilde{c}}) \right) : \\
				&\qquad (
				\pcopen(\pp,f,d,C,\mathbf{\tilde{c}}) = 1 )
				\land v = f(x)  \}
			\end{align*} 
			
		\end{definition}
		
		We denote by $\mathsf{Prove}, \mathsf{Verify}$, the non-interactive prover and verifier algorithms obtained by applying FS to the 
		$\pceval$ public-coin interactive protocol, giving a non-interactive PCS scheme $(\pcsetup,\pccommit,\allowbreak \mathsf{Prove},\mathsf{Verify})$.
		
		\begin{definition}[Succinctness]
			\label{def:pcs-succinct}
			We require the commitments and the evaluation proofs to be of size independent of the degree of the polynomial, that is the scheme is \emph{proof succinct} if
			$|C|$ is $\mathsf{poly}(\secp)$, $|\pi|$ is $\mathsf{poly}(\secp)$ where $\pi$ is the transcript obtained by applying FS to $\pceval$. Additionally, the scheme is \emph{verifier succinct} if $\pceval$ runs in time
			$\mathsf{poly}(\secp) \cdot \allowbreak \mathsf{log} (d)$ for the verifier.
		\end{definition}
		
		
		\subsection{Succinct Argument of Knowledge}
		\label{sec:aok}
		
		Let $\R$ be a NP-relation and $\L$ be the corresponding NP-language, where $\L=\{x : \exists$ $w$ such that $(x,w) \in \R\}$. Here, a prover $\prover$ aims to convince a verifier $\verifier$ that $x\in \L$ by proving that it knows a witness $w$ for a public statement $x$ such that $(x,w)\in \R$. An interactive argument of knowledge for a relation $\R$ consists of a PPT algorithm $\setup$, that takes an input the security parameter $\secp$, and outputs the public parameter $\pp$, and a pair of interactive PPT algorithms $\langle\prover,\verifier\rangle$, where $\prover$ takes as input $(\pp,x,w)$ and $\verifier$ takes as input $(\pp,x)$. An interactive argument of knowledge  $\langle\prover,\verifier\rangle$, must satisfy completeness and knowledge soundness. % and a succinct argument of knowledge must additionally satisfy the property of succinctness.
		
		\begin{definition}[Completeness]
			\label{def:aok-comp}
			For all security parameter $\secp \in \mathbb{N}$ and statement $x$ and witness $w$ such that $(x,w)\in \R$, we have
			\[
			\Pr \left( 
			\begin{matrix}
				%\pccheck(\pp,C,d,z,v,\pi) = 1 
				b=1
			\end{matrix}
			\,:\,
			\begin{matrix}
				\pp \leftarrow \setup(1^\secp) \\
				b \leftarrow \langle\prover (w) ,\verifier\rangle (\pp,x)
			\end{matrix}
			\right) = 1.
			\]
		\end{definition}
		
		\begin{definition}[Knowledge Soundness]
			\label{def:aok-ks}
			%An interactive argument of knowledge  $\langle\prover,\verifier\rangle$ satisfies knowledge soundness with error $\kappa$, if for
			For any PPT malicious prover $\prover^* = (\prover_1^*,\prover_2*)$, there exists a PPT algorithm $\ext$ such that the following probability is negligible:
			\[
			\Pr \left( 
			\begin{matrix}
				b=1 \wedge \\
				(x,w)\not\in \R
			\end{matrix}
			\,:\,
			\begin{matrix}
				\pp \leftarrow \setup(1^\secp) \\
				(x,\mathsf{st}) \leftarrow \prover_1^*(1^\secp,\pp) \\
				b \leftarrow \langle\prover_2^* (\mathsf{st}) ,\verifier\rangle (\pp,x) \\
				w \leftarrow \ext^{\prover_2^*} (\pp,x)
			\end{matrix}
			\right) .
			\]	
		\end{definition}
		
		A \emph{succinct} argument of knowledge $\langle\prover,\verifier\rangle$ for a relation $\R$, must satisfy completeness and knowledge soundness and additionally ensure that the communication complexity between prover and verifier, as well as the verification complexity is bounded by $\mathsf{poly}(\secp,\log |w|)$, where $w$ is the witness for the relation.
		
		
		
		
		
		
		
		\section{Argument for RAM From Polynomial Protocols}\label{sec:poly-proto-ram-app}
		In this section, we give a self-contained argument of knowledge for membership in the language
		$\LRAM{I}{m}{m}$ introduced in Section \ref{sec:model-for-ram}. We first consider the polynomial encoding
		of different RAM artefacts.
		\subsection{Polynomial Encoding}\label{subsec:poly-encoding}
		%The aim of this section is to encode artefacts such as RAM state, operations and transcripts as polynomials, and
		%translate checking memory consistency (equivalently, checking membership in $\LRAM{I}{n}{m}$) to checking polynomial identities
		%over the encoded polynomials. For
		%simplicity and its usefulness later, we consider the case $m=n$, and accordingly check the membership in the language $\LRAM{I}{m}{m}$.
		Let $k=3m$ and let $\omega$ be a primitive $k^{th}$ root of unity in $\F$.
		Let $\nu=\omega^3$, and thus $\nu$ is a primitive $m^{th}$ root of unity in $\F$ (We assume, these roots exist in $\F$).
		We recall $\setV$ as the subgroup consisting of $m^{th}$ roots of unity with associated Lagrange basis polynomials
		$\{\tau_i(X)\}_{i\in [m]}$, while we additionally introduce the set $\setH$ of $k^{th}$ roots of unity with
		$\{\lambda_i(X)\}_{i\in [k]}$ as the associated Lagrange polynomials.
		\begin{gather}\label{eq:interpolation-sets}
			\setH = \{\omega,\ldots,\omega^k\},\quad \setV = \{\nu,\ldots,\nu^m\}
		\end{gather}
		As before, we define the encoding of vectors in $\vec{f}\in \F^k$ as $\enc{f}{\setH}=\sum_{i\in [k]}f_i\lambda_i(X)$.
		%Let $\{\lambda_i(X)\}_{i=1}^k$ be lagrange basis polynomials
		%for the set $\setH$ and $\{\tau_i(X)\}_{i=1}^m$ be the lagrange polynomials for the set $\setV$ satisfying
		%$\lambda_i(\omega^j)=\delta_{ij}$ for $i,j\in [k]$ and $\tau_i(\nu^j)=\delta_{ij}$ for $i,j\in [m]$.
		%We use $\setH$ to encode a vector $\vec{f}=(f_1,\ldots,f_k)$ of size $k$ as the polynomial $f(X)\in \F_{<k}[X]$ such that $f(\omega^i)=f_i$ for $i\in [k]$.
		%Similarly, we use $\setV$ to encode a vector
		%$\vec{g}=(g_1,\ldots,g_m)$ of size $m$ as the polynomial $g(X)\in F_{<m}[X]$ such that $g(\nu^i)=g_i$ for $i\in [m]$. We use
		%the notation $\enc{f}{\setH}$ and $\enc{g}{\setV}$ to denote polynomial encodings of vectors $\vec{f}\in \F^k$ and $\vec{g}\in \F^m$
		%respectively.
		%In the other direction, for a polynomial $f(X)\in \F[X]$,
		%we use $\vec{f}_{|_\setH}$ and $\vec{f}_{|_\setV}$ to denote the vectors $(f(\omega^1),\ldots,f(\omega^k))$ and $(f(\nu^1),\ldots,f(\nu^m))$ respectively.
		%The encoding of vectors as polynomials can be succinctly described using lagrange basis polynomials.
		%Then we have $\enc{f}{\setH}=\sum_{i=1}^k f_i\lambda_i(X)$ and $\enc{g}{\setV}=\sum_{i=1}^m g_i\tau_i(X)$.
		We canonically extend the encoding of vectors to encode RAM, operations and transcripts by encoding their component vectors.
		Thus, for a RAM \DIFdelbegin \DIFdel{$T=(\vec{a},\vec{v})\in \RAM{I}{m}$}\DIFdelend \DIFaddbegin \DIFadd{$\vecT=(\vec{a},\vec{v})\in \RAM{I}{m}$}\DIFaddend , we define its encoding
		$\wt{T}=(a(X),v(X))$ where $a(X),v(X)\in \F_{<m}[X]$ encode vectors $\vec{a}, \vec{v}$ respectively.
		Given an operation sequence
		$\vec{o}=(o_1,\ldots,o_m)$ with $o_i=(\bar{\op}_i,\bar{a}_i,\bar{v}_i)$ we encode $\vec{o}$ as
		$\wt{O}$ $=$ $(\bar{\op}(X)$ ,$\bar{a}(X)$ ,$\bar{v}(X))$
		where $\bar{\op}(X)$ encodes the
		vector $\vec{\op}=(\bar{\op}_1,\ldots,\bar{\op}_m)$, $\bar{a}(X)$ encodes the vector $(\bar{a}_1,\ldots,\bar{a}_m)$ and
		$\bar{v}(X)$encodes the vector $(\bar{v}_1,\ldots,\bar{v}_m)$.
		Finally, a transcript $\tr=(\vec{t},\vec{\op},\vec{A},\vec{V})$ for tuples \DIFdelbegin \DIFdel{$(T,\vec{o},T')$ }\DIFdelend \DIFaddbegin \DIFadd{$(\vecT,\vec{o},\vecT')$ }\DIFaddend where \DIFdelbegin \DIFdel{$T,T'$ }\DIFdelend \DIFaddbegin \DIFadd{$\vecT,\vecT'$ }\DIFaddend are RAMs of size $m$,
		and $\vec{o}$ is an operation sequence of size $m$ is encoded as $\wt{\tr}$ $=$ $(t(X)$, $\op(X)$, $A(X)$, $V(X))$
		where the polynomials $t(X),\op(X),V(X)$ and $A(X)$ encode the respective vectors in $\F^k$ (See Section \ref{sec:model-for-ram}).
		
		\subsection{Relations over Polynomial Encodings}\label{subsec:encoded-relations}
		In this section, we describe polynomial checks for two important relations we need in subsequent sections, viz,
		(i) checking concatenation of vectors and (ii) checking monotonicity and load-store consistency of a transcript.
		The lemma below specifies the polynomial identities for verifying
		that vector $\vec{v}\in \F^k$ is concatenation of vectors $\vec{a},\vec{b},\vec{c}$ in $\F^m$.
		
		\begin{lemma}\label{lem:vec-concatenation}
			Let $\vec{a},\vec{b},\vec{c}\in \F^m$  and $v\in \F^k$ be vectors encoded by polynomials
			$a(X),b(X),c(X)$ and $v(X)$ respectively. Then,
			\begin{align}
				a(X^3) - v(X)  &= 0  \quad \text{ mod $Z(X)$ } \tag{A1}\label{eq:A1}\\
				b(X^3) - v(\omega^m X)  &= 0   \quad \text{ mod $Z(X)$ } \tag{A2}\label{eq:A2}\\
				c(X^3) - v(\omega^{2m} X) &= 0 \quad \text{ mod $Z(X)$ } \tag{A3}\label{eq:A3}
			\end{align}
			for $Z(X)=\prod_{i=1}^m (X-\omega^i)$ if and only if $\vec{v}=\vec{a}||\vec{b}||\vec{c}$.
		\end{lemma}
		\begin{proof}
			Assume that the polynomial identities hold. Substituting $X=\omega^i$ for $i\in [m]$ in above equations implies
			for $i\in [m]$: $a_i=v_i$ (Eq \eqref{eq:A1}), $b_i=v_{m+i}$ (Eq \eqref{eq:A2}) and $c_i=v_{2m+i}$ (Eq \eqref{eq:A3}),
			which together imply $\vec{v}=\vec{a}||\vec{b}||\vec{c}$. Converse follows by observing that $\vec{v}=\vec{a}||\vec{b}||\vec{c}$
			implies that $v(X) = a(X^3)$, $v(\omega^m X)=b(X^3)$ and $v(\omega^{2m} X)=c(X^3)$ holds for all $X=\omega^i, i\in [m]$.
			Thus, the equalities hold modulo the polynomial $Z(X)$ as defined above.
		\end{proof}
		
		
		Next, we specify polynomial checks on the encoding of a transcript to ensure it satisfies address-ordering and load-store consistency.
		Let $N$ be an upper bound on the values of $\vec{A}$, i.e, the index set $\setind\subseteq [N]$.
		Let $\tr=(\vec{t},\vec{\op},\vec{A},\vec{V})$ be a transcript encoded as
		$\wt{\tr}=(t(X),\op(X),A(X),V(X))$. Recall that we need to check two conditions on $\tr$, viz, (i) {\em monotonicty}:
		the transcript is sorted by address and timestamp respectively, i.e, $A_i\leq A_{i+1}$ for all $i < k$ and
		$t_i < t_{i+1}$ whenever $A_i=A_{i+1}$, (ii) {\em load-store consistency}: whenever $\op_{i+1}=0$ and $A_i=A_{i+1}$,
		we have $V_i=V_{i+1}$.
		To do so, we exhibit disjoint sets $I_1,I_2$ with \DIFdelbegin \DIFdel{$I_1\uplus I_2=[k]$ }\DIFdelend \DIFaddbegin \DIFadd{$I_1\uplus I_2=[k-1]$ }\DIFaddend such that: (i) for all
		$i\in I_1$, $A_i < A_{i+1}$, (ii) for all $i\in I_2$, $(A_i = A_{i+1})\wedge (t_i < t_{i+1})$ and (iii) for all $i\in I_2$,
		$(\op_i=1)\vee (V_i = V_{i+1})$.
		\DIFdelbegin \DIFdel{We vacuously allow $k$ to be part of either of the sets.
		}\DIFdelend Note that the conditions on the sets $I_1$ and $I_2$ ensures monotonicity.
		Moreover, it can be seen that load-store consistency requirements are satisfied for all $i\in I_1$ (as $A_i\neq A_{i+1}$).
		Similarly,load-store consistency also holds for all $i\in I_2$.
		It remains to exhibit the sets and show that they satisfy the above invariants using polynomials, as in the following
		lemma:
		\begin{lemma}\label{lem:addr-ordered-transcript}
			Let $\wt{\tr}$ be a polynomial encoding of transcript $\tr$ of size $k$, given by polynomials $t(X),\op(X),A(X)$ and $V(X)$,
			with index set $[N]$. Then assuming $kN<|\F|$, $\tr$ is address ordered and satisfies load-store consistency if and only if there exist polynomials
			$Z_1,Z_2,\delta_T,\delta_A$
			such that the following hold:
			\begin{align}%\label{eq:loadstore-consitency-constraints}
				& A(\omega X) - A(X) - \delta_A(X)= 0 \text{ mod }\, \mathbb{Z}_1(X) \tag{C1} \label{eq:C1} \\
				& A(\omega X) - A(X) = 0  \text{ mod } Z_2(X) \tag{C2} \label{eq:C2} \\
				& t(\omega X) - t(X) - \delta_T(X) = 0  \text{ mod } Z_2(X) \tag{C3} \label{eq:C3} \\
				& (\op(X) - 1)(V(\omega X) - V(X)) = 0  \text{ mod } Z_2(X) \tag{C4} \label{eq:C4} \\
				& Z_1(X)\cdot Z_2(X)\DIFaddbegin \DIFadd{\cdot (X-1) }\DIFaddend = \mathbb{Z}_\setH(X) \quad  \tag{C5} \label{eq:C5} \\
				& 1\leq A(\omega^i) \leq N  \quad \tag{C6} \label{eq:C6} \\
				& 1\leq t(\omega^i) \leq N, 1\leq \delta_A(\omega^i)\leq N,\ 1\leq \delta_T(\omega^i)\leq N \text{ for } i\in [k] \tag{C7} \label{eq:C7}
			\end{align}
		\end{lemma}
		\begin{proof}
			Suppose there exist polynomials $Z_1(X),Z_2(X),\delta_T(X)$ and $\delta_A(X)$ satisfying above identities. From Equation
			~\eqref{eq:C5}, we conclude that their exist sets $I_1,I_2$ with \DIFdelbegin \DIFdel{$I_1\uplus I_2=[k]$ }\DIFdelend \DIFaddbegin \DIFadd{$I_1\uplus I_2=[k-1]$ }\DIFaddend such that $Z_b(X)$, \DIFdelbegin \DIFdel{$b\in \{0,1\}$ }\DIFdelend \DIFaddbegin \DIFadd{$b\in \{1,2\}$ }\DIFaddend is the
			vanishing polynomial of the set $\{\omega^i: i\in I_b\}$. We now note that the following are true for $i\in I_1$:
			\begin{itemize}[leftmargin=1em]
				\item $A(\omega^{i+1})-A(\omega^i)=\delta_A(\omega^i)$. Since $1\leq \delta_A(\omega^i)\leq N$, this ensures $A_i < A_{i+1}$ for the vector $\vec{A}$ encoded
				by $A(X)$. We note that $kN < |\F|$ implies there is no overflow modulo the field characteristic.
			\end{itemize}
			Similarly, it can be seen that for $i\in I_2$, we must have (i) $A_i=A_{i+1}\wedge t_i < t_{i+1}$
			and (ii) $\op_i=1\,\vee\,V_i=V_{i+1}$. Together these imply that the encoded transcript is address-ordered.
		\end{proof}
		
		Protocols facilitating the checks mentioned in Lemma\DIFaddbegin \DIFadd{~}\DIFaddend \ref{lem:vec-concatenation} and Lemma\DIFaddbegin \DIFadd{~}\DIFaddend \ref{lem:addr-ordered-transcript} are presented in Figure \ref{fig:concatenation} and \ref{fig:encoded-relations} respectively.
		
		\begin{figure}[tb!]
			\scriptsize
			\begin{mdframed}
				{
					{\bf Common Input}: Commitments $c_a$, $c_b$, $c_c$, $c_v$, and $\gone{Z}$ (to the polynomial
					$Z(X)=\prod_{i=1}^m (X-\omega^i)$). \\
					{\bf Prover's Input}: Vectors $\vec{a},\vec{b},\vec{c}\in\F^m$ and $\vec{v}\in \F^k$.
					\begin{enumerate}[leftmargin=1em, label=\arabic*.]
						\item $\verifier$ sends $\gamma\gets \F$.
						\item $\prover$ computes:
						\begin{align}
							& h(X) = a(X) + \gamma b(X) + \gamma^2 c(X),\\
							& Q(X) = (h(X^3) - v(X) - \gamma v(\omega^m X) - \gamma^2 v(\omega^{2m} X))/Z(X)\DIFaddbeginFL \DIFaddFL{.
							}\DIFaddendFL \end{align}
						\item $\prover$ sends commitment $\gone{Q}$ = $\gone{Q(X)}$.
						\item $\verifier$ sends $s\gets\F$.
						\item $\prover$ sends evaluations $\val{s}{v}=v(s)$, $\val{\omega^m s}{v}=v(\omega^m s)$,
						$\val{\omega^{2m}s}{v}=v(\omega^{2m} s)$, $\val{s^3}{h}=h(s^3)$, $\val{s}{Q}=Q(s)$ and $\val{s}{Z}=Z(s)$.
						\item $\verifier$ sends $r\gets\F$.
						\item $\prover$ computes $\kzg$ proofs:
						\begin{itemize}[leftmargin=1em]
							\item $\Pi_v=\kzgprove(\srs,v,(s,\omega^m s, \omega^{2m}s))$.
							\item $\Pi_h=\kzgprove(\srs,h,s^3)$.
							\item $\Pi_f=\kzgprove(\srs,f,s)$ where $f(X)=Z(X) + rQ(X)$.
						\end{itemize}
						\item $\prover$ sends $\Pi_v$, $\Pi_h$ and $\Pi_f$.
						\item $\verifier$ Computes commitments $\gone{h}$ and $\gone{f}$ using homomorphism.
						\item $\verifier$ checks:
						\begin{itemize}[leftmargin=1em]
							\item $\kzgverify(\srs,\gone{v}, \vec{e}_v, \vec{p}_v, \Pi_v)$ where
							$\vec{p}_v$ = $(s,\omega^m s, \omega^{2m}s)$ and $\vec{e}_v$ = $(\val{s}{v}, \val{\omega^m s}{v}, \val{\omega^{2m} s}{v})$.
							\item $\kzgverify(\srs,\gone{h},  \val{s^3}{h}, s^3,\Pi_h)$.
							\item $\kzgverify(\srs,\gone{f},  \val{s}{Z} + r\val{s}{Q}, s,\Pi_f)$.
							\item $\val{s}{Q}\cdot \val{s}{Z} = \val{s^3}{h}-\val{s}{v}-\gamma \val{\omega^m s}{v}-\gamma^2\val{\omega^{2m}s}{v}$.
						\end{itemize}
						\item $\verifier$ outputs accept if all the above checks succeed, else it rejects.
					\end{enumerate}
				}
			\end{mdframed}
			%\vspace*{-5mm}
			\DIFdelbeginFL %DIFDELCMD < \caption{%
			{%DIFAUXCMD
				\DIFdelFL{Protocol: Check concatenation via commitments}}
			%DIFAUXCMD
			\DIFdelendFL \DIFaddbeginFL \caption{\DIFaddFL{Check concatenation over committed vectors.}}
			\DIFaddendFL \label{fig:concatenation}
		\end{figure}
		


	\section{Succinct Argument for Verifiable RAM}\label{sec:poly-proto-ram}
	The polynomial encodings in the previous section can be used to polynomial protocol for
	checking the membership in the language $\LRAM{I}{m}{m}$ for $m\in \N$. The polynomial protocol can be subsequently
	be compiled into a succinct argument using an extractable polynomial commitment scheme.
	In this section, we use $\kzg$ polynomial commitment scheme to obtain a succinct argument for checking membership in $\LRAM{I}{m}{m}$
	in the Algebraic Group Model (AGM).
	At a high level, to prove \DIFdelbegin \DIFdel{$(\vec{T},\vec{o},\vec{T'})\in \LRAM{I}{m}{m}$}\DIFdelend \\ \DIFaddbegin \DIFadd{$(\vecT,\vec{o},\vecT')\in \LRAM{I}{m}{m}$}\DIFaddend , the prover
	constructs time ordered transcript $\tr$ and then permutes it to obtain the address sorted transcript $\tr^\ast$.
	It then sends the polynomial encodings of \DIFdelbegin \DIFdel{$\vec{T},\vec{o},\vec{T'},\tr$ }\DIFdelend \DIFaddbegin \DIFadd{$\vecT,\vec{o},\vecT',\tr$ }\DIFaddend and $\tr^\ast$ to the verifier, who verifies that:
	\DIFdelbegin %DIFDELCMD < \begin{enumerate}[leftmargin=1em]
	%DIFDELCMD < 					%%%
	\DIFdelend \DIFaddbegin \begin{enumerate}[leftmargin=2em]
		\DIFaddend \item The time ordered transcript is correctly constructed, i.e, \\ \DIFdelbegin \DIFdel{$\tr=\TOT(\vec{T},\vec{o},\vec{T'})$}\DIFdelend \DIFaddbegin \DIFadd{$\tr=\TOT(\vecT,\vec{o},\vecT')$}\DIFaddend .
		\DIFaddbegin \DIFadd{This is achieved using the protocol in Figure~\ref{fig:time-ordered-transcript}.
		}\DIFaddend \item The transcript $\tr^\ast$ is a permutation of the transcript $\tr$, i.e, $\tr^\ast=\sigma(\tr)$ for some permutation $\sigma$ of $[k]$.
		\DIFaddbegin \DIFadd{The protocol for this check appears in Figure~\ref{fig:permutated-transcripts}.
		}\DIFaddend \item The transcript $\tr^\ast$ is address ordered and satisfies load-store consistency. \DIFdelbegin %DIFDELCMD < \end{enumerate}
		%DIFDELCMD < 				%%%
		\DIFdel{In fact, we consider memebership in }\DIFdelend \DIFaddbegin \DIFadd{We describe }\DIFaddend the \DIFaddbegin \DIFadd{protocol
			to check this property of transcripts in Figure~\ref{fig:encoded-relations}.
	}\end{enumerate}
	\DIFadd{We check }\DIFaddend above \DIFdelbegin \DIFdel{language under }\DIFdelend \DIFaddbegin \DIFadd{conditions over }\DIFaddend commitments. Let $\srs$ denote a $\kzg$ setup over a \DIFdelbegin \DIFdel{bilinear }\DIFdelend \DIFaddbegin \DIFadd{bi-linear }\DIFaddend group, with
	prime order groups $\Gone, \Gtwo$ and $\GT$. We canonically commit to RAM, operation sequences and transcripts by committing to their
	polynomial encodings. Commitment of an encoding represented as tuple of polynomials is simply the tuple consisting of commitments of the component
	polynomials. We now define
	the relation $\CLRAM$ below, and present a succinct argument for the same.
	\begin{definition}\label{defn:committed-vram}
		Let $\CLRAM$ consist of tuples \DIFdelbegin \DIFdel{$((c_T, c_o, c_T'), (T, \vec{o},T'))$ }\DIFdelend \DIFaddbegin \DIFadd{$((c_T, c_o, c_T')$, $(\vecT, \vec{o},\vecT'))$ }\DIFaddend where $c_T$ $=$ $\kzgcommit(\srs, \wt{T})$,\\
		$c_T'$ $=$ $\kzgcommit(\srs, \wt{T'})$,
		$c_o$ $=$ $\kzgcommit(\srs,\wt{O})$ commit to \DIFdelbegin \DIFdel{$T$}\DIFdelend \DIFaddbegin \DIFadd{$\vecT$}\DIFaddend , \DIFdelbegin \DIFdel{$T'$ }\DIFdelend \DIFaddbegin \DIFadd{$\vecT'$ }\DIFaddend and $\vec{o}$ with \\ \DIFdelbegin \DIFdel{$(T,\vec{o},T')\in \LRAM{I}{m}{m}$}\DIFdelend \DIFaddbegin \DIFadd{$(\vecT,\vec{o},\vecT')\in \LRAM{I}{m}{m}$}\DIFaddend .
	\end{definition}
	\noindent In the above definition we have $c_T=(c_a,c_v)$ where $c_a$ and $c_v$ are $\kzg$ commitments to polynomials $a(X)$ and $v(X)$ in
	the encoding $\wt{T}=(a(X), v(X))$. Similarly we parse $c_T'=(c_a', c_v')$ and $c_o=(\bar{c}_\op, \bar{c}_a, \bar{c}_v)$ (see \DIFdelbegin \DIFdel{Section
	}\DIFdelend \DIFaddbegin \DIFadd{Appendix
	}\DIFaddend \ref{subsec:poly-encoding} for polynomial encodings).
	For proving relation\DIFaddbegin \DIFadd{~}\DIFaddend \ref{defn:committed-vram}, prover's input consists of initial RAM state \DIFdelbegin \DIFdel{$T=(\vec{a},\vec{v})$}\DIFdelend \DIFaddbegin \DIFadd{$\vecT=(\vec{a},\vec{v})$}\DIFaddend ,
	final RAM state \DIFdelbegin \DIFdel{$T'=(\vec{a'},\vec{v'})$}\DIFdelend \DIFaddbegin \DIFadd{$\vecT'=(\vec{a'},\vec{v'})$}\DIFaddend , operation sequence $\vec{o}=(o_1,\ldots,o_m)$ with $o_i=(\bar{\op}_i,\bar{a}_i,\bar{v}_i)$,
	time-ordered transcript $\tr=(\vec{t},\vec{\op},\vec{A},\vec{V})$ and address-ordered transcript $\tr^\ast=(\vec{t^\ast},\vec{\op^\ast},
	\vec{A^\ast},\vec{V^\ast})$ obtained from $\tr$ using a permutation $\sigma:[k]\rightarrow [k]$.
	Verifier's input consists of the commitments $c_T, c_o$ and $c_T'$ as described above.
	
	The prover starts the protocol by sending commitments $c_\tr$ and $c^\ast_\tr$ to the transcripts $\tr$ and $\tr^\ast$ respectively.
	To show that $\tr$ is correctly formed, the prover needs to prove the concatenations:
	(i) $\vec{\op}=0^m||(\bar{\op}_1,\ldots,\bar{\op}_m)||0^m$, (ii) $\vec{A}=\vec{a}||(\bar{a}_1,\ldots,\bar{a}_m)||\vec{a'}$
	and (iii) $\vec{V}=\vec{v}||(\bar{v}_1,\ldots,\bar{v}_m)||\vec{v'}$. Note that the time-stamp column $\vec{t}$ is implicitly assumed
	to be $(1,\ldots,k)$.
	The verifier checks the concatenations using Lemma \ref{lem:vec-concatenation}.
	It uses a random challenge $\beta$ to reduce the three concatenations to one concatenation, and uses another challenge $\gamma$
	to reduce the three polynomial checks in Lemma \ref{lem:vec-concatenation} to a single check.
	The complete polynomial protocol is detailed in Figure \ref{fig:time-ordered-transcript}.
	
	
	
%	
	
	\DIFaddbegin \begin{table*}[bt]
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			{\bf \DIFaddFL{Component  }}                                                                                      & {\bf \DIFaddFL{Protocol}} & {\bf \DIFaddFL{Prover Work}}      & {\bf \DIFaddFL{Verifier Work}} & {\bf \DIFaddFL{Communication}}   \\ \hline
			\DIFaddFL{Concatenation of transcripts                                                                    }& \DIFaddFL{Figure ~\ref{fig:time-ordered-transcript}    }& \DIFaddFL{$O(m \log m)\,\F,\, O(m)\,\Gone$      }& \DIFaddFL{$2P$            }& \DIFaddFL{$4\Gone$, $6\F$         }\\ \hline
			\DIFaddFL{Permutation of transcripts                                                                      }& \DIFaddFL{Figure ~\ref{fig:permutated-transcripts}   }& \DIFaddFL{$O(m \log m)\,\F,\, O(m)\,\Gone$       }& \DIFaddFL{$2P$            }& \DIFaddFL{$4\Gone$, $5\F$         }\\ \hline
			\begin{tabular}[c]{@{}l@{}}\DIFaddFL{Memory consistency \& }\\ \DIFaddFL{address ordering of transcript}\end{tabular} & \DIFaddFL{Figure ~\ref{fig:encoded-relations}    }& \DIFaddFL{$O(m \log m)\,\F,\, O(m)\,\Gone$       }& \DIFaddFL{$6P$            }& \DIFaddFL{$20\Gone$,$19\F$        }\\ \hline
			\rowcolor{lightgray}
			\DIFaddFL{Polynomial Protocol for RAM                                                                     }& \DIFaddFL{Figure ~\ref{fig:covering-protocol}   }& \DIFaddFL{$O(m \log m)\,\F,\, O(m)\,\Gone$      }& \DIFaddFL{$7P$            }& \DIFaddFL{$36\Gone$, $30\F$       }\\ \hline
			
		\end{tabular}
		\caption{\DIFaddFL{Efficiency parameters for components of polynomial protocol for RAM. Here $m$ denotes both the size of
				the RAM and number of operations (the special case we consider). $P$ denotes a pairing evaluation, while $\Gone$
				$\Gtwo$ and $\F$ denote the groups and the scalar field of the bilinear group used for instantiating the protocol.
		}}
		\label{tbl:components-poly-ram}
		\vspace*{-5mm}
	\end{table*}
%	
%	
	\DIFaddend %\begin{tcolorbox}
	\begin{figure}[tb!]
		\scriptsize
		
		\begin{mdframed}
			{
				{\bf Common Input}: Commitments $c_T=(c_a,c_v)$, $c_o=(\bar{c}_\op, \bar{c}_a, \bar{c}_v)$, $c_T'=(c_a', c_v')$
				and $c_\tr=(c_t, c_\op, c_A, c_V)$ to \DIFdelbeginFL \DIFdelFL{$T,o,T'$ }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{$\vecT,\vec{o},\vecT'$ }\DIFaddendFL and $\tr$(which is supposed to be the time ordered transcript) respectively. Commitment $\gone{Z(X)}$ to the polynomial
				$Z(X)=\prod_{i=1}^m (X-\omega^i)$\DIFaddbeginFL \DIFaddFL{.}\DIFaddendFL \\
				{\bf Prover's Input}: \DIFdelbeginFL \DIFdelFL{$\tr, T, T', \vec{o}$ }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{$\tr, \vecT, \vecT', \vec{o}$ }\DIFaddendFL and their polynomial encodings, $Z(X)$\DIFaddbeginFL \DIFaddFL{.
				}\DIFaddendFL \begin{enumerate}[leftmargin=1em, label=\arabic*.]
					\item $\verifier$ sends $\beta,\gamma\gets \F$.
					\item $\prover$ computes:
					\begin{align}\label{eq:poly-constraints}
						& G_1(X) = a(X) + \beta v(X),\, G_2(X) = \bar{a}(X) + \beta \bar{v}(X) + \beta^2 \bar{\op}(X) \tag{A1} \\
						& G_3(X) = a\DIFdelbeginFL \DIFdelFL{'}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{^*}\DIFaddendFL (X) + \beta v\DIFdelbeginFL \DIFdelFL{'}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{^*}\DIFaddendFL (X),\, G(X) = A(X) + \beta V(X) + \beta^2 \op(X) \tag{A2} \\
						& H(X) = G_1(X) + \gamma G_2(X) + \gamma^2 G_3(X), \tag{A3} \\
						& Q(X) = [(H(X^3) - G(X) - \gamma G(\omega^m X) - \gamma^2 G(\omega^{2m} X))]/Z(X) \tag{A4}
					\end{align}
					\item $\prover$ sends commitment $\gone{Q}$ to $Q(X)$.
					\item $\verifier$ sends $s\gets\F$.
					\item $\prover$ sends evaluations $\val{s}{G}=G(s)$, $\val{\omega^m s}{G}=G(\omega^m s)$,
					$\val{\omega^{2m}s}{G}=G(\omega^{2m} s)$, $\val{s^3}{H}=H(s^3)$, $\val{s}{Q}=Q(s)$ and $\val{s}{Z}=Z(s)$.
					\item $\verifier$ sends $r\gets\F$.
					\item $\prover$ sends $\kzg$ proofs:
					\begin{itemize}[leftmargin=1em]
						\item $\Pi_G=\kzgprove(\srs,G(X),(s,\omega^m s, \omega^{2m}s))$.
						\item $\Pi_H=\kzgprove(\srs,H(X),s^3)$.
						\item $\Pi_F=\kzgprove(\srs,F(X),s)$ where $F(X)=Z(X) + rQ(X)$.
					\end{itemize}
					\item $\verifier$ computes $\gone{G(X)},\gone{H(X)}$ and $\gone{F(X)}$ using homomorphism.
					\item $\verifier$ checks:
					\begin{itemize}[leftmargin=1em]
						\item $\kzgverify(\srs,\gone{G}, (\val{s}{G}, \val{\omega^m s}{G}, \val{\omega^{2m} s}{G}),\\(s,\omega^m s, \omega^{2m}s),\Pi_G)$.
						\item $\kzgverify(\srs,\gone{H}, \val{s^3}{H}, s^3, \Pi_H)$.
						\item $\kzgverify(\srs,\gone{F}, \val{s}{Z} + r\val{s}{Q}, s, \Pi_F)$.
						\item $\val{s}{Q}\cdot \val{s}{Z} = \val{s^3}{H}-\val{s}{G}-\gamma \val{\omega^m s}{G}-\gamma^2\val{\omega^{2m}s}{G}$.
					\end{itemize}
					\item $\verifier$ accepts if all the above checks succeeds, otherwise it rejects.
				\end{enumerate}
			}
		\end{mdframed}
		%    \vspace*{-5mm}
		\DIFdelbeginFL %DIFDELCMD < \caption{%
		{%DIFAUXCMD
			\DIFdelFL{Protocol: Check correctness of time-ordered transcript}}
		%DIFAUXCMD
		\DIFdelendFL \DIFaddbeginFL \caption{\DIFaddFL{Check the correctness of time-ordered transcript.}}
		\DIFaddendFL \label{fig:time-ordered-transcript}
	\end{figure}
	%\end{tcolorbox}
	
	Next, we show a polynomial protocol for proving that the transcript $\tr^\ast$ is a permutation of the transcript $\tr$.
	We first recall the permutation argument for vectors from ~\cite{EPRINT:GabWilCio19}.
	\begin{lemma}[Permutation Check \cite{EPRINT:GabWilCio19}]\label{lem:perm-argument}
		Let $f(X), g(X)$ be polynomials in $\F[\,X\,]$. Then, the vectors $\vec{f}, \vec{g}\in \F^k$ encoded by the polynomials
		are permutations of each other if and only if with overwhelming probability over the choice of $\alpha\gets \F$,
		there exists a polynomial $z(X)$ satisfying the polynomial constraints:
		\DIFdelbegin %DIFDELCMD < {\small
		%DIFDELCMD < 						%%%
		\DIFdelend \DIFaddbegin 
		
		\DIFaddend \begin{align}
			\lambda_1(X)(z(X) -1) &= 0 \text{ mod } Z_\setH(X) \tag{B1} \label{eq:B1} \\
			(\alpha - g(X))z(\omega X) &= (\alpha - f(X))z(X) \text{ mod } Z_\setH(X) \tag{B2} \label{eq:B2}
		\end{align}
		\DIFdelbegin %DIFDELCMD < }
		%DIFDELCMD < 				%%%
		\DIFdelend \DIFaddbegin 
		
		\DIFaddend \end{lemma}
	The polynomial protocol in Figure \ref{fig:permutated-transcripts} essentially invokes the above argument on
	the random linear combination of the columns of the respective transcripts.
%	\begin{tcolorbox}
\begin{figure}[htbp]
	\small
	
	\begin{mdframed}
		{
			{\bf Common Input}: Commitments $c_\tr=(c_t,c_\op,c_A, c_V)$ and $c_\tr^\ast=(c_t^\ast,c_\op^\ast, c_A^\ast, c_V^\ast)$
			of transcripts $\tr$ and $\tr^\ast$ respectively.\\
			{\bf Prover's Input}: Transcripts $\tr, \tr^{*}$ and their polynomial encodings, permutation $\sigma$ such that $\tr^{*}=\sigma(\tr)$
			\begin{enumerate}[leftmargin=1em, label=\arabic*]
				\item $\verifier$ sends $\alpha,\beta, \chi\gets \F$
				\item $\prover$ computes $f(X)=t(X) + \beta \op(X) + \beta^2 A(X) + \beta^3 V(X)$, $g(X) = t^\ast(X) + \beta \op^\ast(X)$
				$+ \beta^2 A^\ast(X) + \beta^3 V^\ast(X)$. It then computes polynomials $z(X),q(X)$ as:
				\begin{itemize}[leftmargin=1em]
					\item Interpolate polynomial $z(X)$ of degree $k-1$ such that $z(\omega)=1$ and
					$z(\omega^{i+1})=\prod_{j=1}^i (\alpha - f(\omega^j))/(\alpha - g(\omega^j))$ for $1\leq i\leq k-1$.
					\item Compute $q(X) = ((\alpha - g(X))z(\omega X) - (\alpha - f(X))z(X) + \chi\lambda_1(X)(z(X) - 1))/\mathbb{Z}_{\setH}(X)$.
				\end{itemize}
				\item $\prover$ sends commitments $\gone{z(X)}$ and $\gone{q(X)}$ to polynomials $z(X)$ and $q(X)$ respectively.
				\item $\verifier$ computes commitments $[f]_1, [g]_1$ by homomorphism
				\item $\verifier$ checks that $q(X)Z_\setH(X)$ = $(\alpha - g(X))z(\omega X)-(\alpha - f(X))z(X) + \chi\lambda_1(X)(z(X) - 1)$
				by requesting evaluations and $\kzg$ proofs of polynomials $f,g, q, z$ at a random point, say $s$ and evaluation and $\kzg$ proof of $z$ at $\omega s$.
				\item $\verifier$ accepts if all the checks succeed, else it rejects
			\end{enumerate}
		}
	\end{mdframed}
	%    \vspace*{-5mm}
	\caption{\DIFadd{Check that transcripts are permutations of each other.}}
	\label{fig:permutated-transcripts}
\end{figure}
%	
	Finally, we see that Lemma\DIFaddbegin \DIFadd{~}\DIFaddend \ref{lem:addr-ordered-transcript} implies a polynomial protocol to check that the transcript
	$\tr^\ast$ is address ordered and satisfies load-store consistency, which
	essentially involves the prover identifying sets
	$I_1, I_2$ as described in \DIFdelbegin \DIFdel{Section }\DIFdelend \DIFaddbegin \DIFadd{Appendix }\DIFaddend \ref{subsec:encoded-relations} and sending auxiliary polynomials $Z_1(X),Z_2(X),\delta^\ast_A(X)$ and $\delta^\ast_T(X)$ to the verifier.
	The verifier then checks the identities (C1)-(C6) in Lemma \ref{lem:addr-ordered-transcript}.
	The range checks in (C7) can be checked using polynomial protocols in sub-vector lookup arguments such as
	\DIFdelbegin \DIFdel{Caulk+ }\DIFdelend ~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
			\cite{EPRINT:PosKat22}}\hskip0pt%DIFAUXCMD
		, CQ
		~\mbox{%DIFAUXCMD
			\cite{EPRINT:EagFioGab22}}\hskip0pt%DIFAUXCMD
	}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
			\cite{EPRINT:PosKat22,EPRINT:EagFioGab22,PKC:CFFLL24,PKC:ZSG24}}\hskip0pt%DIFAUXCMD
	}\DIFaddend .
	The protocol (compiled using KZG commitments in AGM) can be found in \DIFdelbegin \DIFdel{figure }\DIFdelend \DIFaddbegin \DIFadd{Figure~}\DIFaddend \ref{fig:encoded-relations}.
	The overall \DIFdelbegin \DIFdel{covering }\DIFdelend protocol for $\CLRAM$ which combines \DIFdelbegin \DIFdel{figures }\DIFdelend \DIFaddbegin \DIFadd{invokes protocols in Figures~}\DIFaddend \ref{fig:time-ordered-transcript},\ref{fig:permutated-transcripts}
	\DIFdelbegin \DIFdel{, }\DIFdelend \DIFaddbegin \DIFadd{and }\DIFaddend \ref{fig:encoded-relations} \DIFaddbegin \DIFadd{as sub-protocols is presented }\DIFaddend in \DIFdelbegin \DIFdel{this order can be found in figure }\DIFdelend \DIFaddbegin \DIFadd{Figure~}\DIFaddend \ref{fig:covering-protocol}.
	\begin{figure}[htbp]
		\small
		
		\begin{mdframed}
			{
				{\bf Common Input}: Commitments $c_T=(c_a,c_v)$, $c_o=(\bar{c}_\op, \bar{c}_a, \bar{c}_v)$, $c_T'=(c_a', c_v')$\DIFaddbeginFL \DIFaddFL{.
				}\DIFaddendFL 
				
				{\bf Prover's Input}: \DIFdelbeginFL \DIFdelFL{$ T, T', \vec{o}$ }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{$ \vecT, \vecT', \vec{o}$ }\DIFaddendFL and their polynomial encodings\DIFaddbeginFL \DIFaddFL{.
				}\DIFaddendFL \begin{enumerate}[leftmargin=1em, label=\arabic*.]
					\item $\prover$ computes:
					\begin{itemize}
						\item $\tr$, which is the time ordered transcript corresponding to \DIFdelbeginFL \DIFdelFL{$T, \vec{o}, T'$}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{$\vecT, \vec{o}, \vecT'$}\DIFaddendFL , and its polynomial encoding\DIFaddbeginFL \DIFaddFL{.
						}\DIFaddendFL \item $Z(X)=\prod_{i=1}^m (X-\omega^i)$\DIFaddbeginFL \DIFaddFL{.
						}\DIFaddendFL \item $c_\tr=(c_t, c_\op, c_A, c_V)$ which is the commitment of $\tr$\DIFaddbeginFL \DIFaddFL{.
						}\DIFaddendFL \item $[Z(X)]_1$\DIFaddbeginFL \DIFaddFL{.
						}\DIFaddendFL \end{itemize}
					
					\item $\prover$ sends  $c_\tr=(c_t, c_\op, c_A, c_V)$ and $[Z(X)]_1$\DIFaddbeginFL \DIFaddFL{.
					}\DIFaddendFL \item \DIFdelbeginFL \DIFdelFL{$\left<\prover, \verifier \right>$ }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{$\prover$ and $\verifier$ }\DIFaddendFL run the protocol for checking correctness of time ordered transcript as in \DIFdelbeginFL \DIFdelFL{figure }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{Figure~}\DIFaddendFL \ref{fig:time-ordered-transcript}\DIFaddbeginFL \DIFaddFL{.
					}\DIFaddendFL \item $\prover$ computes the address ordered transcript $\tr^{*}$ from the time ordered transcript $\tr$ by permuting suitably. It also computes its polynomial encoding\DIFaddbeginFL \DIFaddFL{.
					}\DIFaddendFL \item $\prover$ computes the permutation $\sigma$ such that $\tr^{*}= \sigma(\tr)$ (this is computed along with the $\tr^{*}$)\DIFaddbeginFL \DIFaddFL{.
					}\DIFaddendFL \item $\prover$ computes the commitment $c_\tr^\ast=(c_t^\ast,c_\op^\ast, c_A^\ast, c_V^\ast)$ of $\tr^{*}$\DIFaddbeginFL \DIFaddFL{.
					}\DIFaddendFL \item $\prover$ sends  $c_\tr^\ast$\DIFaddbeginFL \DIFaddFL{.
					}\DIFaddendFL \item \DIFdelbeginFL \DIFdelFL{$\left<\prover, \verifier \right>$ }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{$\prover$ and $\verifier$ }\DIFaddendFL run the protocol for checking that the two transcripts are \DIFdelbeginFL \DIFdelFL{permuatations }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{permutations }\DIFaddendFL of each other
					as in \DIFdelbeginFL \DIFdelFL{figure }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{Figure~}\DIFaddendFL \ref{fig:permutated-transcripts}\DIFaddbeginFL \DIFaddFL{.
					}\DIFaddendFL \item \DIFdelbeginFL \DIFdelFL{$\left<\prover, \verifier \right>$ }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{$\prover$ and $\verifier$ }\DIFaddendFL run  the protocol for checking the constraints given in \DIFdelbeginFL \DIFdelFL{lemma }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{Lemma~}\DIFaddendFL \ref{lem:addr-ordered-transcript}
					as in \DIFdelbeginFL \DIFdelFL{figure }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{Figure~}\DIFaddendFL \ref{fig:encoded-relations}\DIFaddbeginFL \DIFaddFL{.
					}\DIFaddendFL \item $\verifier$ accepts if all the three \DIFdelbeginFL \DIFdelFL{sub protocols lead to }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{sub-protocols }\DIFaddendFL accept. $\verifier$ rejects otherwise\DIFaddbeginFL \DIFaddFL{.
					}\DIFaddendFL \end{enumerate}
			}
		\end{mdframed}
		%    \vspace*{-5mm}
		\DIFdelbeginFL %DIFDELCMD < \caption{%
		{%DIFAUXCMD
			\DIFdelFL{Protocol:Overall protocol for the relation $\CLRAM$}}
		%DIFAUXCMD
		\DIFdelendFL \DIFaddbeginFL \caption{\DIFaddFL{Overall protocol for the relation $\CLRAM$}}
		\DIFaddendFL \label{fig:covering-protocol}
	\end{figure}
	
	\DIFaddbegin \smallskip
	
	\noindent{\bf Efficiency.}
	\DIFadd{We provide a break-up of costs incurred by different components involved in construction
		of RAM based on memory-checking techniques in Table ~\ref{tbl:components-poly-ram}. To reduce pairing
		checks we use standard technique of batching pairing checks involving common generators. In addition, to reduce
		communication, instead of naively invoking four instances of sub-vector argument in Step 15 of the protocol
		in Figure~\ref{fig:encoded-relations}, we concatenate the four vectors using a variant of protocol for
		concatenation of vectors in Figure~\ref{fig:concatenation}, and then use the sub-vector argument to show that
		the concatenated vector is a sub-vector of the vector $(1,\ldots,N)$. For CQ~\mbox{%DIFAUXCMD
			\cite{EPRINT:EagFioGab22} }\hskip0pt%DIFAUXCMD
		based instantiation,
		this reduces the total communication of this
		check from $4\times (8\Gone + 3\F)$ to $(4\Gone+6\F) + (8\Gone + 3\F)$, a saving of $\approx 20\Gone$. The reported
		overheads in Table ~\ref{tbl:components-poly-ram} take into account such optimizations.
	}
	
	\DIFaddend \section{Proof of Lemma ~\ref{lem:sum-computation}}
	%Recall the statement of lemma \ref{lem:sum computation}. We want to compute $e_i$ for all $i \in \setind$ and $b_j$ for all $j \in K$ in $O(|K|\log^2|K|)$ $\mathbb{F}$ operations.\\\\
	Before starting the proof, we collect some preliminaries which will be useful in the proof.
	
	\subsection{Computational Algebra Preliminaries}\label{subsec:comp-algebra-app}
	Let $\F$ be a finite field of prime order $p$ and $\G$ be a cyclic additive group of order $p$ with generator $g$. For $s \in \F$, we use
	the notation $\elt{s}$ to denote the group element $s\cdot g$. Assume that $\F$ contains $n^{th}$ root of unity $\xi$
	satisfying $\xi^n=1$ for large $n$ (All polynomial degrees are assumed less than $n$).
	
	\begin{fact}[\textsf{Fast Evaluation}]\label{fc:fft}
		Let $f\in \F[X]$ be a polynomial of degree $<d$ and $(\xi_1,\ldots,\xi_r)\in \F^r$ be distinct points in $\F$.
		Then the vector $(f(\xi_1),\ldots,f(\xi_r))$ can be computed in $O((d+r)\log (d+r))$ $\F$ operations if $\xi_1,\ldots,\xi_r$ form roots
		of unity, and in $O((d+r)\log^2(d+r))$ $\F$ operations otherwise.
	\end{fact}
	
	\begin{fact}[\textsf{Fast Interpolation}]\label{fc:ifft}
		Let $\xi_1,\ldots,\xi_d$ be distinct points in $\F$ and $(v_1,\ldots,v_d)\in \F^d$. Then $(f_0,\ldots,f_{d-1})\in \F^d$
		can be computed in $O(d\log^2 d)$ operations in $\F$ such that $f(\xi_i)=v_i$ for all $i\in [d]$ where
		$f(X)=\sum_{i=0}^{d-1}f_iX^i$.
	\end{fact}
	
	\begin{fact}[\textsf{Fast Multiplication}]\label{fc:mult}
		Let $\xi_1,\ldots,\xi_r$ be distinct points in $\F$. Then coefficients of $f(X)=\prod_{i=1}^r (X-\xi_i)$
		can be computed in $O(r\log^2 r)$ operations in $\F$.
	\end{fact}
	
	\begin{fact}[\textsf{Multi KZG proofs} ~\cite{EPRINT:FeiKho23}]\label{fc:multkzg}
		Let $\{\elt{x^i}\}_{i=1}^d$ be given for some $x\in \F$. Then for set of $r$ distinct points $\xi_1,\ldots,\xi_r$,
		and a polynomial $f(X)\in \F[X]$ of degree $<d$,the vector $(\elt{h_1(x)},\ldots,\elt{h_r(x)})$,
		where $h_i(X) = (f(X) - f(\xi_i))/(X - \xi_i)$ can be computed in
		$O((r+d)\log(r+d))$ group and field operations when $\xi_1,\ldots,\xi_r$ are roots of unity, and in
		$O(rlog^2 r + d\log d)$ group and field operations otherwise.
	\end{fact}
	
	\begin{fact}[\textsf{Lagrange Polynomials}]\label{fc:lagrange}
		Let $\mathbb{S}=\{\xi_1,\ldots,\xi_r\}$ be a set of $r$ distinct points and let $\tau_1(X),\ldots,\tau_r(X)$ be
		the corresponding lagrange polynomials of degree $r-1$ each. Let $Z_{\mathbb{S}}(X)=\prod_{i=1}^r (X-\xi_r)$ denote the vanishing polynomial
		for $\mathbb{S}$. Then we have:
		\begin{gather*}
			\sum_{i=1}^r \tau_i(X) = 1 \\
			\tau_i(X) = \frac{Z_{\mathbb{S}}(X)}{Z_{\mathbb{S}}'(\xi_i)(X-\xi_i)} \text{ for all } i\in [r]
		\end{gather*}
	\end{fact}
	
	\noindent{\bf Formal Derivative}: For a polynomial $p(X) \in \F[X]$, we define the formal derivative of $p(X)$ as the polynomial
	$u(X,X)$ where $u(X,Y)=\frac{p(X)-p(Y)}{X-Y}$. It can be seen that $u(X,X)$ equals the polynomial $p'(X)$ obtained by differentiating
	$p(X)$ according to regular rules of calculus. \DIFaddbegin \DIFadd{Thus, this definition agrees with the one given earlier in the preliminaries.
	}\DIFaddend %Let us denote the formal derivative of $p(X)$ as $p^f(X)$.\\
	%Then, $p^f(X)= u(X,X)$ where $u(X,Y)=\frac{p(X)-p(Y)}{X-Y}$\\\\
	%It is well known that the formal derivative of a polynomial satisfies all the properties that we want the derivative of a polynomial to satisfy, like linearity, product rule, quotient rule etc.\\\\
	%As a result, the formal derivative $p^f(X)$ defined as above can be computed from $p(X)$ in the \textit{usual} way, that is $\frac{d}{dx}X^k=k X^{k-1}$ and extend using linearity of derivative.\\
	%Henceforth, we will represent the formal derivative of $p(X)$ as $p'(X)$ and because of the above statements, we will compute it in the usual way and freely use all the properties of derivatives on it.\\
	
	\subsection{Some Useful Results}\label{subsec:sub-results}
	We state and prove some facts which are used later \DIFdelbegin \DIFdel{througout }\DIFdelend \DIFaddbegin \DIFadd{throughout }\DIFaddend the proof.
	
	\begin{lemma}\label{lem:sumtoder}
		For $K\subset [N]$, define $H_K$ to be $\{\xi^i:i \in K\}$. Let $p(X)$ be the vanishing polynomial of $H_{K}$.
		Let $p'(X)$ and $p''(X)$ denote the formal first derivative and second derivative of $p(X)$ respectively.
		Then, $p''(\xi^i)/p'(\xi^i)=2 \cdot \sum_{j\in K\setminus \{i\}} 1/(\xi^i-\xi^j)$ for all $i \in K$
	\end{lemma}
	
	\begin{proof}
		Observe that $p'(X)=\sum_{i\in K}\prod_{j\in K\setminus \{i\}}(X-\xi^j)$ and \DIFaddbegin \\
		\DIFaddend $p''(X)=\sum_{i\in K}\sum_{j\in K\setminus \{i\}}\prod_{k\in K\setminus \{i,j\}}(X-\xi^k)$. Thus for $r\in K$,
		we have:
		\begin{align*}
			p'(\xi^r) &= \prod_{j \in K \setminus \{r\}}(\xi^r-\xi^j), \\
			p''(\xi^r)&=\sum_{j\in K\setminus \{r\}}\prod_{k\in K\setminus \{r,j\}} (\xi^r-\xi^k)
			+\sum_{i\in K\setminus \{r\}}\prod_{k\in K\setminus \{r,i\}} (\xi^r-\xi^k)
		\end{align*}
		Note that only non-zero products in the expansion of $p''(\xi^r)$ occur when $i=r$ or $j=r$, resulting in the two summands for the same in the above equation.
		Moreover, we notice that both summands are the same, giving us $p''(\xi^r)=2\sum_{i\in K\setminus \{r\}}\prod_{k\in \setminus \{r,i\}}(\xi^r-\xi^k)$. One may
		now verify that $p'(\xi^r)/p''(\xi^r)$ gives the desired result claimed in the lemma.
	\end{proof}
	%To compute $p''(\xi^r)$, note that $p''(X)$ is a (double) sum of many products. When one substitutes $\xi^r$ in a product, the product is non zero if and only if $r=i\, \text{or}\, r=j$\\\\
	
	%    Putting $\xi^r$ gives:
	%    } p''(\xi^r)=\sum_{j\in K\setminus \{r\}}\prod_{k\in K\setminus \{r,j\}} (\xi^r-\xi^k)+$$
	%    $$ \sum_{i\in K\setminus \{r\}}\prod_{k\in K\setminus \{r,i\}} (\xi^r-\xi^k) $$
	%    But this is just,
	%    $$2\cdot \sum_{j\in K\setminus \{r\}}\prod_{k\in K\setminus \{r,j\}} (\xi^r-\xi^k) $$
	
	%    Now, we are left to show that $\frac{\frac{p''(\xi^r)}{2}}{p'(\xi^r})=\sum_{j\in K\setminus \{r\}} 1/(\xi^r-\xi^j)$, or
	%    $$\sum_{j\in K\setminus \{r\}}\prod_{k\in K\setminus \{r,j\}} (\xi^r-\xi^k)=\prod_{j \in K \setminus \{r\}}(\xi^r-\xi^j) \sum_{j\in K\setminus \{r\}} 1/(\xi^r-\xi^j)$$
	
	%    But we observe that:
	%    $$\sum_{j\in K\setminus \{r\}}\prod_{k\in K\setminus \{r,j\}} (\xi^r-\xi^k)=\sum_{j\in K\setminus \{r\}}\frac{\prod_{k\in K\setminus \{r\}} (\xi^r-\xi^k)}{(\xi^r-\xi^j)}$$
	
	%    But $\prod_{k\in K\setminus \{r\}} (\xi^r-\xi^k)$ is independent of $j$, so it can be pulled outside the sum:
	%    $$\sum_{j\in K\setminus \{r\}}\prod_{k\in K\setminus \{r,j\}} (\xi^r-\xi^k)=\prod_{k\in K\setminus \{r\}} (\xi^r-\xi^k) \sum_{j\in K\setminus \{r\}} 1/(\xi^r-\xi^j) $$
	%    But $k$ is just a dummy variable. We can replace $k$ by $j$ on the RHS:
	%    $$\sum_{j\in K\setminus \{r\}}\prod_{k\in K\setminus \{r,j\}} (\xi^r-\xi^k)=\prod_{j\in K\setminus \{r\}} (\xi^r-\xi^j) \sum_{j\in K\setminus \{r\}} 1/(\xi^r-\xi^j) $$
	%    This completes the proof.
	%\end{proof}
	
	\DIFdelbegin %DIFDELCMD < \begin{comment}%DIFDELCMD < 
	%DIFDELCMD < 					************ ALTERNATIVE PROOF **************
	%DIFDELCMD < 					\textbf{An alternative proof of \ref{lem:sumtoder}}:\\
	%DIFDELCMD < 					An alternative proof is possible by inducting on the cardinality of $K\subset N$.\\
	%DIFDELCMD < 					The base case where $K$ has one element, say $i$ is trivial. This is because in that case $p$ is a linear polynomial $X-\xi^i$ and LHS=$\frac{p''}{p'}=\frac{0}{1}=0$ and on RHS we are summing over the empty set so the sum is $0$.\\\\
	%DIFDELCMD < 					Now let $|K|=k$ and the elements of $K$ be $a_1, a_2, \cdots, a_k$.
	%DIFDELCMD < 					Consequently $p(X)=\prod_{i \in K}(X-\xi^i)$\\
	%DIFDELCMD < 					We assume that the result holds for this case. Thus,
	%DIFDELCMD < 					$$p''(\xi^i)/p'(\xi^i)=2 \cdot \sum_{j\in K\setminus \{i\}} 1/(\xi^i-\xi^j)$$ for all $i \in K$.\\
	%DIFDELCMD < 					Suppose we introduce another element in the set $K$, call it $\alpha$
	%DIFDELCMD < 					The vanishing polynomial for the new $K$(call it $K')$ is $q(X)=p(X)(X-\xi^{\alpha})$\\
	%DIFDELCMD < 					Then by the product rule for derivatives, we have:
	%DIFDELCMD < 					$$q'(X)=p'(X)(X-\xi^{\alpha})+p(X)$$ and
	%DIFDELCMD < 					$$q''(X)=2p'(X)+p''(X)(X-\xi^{\alpha})$$
	%DIFDELCMD < 					
	%DIFDELCMD < 					It suffices to show that $$\frac{q''(\xi^i)}{q'(\xi^i)}=2\sum_{j\in K'\setminus \{i\}}\frac{1}{\xi^i-\xi^j}$$
	%DIFDELCMD < 					for all $i \in K'$\\
	%DIFDELCMD < 					For this we make 2 cases:\\
	%DIFDELCMD < 					\textbf{Case 1-}$i \in K$
	%DIFDELCMD < 					Then
	%DIFDELCMD < 					$$q'(\xi^i)=p'(\xi^i)(\xi^i-\xi^{\alpha})$$ and
	%DIFDELCMD < 					$$q''(\xi^i)=2p'(\xi^i)+p''(\xi^i)(\xi^i-\xi^{\alpha})$$
	%DIFDELCMD < 					This used that $p(\xi^i)=0$\\
	%DIFDELCMD < 					Thus, $$\frac{q''(\xi^i)}{q'(\xi^i)}=\frac{2p'(\xi^i)+p''(\xi^i)(\xi^i-\xi^{\alpha})}{p'(\xi^i)(\xi^i-\xi^{\alpha})}$$
	%DIFDELCMD < 					Dividing numerator and denominator by $p'(\xi^i)(\xi^i-\xi^{\alpha})$ gives:
	%DIFDELCMD < 					$$\frac{q''(\xi^i)}{q'(\xi^i)}=\frac{2}{\xi^i-\xi^\alpha}+\frac{p''(\xi^i)}{p'(\xi^i)}$$
	%DIFDELCMD < 					By induction hypothesis this becomes:
	%DIFDELCMD < 					$$\frac{q''(\xi^i)}{q'(\xi^i)}=\frac{2}{\xi^i-\xi^\alpha}+2 \cdot \sum_{j\in K\setminus \{i\}} 1/(\xi^i-\xi^j)$$
	%DIFDELCMD < 					Note that $$\frac{1}{\xi^i-\xi^\alpha}+ \sum_{j\in K\setminus \{i\}} 1/(\xi^i-\xi^j)=\sum_{j\in K'\setminus \{i\}}\frac{1}{\xi^i-\xi^j}$$
	%DIFDELCMD < 					Using that $K'=K \cup \{\alpha\}$\\
	%DIFDELCMD < 					Thus, $$\frac{q''(\xi^i)}{q'(\xi^i)}=2\sum_{j\in K'\setminus \{i\}}\frac{1}{\xi^i-\xi^j}$$ which completes case 1. \\\\
	%DIFDELCMD < 					\textbf{Case 2-}$i=\alpha$. Then
	%DIFDELCMD < 					$$q'(\xi^{\alpha})=p(\xi^{\alpha})$$ and
	%DIFDELCMD < 					$$q''(\xi^{\alpha})=2p'(\xi^{\alpha})$$
	%DIFDELCMD < 					These we get by replacing $X$ by $\xi^{\alpha}$ in the expressions of $q', q''$
	%DIFDELCMD < 					So, it is sufficient to prove that:
	%DIFDELCMD < 					$$\frac{2p'(\xi^{\alpha})}{p(\xi^{\alpha})}=2\sum_{j\in K'\setminus \{\alpha\}}\frac{1}{\xi^{\alpha}-\xi^j}$$
	%DIFDELCMD < 					
	%DIFDELCMD < 					But $$p(\xi^{\alpha})=\prod_{i \in K}(\xi^{\alpha}-\xi^i)=\prod_{i \in K'\setminus \{\alpha\}}(\xi^{\alpha}-\xi^i)$$ and $$p'(\xi^{\alpha})=\sum_{j \in K} \prod_{i \in K \setminus \{j\}}(\xi^{\alpha}-\xi^i)=\sum_{j \in K'\setminus \{\alpha\}} \prod_{i \in K' \setminus \{j, \alpha\}}(\xi^{\alpha}-\xi^i)$$
	%DIFDELCMD < 					But now, note that $$\sum_{j \in K'\setminus \{\alpha\}} \prod_{i \in K' \setminus \{j, \alpha\}}(\xi^{\alpha}-\xi^i)=\prod_{i \in K' \setminus \{ \alpha\}}(\xi^{\alpha}-\xi^i)\sum_{j \in K'\setminus \{\alpha\}}\frac{1}{\xi^{\alpha}-\xi^j}$$
	%DIFDELCMD < 					So, $$\frac{2p'(\xi^{\alpha})}{p(\xi^{\alpha})}=\frac{2\cdot \prod_{i \in K' \setminus \{ \alpha\}}(\xi^{\alpha}-\xi^i)\sum_{j \in K'\setminus \{\alpha\}}\frac{1}{\xi^{\alpha}-\xi^j}}{\prod_{i \in K'\setminus \{\alpha\}}(\xi^{\alpha}-\xi^i)}=$$\\
	%DIFDELCMD < 					$$=2\cdot\sum_{j \in K'\setminus \{\alpha\}}\frac{1}{\xi^{\alpha}-\xi^j}$$
	%DIFDELCMD < 					
	%DIFDELCMD < 					This completes the proof of case 2 and thus the proof of the lemma.
	%DIFDELCMD < 				%DIFDELCMD < \end{comment}
	%DIFDELCMD < 				%%%
	\DIFdelend \DIFaddbegin \begin{comment}
	************ ALTERNATIVE PROOF **************
	\textbf{An alternative proof of \ref{lem:sumtoder}}:\\
	An alternative proof is possible by inducting on the cardinality of $K\subset N$.\\
	The base case where $K$ has one element, say $i$ is trivial. This is because in that case $p$ is a linear polynomial $X-\xi^i$ and LHS=$\frac{p''}{p'}=\frac{0}{1}=0$ and on RHS we are summing over the empty set so the sum is $0$.\\\\
	Now let $|K|=k$ and the elements of $K$ be $a_1, a_2, \cdots, a_k$.
	Consequently $p(X)=\prod_{i \in K}(X-\xi^i)$\\
	We assume that the result holds for this case. Thus,
	$$p''(\xi^i)/p'(\xi^i)=2 \cdot \sum_{j\in K\setminus \{i\}} 1/(\xi^i-\xi^j)$$ for all $i \in K$.\\
	Suppose we introduce another element in the set $K$, call it $\alpha$
	The vanishing polynomial for the new $K$(call it $K')$ is $q(X)=p(X)(X-\xi^{\alpha})$\\
	Then by the product rule for derivatives, we have:
	$$q'(X)=p'(X)(X-\xi^{\alpha})+p(X)$$ and
	$$q''(X)=2p'(X)+p''(X)(X-\xi^{\alpha})$$
	
	It suffices to show that $$\frac{q''(\xi^i)}{q'(\xi^i)}=2\sum_{j\in K'\setminus \{i\}}\frac{1}{\xi^i-\xi^j}$$
	for all $i \in K'$\\
	For this we make 2 cases:\\
	\textbf{Case 1-}$i \in K$
	Then
	$$q'(\xi^i)=p'(\xi^i)(\xi^i-\xi^{\alpha})$$ and
	$$q''(\xi^i)=2p'(\xi^i)+p''(\xi^i)(\xi^i-\xi^{\alpha})$$
	This used that $p(\xi^i)=0$\\
	Thus, $$\frac{q''(\xi^i)}{q'(\xi^i)}=\frac{2p'(\xi^i)+p''(\xi^i)(\xi^i-\xi^{\alpha})}{p'(\xi^i)(\xi^i-\xi^{\alpha})}$$
	Dividing numerator and denominator by $p'(\xi^i)(\xi^i-\xi^{\alpha})$ gives:
	$$\frac{q''(\xi^i)}{q'(\xi^i)}=\frac{2}{\xi^i-\xi^\alpha}+\frac{p''(\xi^i)}{p'(\xi^i)}$$
	By induction hypothesis this becomes:
	$$\frac{q''(\xi^i)}{q'(\xi^i)}=\frac{2}{\xi^i-\xi^\alpha}+2 \cdot \sum_{j\in K\setminus \{i\}} 1/(\xi^i-\xi^j)$$
	Note that $$\frac{1}{\xi^i-\xi^\alpha}+ \sum_{j\in K\setminus \{i\}} 1/(\xi^i-\xi^j)=\sum_{j\in K'\setminus \{i\}}\frac{1}{\xi^i-\xi^j}$$
	Using that $K'=K \cup \{\alpha\}$\\
	Thus, $$\frac{q''(\xi^i)}{q'(\xi^i)}=2\sum_{j\in K'\setminus \{i\}}\frac{1}{\xi^i-\xi^j}$$ which completes case 1. \\\\
	\textbf{Case 2-}$i=\alpha$. Then
	$$q'(\xi^{\alpha})=p(\xi^{\alpha})$$ and
	$$q''(\xi^{\alpha})=2p'(\xi^{\alpha})$$
	These we get by replacing $X$ by $\xi^{\alpha}$ in the expressions of $q', q''$
	So, it is sufficient to prove that:
	$$\frac{2p'(\xi^{\alpha})}{p(\xi^{\alpha})}=2\sum_{j\in K'\setminus \{\alpha\}}\frac{1}{\xi^{\alpha}-\xi^j}$$
	
	But $$p(\xi^{\alpha})=\prod_{i \in K}(\xi^{\alpha}-\xi^i)=\prod_{i \in K'\setminus \{\alpha\}}(\xi^{\alpha}-\xi^i)$$ and $$p'(\xi^{\alpha})=\sum_{j \in K} \prod_{i \in K \setminus \{j\}}(\xi^{\alpha}-\xi^i)=\sum_{j \in K'\setminus \{\alpha\}} \prod_{i \in K' \setminus \{j, \alpha\}}(\xi^{\alpha}-\xi^i)$$
	But now, note that $$\sum_{j \in K'\setminus \{\alpha\}} \prod_{i \in K' \setminus \{j, \alpha\}}(\xi^{\alpha}-\xi^i)=\prod_{i \in K' \setminus \{ \alpha\}}(\xi^{\alpha}-\xi^i)\sum_{j \in K'\setminus \{\alpha\}}\frac{1}{\xi^{\alpha}-\xi^j}$$
	So, $$\frac{2p'(\xi^{\alpha})}{p(\xi^{\alpha})}=\frac{2\cdot \prod_{i \in K' \setminus \{ \alpha\}}(\xi^{\alpha}-\xi^i)\sum_{j \in K'\setminus \{\alpha\}}\frac{1}{\xi^{\alpha}-\xi^j}}{\prod_{i \in K'\setminus \{\alpha\}}(\xi^{\alpha}-\xi^i)}=$$\\
	$$=2\cdot\sum_{j \in K'\setminus \{\alpha\}}\frac{1}{\xi^{\alpha}-\xi^j}$$
	
	This completes the proof of case 2 and thus the proof of the lemma.
	\end{comment}
	\DIFaddend 
	
	\begin{lemma}[Sumcheck]\label{lem:sumcheck}
		Let $u(X,Y)$ be a bi-variate polynomial over a finite field $\F$ with degree less than $N$ in each of the variables and
		$\nroots$ be defined as the group of $N^{\text{th}}$ roots of unity $(N<<|\F|)$
		with generator $\xi \in \F$. Then $\sum_{i\in [N]}u(X,\xi^i)= Nu(X,0)$
	\end{lemma}
	
	\begin{proof}
		For some $d<N$, we write $u(X,Y)=a_0+a_1 Y+ a_2 Y^2+\cdots+ a_d Y^d$
		where each $a_i$ is a polynomial in $X$ of degree less than $N$.
		Now we write the sum:
		\begin{align*}
			\sum_{i\in [N]}u(X,\xi^i) &= N a_0+a_1(\xi^+\xi^2+\cdots+\xi^N) \\
			&\, +a_2(\xi^2+\xi^4+\cdots+\xi^{2N})+ \cdots + a_d(\xi^d+\cdots +\xi^{Nd})
		\end{align*}
		But for any $\alpha=\xi^k$ for $k<N$, $\alpha+\alpha^2+\cdots \alpha^N=0$. Thus, all terms vanish except the first term
		and hence $\sum_{i\in [N]}u(X,\xi^i)=N a_0$.
		The lemma follows by observing that $a_0=u(X, 0)$.
	\end{proof}
	
	\begin{lemma}\label{lem:zk-hat}
		Let $Z_\nroots(X)$ be the vanishing polynomial for $\nroots$, let $\widehat{Z}_K(X)$ and  $Z_K(X)$ be the vanishing polynomials for $H_{[N]\setminus K}$ and $H_K$ respectively.
		Let $\mu_1(X),\ldots,\mu_N(X)$ be Lagrange polynomials for the set $\nroots=\{\xi,\ldots,\xi^N\}$. Then:
		\begin{gather}
			\widehat{Z}_K(X) = \sum_{j\in K}\frac{Z_\nroots'(\xi^j)}{Z_K'(\xi^j)}\mu_j(X), \\
			\widehat{Z}_K'(X)= \sum_{j\in K}\frac{Z_\nroots'(\xi^j)}{Z_K'(\xi^j)}\mu_j'(X)
		\end{gather}
	\end{lemma}
	We use the following standard observation:
	\begin{fact}
		If polynomials $f,g$ of degree $<N$ agree on $N$ points, then they are equal as polynomials, that is, $f(X)=g(X)$
	\end{fact}
	
	\begin{proof}
		Note that the second equation follows from the first by linearity of derivatives, so it suffices to prove the first equation.
		Both the sides of the identity are polynomials of degree $<N$, so it suffices to show their evaluations are identical over $N$ \DIFdelbegin \DIFdel{distcint }\DIFdelend \DIFaddbegin \DIFadd{distinct }\DIFaddend points.
		In particular we show their evaluations are identical over $\nroots$.
		Consider evaluating LHS and RHS at $\xi^i$ for $i \in [N]\setminus K$.
		The left side is $0$ by definition of $\hat{Z}_K(X)$, while the right hand side is zero by the properties of Lagrange polynomials.
		Now consider evaluations LHS and RHS at $\xi^i$ for $i \in K$.
		The RHS is $\frac{Z_\nroots'(\xi^i)}{Z_K'(\xi^i)}$ by properties of Lagrange polynomials, while the
		LHS is $\prod_{j \in [N]\setminus K} (\xi^i-\xi^j)$\\
		Multiplying dividing by $\prod_{ j \in K \setminus \{i\}}(\xi^i-\xi^j)$ gives:
		$$LHS = \frac{\prod_{j \in [N] \setminus \{i\}} (\xi^i-\xi^j)}{\prod_{j \in K \setminus \{i\}}(\xi^i-\xi^j)}$$
		Which is $\frac{Z_\nroots'(\xi^i)}{Z_K'(\xi^i)}$, the same as the right hand side.
		This proves the claim.
	\end{proof}
	
	\begin{lemma}\label{lem:lamda-deriv}
		Let $\mu_1,\ldots,\mu_N$ be the lagrange polynomials for the set $\nroots=\{\xi^i:i\in [N]\}$
		of the $N^{th}$ roots of unity. Then we have:
		\begin{equation*}
			\mu_i'(\xi^j) = \begin{cases}
				\frac{(N-1)}{2\xi^{i}}  \text{ if } j=i\\
				\frac{\xi^i}{\xi^j(\xi^j-\xi^i)} \text{ otherwise }
			\end{cases}
		\end{equation*}
	\end{lemma}
	
	\begin{proof}
		Suppose first that $i \neq j$.
		We know that $\mu_i(X)= \frac{Z_H(X)}{Z_H'(\xi^i)(X-\xi^i)}$.
		Thus, by applying quotient rule(we can apply quotient rule because $\mu_i$ is defined at $\xi^j$ as $j\neq i$):
		$$\mu_i'(X) \cdot Z_H'(\xi^i)= \frac{(X-\xi^i)(N\cdot X^{N-1})-(X^N-1)}{(X-\xi^i)^2}$$
		Substituting $X$ by $\xi^j$, we get:
		$$\mu_i'(\xi^j) \cdot \frac{N}{\xi^i}= \frac{N(\xi^j-\xi^i)}{\xi^j (\xi^j-\xi^i)^2}$$
		Thus, we get:
		$$\mu_i'(\xi^j)=\frac{\xi^i}{\xi^j(\xi^j-\xi^i)}$$
		Now consider $i=j$:
		Then, $\mu_i(X)=\frac{\prod_{j \in [N] \setminus \{i\}}(X-\xi^j)}{Z_H'(\xi^i)}$\\
		Thus,  $\mu_i(X)\cdot Z_H'(\xi^i)=\prod_{j \in [N] \setminus \{i\}}(X-\xi^j)$\\
		Thus, $\mu_i'(X) \cdot \frac{N}{\xi^i}=\sum_{j \in [N] \setminus \{i\}}\prod_{k \in [N]\setminus\{i,j\}}(X-\xi^k)$\\
		Now, substituting $X$ by $\xi^i$ gives:
		\begin{align*}
			\mu_i'(\xi^i) \cdot \frac{N}{\xi^i} &= \sum_{j \in [N] \setminus \{i\}}\prod_{k \in [N]\setminus\{i,j\}}(\xi^i-\xi^k) \\
			&=\sum_{j \in [N] \setminus \{i\}}\frac{\prod_{k \in [N]\setminus\{i\}}(\xi^i-\xi^k)}{\xi^i-\xi^j} \\
			&= \prod_{k \in [N]\setminus\{i\}}(\xi^i-\xi^k)\sum_{j\in [N]\setminus \{i\}}\frac{1}{\xi^i-\xi^j} \\
			&=Z_H'(\xi^i)\sum_{j\in [N]\setminus \{i\}}\frac{1}{\xi^i-\xi^j}
		\end{align*}
		Cancelling off $Z_H'(\xi^i)=N/\xi^i$ in the above, and using Lemma ~\ref{lem:sumtoder}, we get:
		\begin{align*}
			\mu_i'(\xi^i) &= \sum_{j\in [N]\setminus \{i\}}\frac{1}{\xi^i-\xi^j} =\frac{Z_H''(\xi^i)}{2 Z_H'(\xi^i)}
			=\frac{N-1}{2\xi^i}
		\end{align*}
		
	\end{proof}
	
	\begin{lemma}\label{lem:tau}
		Let $K\subseteq \F$ be a set of cardinality $k$ and $\mathcal{X}=\{x_j:j\in K\}$ be a set
		where $x_j$ for $j\in K$ are distinct elements of $\F$. Let $Z_\mathcal{X}(X)=z_k X^k+\cdots+z_0$ denote the vanishing polynomial of $\mathcal{X}$
		and $\{\eta_j(X)\}_{j\in K}$ denote the lagrange polynomials such that $\eta_i(x_j)=\delta_{ij}$ for $i,j\in K$. Then for all $j\in K$,
		we have $\eta_j'(x_j)=F_K(x_j)/Z_\mathcal{X}'(x_j)$ where the polynomial
		$F_K(X)$ is defined as
		\[F_K(X)=\binom{k}{2}z_k X^{k-2}+\cdots+\binom{2}{2}z_2=\sum_{j=2}^k z_j\binom{j}{2}X^{j-2} \]
	\end{lemma}
	\begin{proof}
		For $j\in K$ we have $\tau_j(X)=\frac{Z_\mathcal{X}(X)}{(X-x_j)Z_\mathcal{X}'(x_j)}= \frac{1}{Z_\mathcal{X}'(x_j)}\frac{Z_\mathcal{X}(X)}{X-x_j}$ by definition of Lagrange polynomials.\\
		By doing long division of $Z_\mathcal{X}(X)$
		by $X-x_j$ we have:
		\begin{align*}
			\tau_j(X) &= \frac{1}{Z_\mathcal{X}'(x_j)}\big(z_k X^{k-1} + (x_jz_k + z_{k-1})X^{k-2} + \cdots\\ +
			(x_j^{k-1}z_k + \cdots + z_1)\big)
			&= \frac{1}{Z_\mathcal{X}'(x_j)}\sum_{p=0}^{k-1} \left(\sum_{q=p+1}^k z_q x_j^{q-p-1}\right)X^p
		\end{align*}
		
		Now, we use linearity property of derivative to differentiate:
		
		$$   \tau_j'(X) = \frac{1}{Z_\mathcal{X}'(x_j)}\sum_{p=0}^{k-1} \left(\sum_{q=p+1}^{k}z_q x_j^{q-p-1}\right)p X^{p-1} $$
		In this step we just differentiated $X^p$ to get $p X^{p-1}$
		$$ \tau_j'(X) = \frac{1}{Z_\mathcal{X}'(x_j)}\sum_{p=1}^{k-1} p  \sum_{q=p+1}^{k}z_q x_j^{q-p-1} X^{p-1} $$
		In this step, we just applied distributive property to the rightmost bracket and brought $p$ outside the sum over $q$\\
		Now we put $X=x_j$:
		
		$$\tau_j'(x_j) = \frac{1}{Z_\mathcal{X}'(x_j)} \sum_{p=1}^{k-1} p \sum_{q=p+1}^k z_q x_j^{q-2} $$
		$$= \frac{1}{Z_\mathcal{X}'(x_j)}\sum_{q=2}^k z_q x_j^{q-2} \sum_{p=1}^{q-1} p $$
		Here, we reversed the order of the sum. $p$ going from 1 to $k-1$ and $q$ going from $p+1$ to $k$ is same as $q$ going from $2$ to $k$ and $p$ going from 1 to $q-1$
		$$\tau_j'(x_j)= \frac{1}{Z_\mathcal{X}'(x_j)}\sum_{q=2}^k z_q \binom{q}{2} x_j^{q-2} $$
		Here we used that $\sum_{p=1}^{q-1}p=\frac{q(q-1)}{2}=\binom{q}{2}$
		
		But $q$ is just a dummy variable. We might as well replace $q$ by $j$
		
		Thus, $$\tau_j'(x_j)= \frac{1}{Z_\mathcal{X}'(x_j)}\sum_{j=2}^k z_j \binom{j}{2} x_j^{j-2} $$
		If we compare this to the definition of $F_K(X)$ given in the lemma statement, we get:
		$$\tau_j'(x_j)= \frac{F_K(x_j)}{Z_\mathcal{X}'(x_j)}$$
		
		
	\end{proof}
	
	\DIFdelbegin %DIFDELCMD < \begin{comment}%DIFDELCMD < 
	%DIFDELCMD < 					\subsection{Brief Proof Summary of lemma \ref{lem:sum-computation}}\label{subsec:sum-computational}
	%DIFDELCMD < 					What we want to compute is a sum over the set $K$ for all $i \in I$ or a sum over the set $I$ for all $j \in K$. Let us give a sketch for the first case.
	%DIFDELCMD < 					
	%DIFDELCMD < 					We interpolate a polynomial $p$ in such a way that its evaluations at $\nroots$ correspond to the numerators of the required sum. We take the full $\nroots$ and not subsets of it so that we can apply sumcheck easily later.
	%DIFDELCMD < 					Next we introduce some rational functions. These will be very useful to reduce the sum we need to compute to an evaluation of the rational functions at some points.
	%DIFDELCMD < 					These evaluations can be reduced to evaluations of polynomials using the simple form of $Z_{\nroots}$. By introducing some more polynomials and using formulas for sumcheck, finally, the entire task can be reduced to calculating $p(0)$ and $p'(X)$ at certain points.
	%DIFDELCMD < 					
	%DIFDELCMD < 					The polynomial $p$ turns out to be a product of polynomials: $p(X)=\widehat{Z}_K(X)\cdot q(X)$ and so the next step is to get the evaluations of $\widehat{Z}_K(X)$ and $q(X)$ at those points.
	%DIFDELCMD < 					For this, some further lemmas are used to get evaluations of $q(X)$ at degree $q$ many points and thus $q(X)$ is obtained by interpolation.
	%DIFDELCMD < 					Next $q(0)$ can be computed and thus $p(0)$ is also computed.
	%DIFDELCMD < 					$Z_K(X)$ and $q'(X)$ can also be computed easily(details in the proof)\\
	%DIFDELCMD < 					This leaves computation of $\widehat{Z}_K(X)$ at those points as the main task. For this another lemma is needed related to derivatives of Lagrange polynomials.
	%DIFDELCMD < 					Eventually the task reduces to needing to compute something which can be computed efficiently using another lemma.\\
	%DIFDELCMD < 					Finally, every single step involves a reduction which can be done in the number of operations allowed in the statement of the lemma. So, we get what we want.\\\\
	%DIFDELCMD < 					For the second case, the proof is very similar, except for some interchanging of $I$ and $K$, and interchanging of $i$ and $j$. \\
	%DIFDELCMD < 					This completes a brief summary of the proof.
	%DIFDELCMD < 				%DIFDELCMD < \end{comment}
	%DIFDELCMD < 				%%%
	\DIFdelend \DIFaddbegin \begin{comment}
	\subsection{Brief Proof Summary of lemma \ref{lem:sum-computation}}\label{subsec:sum-computational}
	What we want to compute is a sum over the set $K$ for all $i \in I$ or a sum over the set $I$ for all $j \in K$. Let us give a sketch for the first case.
	
	We interpolate a polynomial $p$ in such a way that its evaluations at $\nroots$ correspond to the numerators of the required sum. We take the full $\nroots$ and not subsets of it so that we can apply sumcheck easily later.
	Next we introduce some rational functions. These will be very useful to reduce the sum we need to compute to an evaluation of the rational functions at some points.
	These evaluations can be reduced to evaluations of polynomials using the simple form of $Z_{\nroots}$. By introducing some more polynomials and using formulas for sumcheck, finally, the entire task can be reduced to calculating $p(0)$ and $p'(X)$ at certain points.
	
	The polynomial $p$ turns out to be a product of polynomials: $p(X)=\widehat{Z}_K(X)\cdot q(X)$ and so the next step is to get the evaluations of $\widehat{Z}_K(X)$ and $q(X)$ at those points.
	For this, some further lemmas are used to get evaluations of $q(X)$ at degree $q$ many points and thus $q(X)$ is obtained by interpolation.
	Next $q(0)$ can be computed and thus $p(0)$ is also computed.
	$Z_K(X)$ and $q'(X)$ can also be computed easily(details in the proof)\\
	This leaves computation of $\widehat{Z}_K(X)$ at those points as the main task. For this another lemma is needed related to derivatives of Lagrange polynomials.
	Eventually the task reduces to needing to compute something which can be computed efficiently using another lemma.\\
	Finally, every single step involves a reduction which can be done in the number of operations allowed in the statement of the lemma. So, we get what we want.\\\\
	For the second case, the proof is very similar, except for some interchanging of $I$ and $K$, and interchanging of $i$ and $j$. \\
	This completes a brief summary of the proof.
	\end{comment}
	\DIFaddend 
	
	\subsection{Proof of lemma ~\ref{lem:sum-computation}}\label{subsec:sum-computation}
	
\begin{proof}
		
		We give the proof for \DIFdelbegin \DIFdel{$e_i$ }\DIFdelend \DIFaddbegin \DIFadd{computation of $a_i$ }\DIFaddend for all $i \in \setind$ in full detail and briefly mention the modifications needed to compute $b_j$ for all $j \in K$. \\
		
		\begin{equation}\label{eq:ei}
			\DIFdelbegin \DIFdel{e}\DIFdelend \DIFaddbegin \DIFadd{a}\DIFaddend _i = \sum_{j\in K\setminus \{i\}} \DIFdelbegin \DIFdel{\frac{\Delta t_j}{\xi^i - \xi^j}
			}\DIFdelend \DIFaddbegin \DIFadd{\frac{d_j}{\xi^i - \xi^j}
			}\DIFaddend \end{equation}
		\DIFdelbegin \DIFdel{We need the above for all $i \in \setind$. }\DIFdelend Recall that we assume $\setind \subset K$ in this case.
		To compute \DIFdelbegin \DIFdel{$e_i$}\DIFdelend \DIFaddbegin \DIFadd{$a_i$}\DIFaddend , we first define a polynomial $p(X)$ of degree at most $N-1$ such that
		\DIFdelbegin \DIFdel{$p(\xi^j)=\Delta t_j$ }\DIFdelend \DIFaddbegin \DIFadd{$p(\xi^j)=d_j$ }\DIFaddend for $j\in K$ and $p(\xi^j)=0$ for $j \in [N]\setminus K$.
		\DIFdelbegin \DIFdel{Therefore}\DIFdelend \DIFaddbegin \DIFadd{Then}\DIFaddend , the vanishing polynomial of $H_{[N]\setminus K}$ divides $p(X)$ and
		there exists a polynomial $q(X)$ of degree at most $|K|-1$ such that:
		\begin{equation}\label{eq:px}
			p(X) = \hat{Z}_K(X)\cdot q(X)
		\end{equation}
		where $\hat{Z}_K(X)=\prod_{i\in [N]\setminus K}(X-\xi^i)$ is the vanishing polynomial of $H_{[N]\setminus K}$\\
		
		Now \DIFaddbegin \DIFadd{we }\DIFaddend introduce the rational functions:
		\begin{gather}\label{eq:fgr}
			f_i(X) = \sum_{j\in [N]\setminus \{i\}}\frac{p(\xi^j)}{X-\xi^j}, i\in \setind \\
			g_i(X) = \sum_{j\in [N]\setminus \{i\}}\frac{p(X)}{X-\xi^j}, i\in \setind \\
			r_i(X) = \sum_{j\in [N]\setminus \{i\}}\frac{p(X) - p(\xi^j)}{X-\xi^j}, i\in \setind
		\end{gather}
		Note that\DIFdelbegin \DIFdel{$f_i(\xi^i)=\sum_{j\in [N]\setminus \{i\}}\frac{p(\xi^j)}{\xi^i-\xi^j}$.
			But}\DIFdelend , by the way $p(X)$ is defined, \DIFdelbegin \DIFdel{we have that $f_i(\xi^i)=\sum_{j\in K\setminus \{i\}}\frac{\Delta t_j}{\xi^i-\xi^j}=e_i$}%DIFDELCMD < \\
		%DIFDELCMD < 					%%%
		\textbf{\DIFdel{Thus, we need $f_i(\xi^i)$ for all $i \in I$.}}
		%DIFAUXCMD
		\DIFdelend \DIFaddbegin \DIFadd{$f_i(\xi^i)=a_i \,\forall i$
		}\DIFaddend 
		
		\DIFdelbegin \DIFdel{Note that }\DIFdelend \DIFaddbegin \textbf{\DIFadd{Thus, it suffices to compute $f_i(\xi^i)$ for all $i \in I$.}}
		
		\DIFadd{Since }\DIFaddend $f_i(X) = g_i(X) - r_i(X)$ for $i\in I$\DIFdelbegin \DIFdel{.
			Thus}\DIFdelend , \DIFdelbegin \DIFdel{$e_i=g_i(\xi^i)-r_i(\xi^i)$}\DIFdelend \DIFaddbegin \DIFadd{we have that
			$a_i=g_i(\xi^i)-r_i(\xi^i)$}\DIFaddend .
		Thus, we need to compute $g_i(\xi^i)$ and $r_i(\xi^i)$ for all $i \in I \subset K$.
		\begin{align*}
			g_i(\xi^i) &= p(\xi^i)\sum_{j\in [N]\setminus \{i\}} \frac{1}{\xi^i-\xi^j} \\
			&= \frac{p(\xi^i)Z_{\nroots}''(\xi^i)}{2 Z_{\nroots}'(\xi^i)} \text{From lemma \ref{lem:sumtoder}} \\
			&= \DIFdelbegin \DIFdel{\frac{(N-1)\Delta t_i}{2\xi^i}
			}\DIFdelend \DIFaddbegin \DIFadd{\frac{(N-1)d_i}{2\xi^i}
			}\DIFaddend \end{align*}
		
		We used the fact that $Z_H(X)=X^N-1$ and that \DIFdelbegin \DIFdel{$p(\xi^i)=\Delta t_i$}\DIFdelend \DIFaddbegin \DIFadd{$p(\xi^i)=d_i$}\DIFaddend .
		\DIFdelbegin \DIFdel{Thus, in }\DIFdelend \DIFaddbegin \DIFadd{In }\DIFaddend $O(|\setind|)$ operations, \DIFdelbegin \DIFdel{we get }\DIFdelend $g_i(\xi^i)$ for all $i$ \DIFaddbegin \DIFadd{can be obtained}\DIFaddend .\DIFdelbegin \DIFdel{If we compute $r_i(\xi^i)$ for all $i \in \setind$, then computing $f_i(\xi^i)$ takes only
			$O(|\setind|)$ operations. Therefore, it suffices to compute $r_i(\xi^i)$ for all $i \in I$.
		}\DIFdelend \DIFaddbegin \\
		\textbf{\DIFadd{Therefore, it suffices to compute $r_i(\xi^i)$ for all $i \in I$}}\\
		\DIFaddend We write $r_i(X)$ as:
		\begin{align}\label{eq:rx}
			r_i(X) &= \sum_{j\in [N]}\frac{p(X)-p(\xi^j)}{X-\xi^j} - \frac{p(X)-p(\xi^i)}{X-\xi^i} \nonumber \\
			\DIFdelbegin \intertext{\DIFdel{Now, defining the bivariate polynomial $u(X,Y)=(p(X) - p(Y))/(X-Y)$ gives:}}
			%DIFAUXCMD
			\DIFdelend \DIFaddbegin \intertext{\DIFadd{Now, defining the bi-variate polynomial $u(X,Y)=(p(X) - p(Y))/(X-Y)$ gives:}}
			\DIFaddend &r_i(X)= \sum_{j\in [N]}u(X,\xi^j) - u(X,\xi^i) \nonumber
		\end{align}
		\DIFdelbegin \DIFdel{Next, we define }\DIFdelend \DIFaddbegin \DIFadd{Defining }\DIFaddend $r(X)=\sum_{j\in [N]}u(X,\xi^j)$\DIFdelbegin \DIFdel{.
			Thus}\DIFdelend , we have:
		\DIFaddbegin 
		
		\DIFaddend $$r_i(X)=r(X)-u(X, \xi^i)$$
		or
		$$r_i(\xi^i)=r(\xi^i) - u(\xi^i,\xi^i)$$
		\DIFdelbegin \DIFdel{But, by }\DIFdelend \DIFaddbegin \DIFadd{Recalling the }\DIFaddend definition \DIFdelbegin \DIFdel{and properties }\DIFdelend of formal derivative \DIFdelbegin \DIFdel{, we have the following polynomial identity
		}\DIFdelend \DIFaddbegin \DIFadd{gives:}\\
		
		\DIFaddend $$ r_i(\xi^i)=r(\xi^i) - p'(\xi^i) $$
		
		
		\DIFdelbegin \textbf{\DIFdel{Thus, $r_i(\xi^i)$ for all $i \in \setind$ can be computed by evaluating polynomials $r(X)$ and $p'(X)$ at the set $H_{\setind}$.}}
		%DIFAUXCMD
		\DIFdelend Now, from Lemma \ref{lem:sumcheck}, we have that \\ \DIFdelbegin \DIFdel{$r(X)=N u(X,0)=N \frac{(p(X) - p(0))}{X}$ and thus }\DIFdelend \DIFaddbegin \DIFadd{$r(X)=N u(X,0)$ }\\ 
		\DIFadd{$=N \frac{(p(X) - p(0))}{X}$ which gives }\DIFaddend $r(\xi^i)$ for $i\in \setind$ \DIFdelbegin \DIFdel{is given by}\DIFdelend \DIFaddbegin \DIFadd{as}\DIFaddend :
		$$r(\xi^i)=N \DIFdelbegin \DIFdel{\frac{(\Delta t_i - p(0))}{\xi^i}}\DIFdelend \DIFaddbegin \DIFadd{\frac{(d_i - p(0))}{\xi^i}}\DIFaddend $$
		\DIFdelbegin \DIFdel{If we get $p(0)$, then we can get $r(\xi^i)$ for all $i$ in $O(m)$ operations.
		}\DIFdelend \DIFaddbegin 
		
		\DIFaddend \textbf{Thus, it suffices to compute $p(0)$ and $p'(X)$ at the set $\{\xi^i:i \in I\}$}\\\\
		Let $p(X)=\widehat{Z}_K(X)\cdot q(X)$
		as in Equation \eqref{eq:px} and let $Z_K(X)$ be the vanishing polynomial for $H_K$.
		Note that $Z_K(X) \cdot \hat{Z}_K(X)=X^N-1$ \DIFdelbegin \DIFdel{(by definition).
			Notice }\DIFdelend \DIFaddbegin \DIFadd{and also }\DIFaddend that
		$Z_\nroots'(\xi^j)= \frac{N}{\xi^j} $.
		\DIFdelbegin \DIFdel{Moreover}\DIFdelend \DIFaddbegin \DIFadd{Also}\DIFaddend , $Z_K(X)$ can be computed by fast multiplication in $O(|K|\log^2 |K|)$ operations and then it can be differentiated monomial by monomial to get $Z_K'(X)$.\\
		\DIFdelbegin \DIFdel{Then, we can do fast evaluation to get $Z_K'(X)$ at $\{\xi^j:j\in K\}$ in $O(|K|\log^2 |K|)$ operations}\DIFdelend \DIFaddbegin \textbf{\DIFadd{Then, fast evaluation gives $Z_K'(X)$ at $\{\xi^j:j\in K\}$ in $O(|K|\log^2 |K|)$ operations}}\DIFaddend \\
		Recall \DIFdelbegin \DIFdel{that }\DIFdelend \DIFaddbegin \DIFadd{from }\DIFaddend Lemma \ref{lem:zk-hat} \DIFdelbegin \DIFdel{tells us }\DIFdelend that $\hat{Z}_K(\xi^j)=\frac{Z_{\nroots}'(\xi^j)}{Z_K'(\xi^j)}$ and
		thus \DIFdelbegin \DIFdel{we can compute }\DIFdelend $\hat{Z}_K(\xi^j)=\frac{\frac{N}{\xi^j}}{Z_K'(\xi^j)}$ for all $j \in K$.
		\DIFdelbegin \DIFdel{Moreover, recall }\DIFdelend \DIFaddbegin \DIFadd{Further recalling }\DIFaddend from \eqref{eq:px} that \DIFdelbegin \DIFdel{$q(\xi^j)=\frac{p(\xi^j)}{\hat{Z}_K(\xi^j)}=\frac{\Delta t_j}{\widehat{Z}_K(\xi^j)}$.
			So, we can }\DIFdelend \DIFaddbegin \DIFadd{$q(\xi^j)=\frac{p(\xi^j)}{\hat{Z}_K(\xi^j)}=\frac{d_j}{\widehat{Z}_K(\xi^j)}$ enables us to }\DIFaddend compute
		$q(\xi^j)$ for all $j \in K$.
		As degree of $q(X)$ is strictly less than $|K|$ we can interpolate to get $q(X)$ in $O(|K|\log^2 |K|)$ field operations.
		\DIFdelbegin \DIFdel{Again by }\DIFdelend \DIFaddbegin \DIFadd{By }\DIFaddend differentiating monomial by monomial, we get $q'(X)$ and by fast evaluation we \DIFdelbegin \DIFdel{can }\DIFdelend get evaluations of $q'(X)$ at $\xi^i$ for all $i \in I$, again in $O(|K|\log^2 |K|)$ field operations.
		
		\DIFdelbegin \DIFdel{Now we can compute $p(0)$:
		}\DIFdelend Since $p(0)=q(0)\cdot \hat{Z}_K(0)$, and
		$\hat{Z}_K(0)=\frac{Z_H(0)}{Z_K(0)}=\frac{-1}{Z_K(0)}$, this enables us to compute $p(0)$ \DIFdelbegin \DIFdel{,
		}\DIFdelend since \DIFdelbegin \DIFdel{we know $Z_K(X)$ }\DIFdelend \DIFaddbegin \DIFadd{$q(0)$ }\DIFaddend and $Z_K(0)$ \DIFdelbegin \DIFdel{is }\DIFdelend \DIFaddbegin \DIFadd{are }\DIFaddend just \DIFdelbegin \DIFdel{its constant term.
			Similarly, we know $q(X)$ so can compute $q(0)$ as it is just }\DIFdelend the constant \DIFdelbegin \DIFdel{term}\DIFdelend \DIFaddbegin \DIFadd{terms of known polynomials $q(X)$ and $Z_K(X)$}\DIFaddend .
		\DIFdelbegin \DIFdel{Thus, we can compute $p(0)$.
		}\DIFdelend 
		
		\textbf{Now it remains to compute $p'(X)$ at $\{\xi^i:i \in I\}=H_I$.}
		Using the  product rule for derivatives we have: $p'(X)=q(X) \hat{Z}_K'(X)+q'(X) \hat{Z}_K(X)$.
		
		Thus, to get $p'(X)$ at $H_I$, we need $q, q', \hat{Z}_K$ and $\hat{Z}_K'$ at $H_I$. \DIFdelbegin \DIFdel{If we get these}\DIFdelend \DIFaddbegin \DIFadd{Out of which}\DIFaddend , \DIFdelbegin \DIFdel{then in $O(|I|)$ operations }\DIFdelend we \DIFdelbegin \DIFdel{would get $p'(X)$ at $H_I$.
			So far we }\DIFdelend \DIFaddbegin \DIFadd{already }\DIFaddend have \DIFdelbegin \DIFdel{computed }\DIFdelend \DIFaddbegin \DIFadd{the evaluations for }\DIFaddend $q, q', \hat{Z}_K$\DIFdelbegin \DIFdel{at $H_I$}\DIFdelend .
		\DIFdelbegin \DIFdel{There's only one last remaining item which
			is $\hat{Z}_K'(X)$ at the set $H_I$.
		}\DIFdelend 
		
		\DIFdelbegin \textbf{\DIFdel{So, it suffices to compute $\hat{Z}_K'(X)$ at $H_I$}%DIFDELCMD < \\%%%
		}
		%DIFAUXCMD
		\DIFdelend \DIFaddbegin \textbf{\DIFadd{Thus, it suffices to obtain
				$\hat{Z}_K'(X)$ at the set $H_I$}}
		\DIFaddend 
		
		\DIFdelbegin \DIFdel{For this, we recall }\DIFdelend \DIFaddbegin \DIFadd{Recall }\DIFaddend the second equation of lemma \ref{lem:zk-hat} and replace $X$ by $\xi^i$ to get:
		$$  \widehat{Z}_K'(\xi^i) = \sum_{j\in K\setminus \{i\}} \frac{Z_\nroots'(\xi^j)}{Z_K'(\xi^j)}\mu_j'(\xi^i) + \frac{Z_\nroots'(\xi^i)}{Z_K'(\xi^i)}\mu_i'(\xi^i)$$
		\DIFdelbegin \DIFdel{We obtained the above by splitting the sum $j \in K$ into two sums, one with $j \in K \setminus \{i\}$ and other with $j=i$.
		}\DIFdelend \DIFaddbegin 
		
		\DIFaddend Using lemma \ref{lem:lamda-deriv}, this becomes:
		$$ \widehat{Z}_K'(\xi^i)= N\xi^{-i}\sum_{j\in K\setminus \{i\}}\frac{1}{Z_K'(\xi^j)(\xi^i-\xi^j)} + \frac{N(N-1)}{2\xi^{2i}Z_K'(\xi^i)}.$$
		\DIFdelbegin \DIFdel{So, if we can compute $\sum_{j\in K\setminus \{i\}}\frac{1}{Z_K'(\xi^j)(\xi^i-\xi^j)}$ for all $i \in I$, then in $O(|I|)$ operations, we will get $\hat{Z}_K'(\xi^i)$ for all $i \in I$ which is what we want.
		}\DIFdelend 
		
		
		Let $$\varphi_i=\sum_{j\in K\setminus \{i\}}\frac{1}{Z_K'(\xi^j)(\xi^i-\xi^j)}$$
		Thus,\textbf{ it suffices to compute $\varphi_i$ for all $i \in I$}\\
		
		Define:
		$$ \Phi_i(X) = \sum_{j\in K\setminus \{i\}} \frac{1}{Z_K'(\xi^j)(X-\xi^j)} $$
		at $X=\xi^i$. \DIFdelbegin \DIFdel{Then it }\DIFdelend \DIFaddbegin \DIFadd{It }\DIFaddend is clear that $\varphi_i=\Phi_i(\xi^i)$.
		Let $\{\eta_i(X)\}_{i\in K}$ be the Lagrange polynomials for the set $\{\xi^i:i\in K\}= H_K$.
		Then, \DIFdelbegin \DIFdel{$\eta_j(X)=\frac{Z_K(X)}{Z_K'(\xi^j)(X-\xi^j)}$.
			Thus, }\DIFdelend $\frac{\eta_j(X)}{Z_K(X)}=\frac{1}{Z_K'(\xi^j)(X-\xi^j)}$.\\
		Thus, $\Phi_i(X)$ can be rewritten as:
		\begin{align*}
			\Phi_i(X)&=\sum_{j\in K\setminus \{i\}} \frac{\eta_j(X)}{Z_K(X)}\\
			&= \sum_{j\in K\setminus \{i\}} \frac{\eta_j(X)/(X-\xi^i)}{Z_K(X)/(X-\xi^i)}
		\end{align*}
		
		\DIFdelbegin \DIFdel{We need to compute $\varphi_i=\Phi_i(\xi^i)$ for all $i \in I$. Thus, putting }\DIFdelend \DIFaddbegin \DIFadd{Putting }\DIFaddend $X=\xi^i$ in the above, we have:
		
		$$\varphi_i = \Phi_i(\xi^i) = \left(\sum_{j\in K\setminus \{i\}}\frac{\eta_j(X)/(X-\xi^i)}{Z_K(X)/(X-\xi^i)}\right)(\xi^i)$$
		This can be simplified as:
		\begin{align*}\varphi_i = \Phi_i(\xi^i) &= \left(\sum_{j\in K\setminus \{i\}}\frac{\eta_j(X)/(X-\xi^i)}{\prod_{k \in K \setminus \{i\}}(X-\xi^k)}\right)(\xi^i)\\
			&= \sum_{j\in K\setminus \{i\}}\left(\frac{\eta_j(X)/(X-\xi^i)}{\prod_{k \in K \setminus \{i\}}(X-\xi^k)}\right)(\xi^i)\\
			&= \sum_{j\in K\setminus \{i\}}\frac{\left(\eta_j(X)/(X-\xi^i)\right)(\xi^i)}{\left(\prod_{k \in K \setminus \{i\}}(X-\xi^k)\right)(\xi^i)}\\
			&= \frac{1}{Z_K'(\xi^i)}\sum_{j\in K\setminus \{i\}}\left(\eta_j(X)/(X-\xi^i)\right)(\xi^i)
		\end{align*}
		
		Now, \begin{align*}
			\left(\eta_j(X)/(X-\xi^i)\right)(\xi^i)&=\left(\frac{Z_K(X)}{(X-\xi^j)(X-\xi^i)Z_K'(\xi^j)}\right)(\xi^i)\\
			&= \frac{Z_K'(\xi^i)}{(\xi^i-\xi^j)Z_K'(\xi^j)}
		\end{align*}
		
		But if we compute, $\eta_j'(\xi^i)$ by quotient rule, we get:
		$$\eta_j'(\xi^i)=\frac{Z_K'(\xi^i)}{(\xi^i-\xi^j)Z_K'(\xi^j)}$$
		Thus,
		$$\left(\eta_j(X)/(X-\xi^i)\right)(\xi^i)=\eta_j'(\xi^i) $$
		\DIFdelbegin \DIFdel{As both are separately equal to $\frac{Z_K'(\xi^i)}{(\xi^i-\xi^j)Z_K'(\xi^j)}$.}%DIFDELCMD < \\
		%DIFDELCMD < 					%%%
		\DIFdelend \DIFaddbegin 
		
		\DIFaddend Thus, we get:
		
		
		$$\varphi_i =\Phi_i(\xi^i) =  \frac{1}{Z_K'(\xi^i)}\sum_{j\in K\setminus \{i\}}\eta_j'(\xi^i)$$
		But, $$\sum_{j\in K\setminus \{i\}}\eta_j'(\xi^i)= \sum_{j\in K}\eta_j'(\xi^i)-\eta_i'(\xi^i)=-\eta_i'(\xi^i)$$
		using that the sum of Lagrange polynomials is \DIFdelbegin \DIFdel{$1$ and thus }\DIFdelend the \DIFdelbegin \DIFdel{sum of the derivatives of the Lagrange polynomials is $0$
		}\DIFdelend \DIFaddbegin \DIFadd{constant $1$
		}\DIFaddend 
		
		Thus,
		$$\varphi_i = \DIFdelbegin \DIFdel{\Phi_i(\xi^i) = \frac{-\eta_i'(\xi^i)}{Z_K'(\xi^i)}}\DIFdelend \DIFaddbegin \DIFadd{\frac{-\eta_i'(\xi^i)}{Z_K'(\xi^i)}}\DIFaddend $$
		
		\DIFdelbegin \DIFdel{If we get $\eta_i'(\xi^i)$ for all $i\in \setind$, then as we already have $Z_K'(\xi^i)$ for all $i \in I$, we get $\varphi_i$ for all $i \in I$ in $O(|I|)$ computations which is what we need. }\DIFdelend \textbf{Thus, it suffices to efficiently compute $\eta_i'(\xi^i)$ for $i\in \setind$}
		
		For this recall Lemma \ref{lem:tau} and observe that in our setting $\mathcal{X}$ is just $H_K$, $x_j$ are $\xi^j$ and $Z_\mathcal{X}(X)$ which is the vanishing polynomial of \DIFdelbegin \DIFdel{$H_K$ }\DIFdelend \DIFaddbegin \DIFadd{$\mathcal{X}=H_K$ }\DIFaddend is just $Z_K(X)$. The $\eta_i$ in the lemma are the Lagrange polynomials corresponding to $\mathcal{X}=H_K$ so they are same as the $\eta_i$ in our setting\\\\
		Thus, we get:
		$$\eta_i'(\xi^i)=\frac{F_K(\xi^i)}{Z_K'(\xi^i)}$$
		
		
		
		\DIFdelbegin \DIFdel{Since we already have $Z_K'(\xi^i)$ for all $i \in I$,
			it suffices to compute $F_K(\xi^i)$ for all $i \in I$ to compute $\eta_i'(\xi^i)$ , $i \in I$.
			%DIF <  in $O(|I|)$ operations which is what we want.
		}\DIFdelend \textbf{Thus, it suffices to compute $F_K(\xi^i)$ for all $i \in I$}\\
		
		From the definition $F_K$ (Lemma ~\ref{lem:tau}), it can be computed once we know
		coefficients  $z_0, z_1, z_2, \cdots, z_k$ of the polynomial $Z_K(X)$.
		But, we know the polynomial $Z_K$ (via fast multiplication) hence we know $z_0, z_1, \cdots, z_{k}$. Thus, we \DIFdelbegin \DIFdel{get the coefficients of the polynomial }\DIFdelend \DIFaddbegin \DIFadd{can easily compute }\DIFaddend $F_K(X)$\DIFdelbegin \DIFdel{, hence the polynomial $F_K(X)$}\DIFdelend .
		\DIFaddbegin 
		
		\DIFaddend Now, by fast evaluation, we can get $F_K(\xi^i)$ for all $i \in I$ \DIFaddbegin \DIFadd{as required}\DIFaddend . This will take $O(|K| \log^2|K|)$ field operations.
		\DIFdelbegin \textbf{\DIFdel{Thus, we have computed $F_K(\xi^i)$ for all $i \in I$}}%DIFAUXCMD
		%DIFDELCMD < \\
		%DIFDELCMD < 					%%%
		\DIFdelend 
		
		
		This completes the proof for \DIFdelbegin \DIFdel{$e_i$ }\DIFdelend \DIFaddbegin \DIFadd{$a_i$ }\DIFaddend (Note that in the proof we throughout used that $|\setind| \leq |K|$ so that the various $O(|\setind|)$ operations do not contribute to the final complexity).\\
		For the case of $b_j$, notice first that it is very similar to the case of \DIFdelbegin \DIFdel{$e_i$ }\DIFdelend \DIFaddbegin \DIFadd{$a_i$ }\DIFaddend except that the roles of $\setind$ and $K$ are reversed and $i$ and $j$ are reversed.
		
		First of all define $c_i$ for $i$ outside $\setind$ to be 0. For $i \in \setind$, \DIFdelbegin \DIFdel{we already know what }\DIFdelend $c_i$ is \DIFdelbegin \DIFdel{(can be computed }\DIFdelend \DIFaddbegin \DIFadd{given }\DIFaddend by \DIFdelbegin \DIFdel{fast evaluation)}\DIFdelend \DIFaddbegin \DIFadd{the specific lookup argument and so is known}\DIFaddend .
		Next, define polynomial $p$ of degree atmost $N-1$ such that $p(\xi^i)=c_i$ for all $i \in [N]$.
		From here on, we proceed exactly as we did for the \DIFdelbegin \DIFdel{$e_i$ }\DIFdelend \DIFaddbegin \DIFadd{$a_i$ }\DIFaddend case, replacing every \DIFaddbegin \DIFadd{instance of }\DIFaddend $\setind$ with $K$, every $K$ with $\setind$, every $i$ with $j$ and every $j$ with $i$.
		All the required lemmas are also modified \DIFaddbegin \DIFadd{appropriately }\DIFaddend in this way.
		
		We reach till
		$$\Phi_j(X)=\sum_{i\in I\setminus \{j\}} \frac{\eta_i(X)}{Z_I(X)}$$
		and we need to compute $\varphi_j=\Phi_j(\xi^j)$ for all $j \in K$.\\
		
		Here, we deviate: For $j \in K\setminus I$, we can very easily compute $\varphi_j=\Phi_j(\xi^j)$ as
		\begin{align*}
			\Phi_j(\xi^j)&=\sum_{i\in I\setminus \{j\}} \frac{\eta_i(\xi^j)}{Z_I(\xi^j)}\\
			&=\frac{1}{Z_I(\xi^j)} \sum_{i\in I}\eta_i(\xi^j)\\
			&=\frac{1}{Z_I(\xi^j)}
		\end{align*}
		
		\DIFdelbegin \DIFdel{Using that the sum of all Lagrange polynomials is $1$. }\DIFdelend \textbf{So, it suffices to compute $\varphi_j$ for all $j \in \setind$}
		
		Now, again continue exactly as in the \DIFdelbegin \DIFdel{$e_i$ }\DIFdelend \DIFaddbegin \DIFadd{$a_i$ }\DIFaddend case until we reach:
		$$\varphi_j =\Phi_j(\xi^j) = \frac{-\eta_j'(\xi^j)}{Z_I'(\xi^j)}$$
			for all $j \in \setind$. But $Z_I'(\xi^j)$ we would have already computed for all $j \in \setind$ (fast multiplication and fast evaluation). %DIFDELCMD < \\
		%DIFDELCMD < 					%%%
		\textbf{\DIFdel{So, it suffices to compute $-\eta_j'(\xi^j)$ for all $j \in \setind$}%DIFDELCMD < \\%%%
		}
		%DIFAUXCMD
		\DIFdel{But we computed $\eta_i'(\xi^i)$ for $i\in \setind$ during the computation of $e_i$. Just reuse whatever we got!
			This completes the computation for $b_j$ for all $j \in K$.
			Thus, this finishes the proof of lemma \ref{lem:sum-computation}.
		}%DIFDELCMD < 
	\end{proof}
	
	%DIF > 										\input{app-CIlookup-caulkplus}
	
	
	\section{Committed Index Lookup from Caulk+}\label{app:committed-index-lookup}
	
	In this section, we present an explicit (non-black-box) adaptation of~\mbox{%DIFAUXCMD
		\cite{EPRINT:PosKat22} }\hskip0pt%DIFAUXCMD
	to obtain a committed index lookup, which again incurs costs comparable to a single instance of the underlying sub-vector protocol. Let $m,N\in \N$ be fixed parameters with $m < N$ and let $\srs$ denote a $\kzg$ setup of degree $d\geq N$ over bi-linear group $(\F$, $\Gone$, $\Gtwo$, $\GT$, $e$, $\gone{1}$, $\gtwo{1}$, $[1]_t)$. Recall that the committed index
	lookup relation in Definition ~\ref{defn:comm-index-lookup} involves the prover showing knowledge of vectors $\vecT\in \F^N$,
	$\vec{a}\in \F^m$ and $\vec{v}\in \F^m$ corresponding to public commitments $c_T, c_a$ and $c_v$ such that they
	satisfy $v_i = \vecT[\,a_i\,] = T_{a_i}$.
	We present a polynomial protocol for the same, which is an adaptation of the lookup protocol from Caulk+ ~\mbox{%DIFAUXCMD
		\cite{EPRINT:PosKat22}}\hskip0pt%DIFAUXCMD
	.
	However, here we do not aim for zero-knowledge. Let $T(X)=\enc{t}{\setN}$, $a(X)=\enc{a}{\setV}$ and
	$v(X)=\enc{v}{\setV}$ denote the polynomials encoding the vectors $\vec{t},\vec{a}$ and $\vec{v}$ respectively.
	The verifier knows commitments to these polynomials at the start of the protocol.
	Now $v_i = \vec{t}[a_i]$ for $i\in [m]$ is equivalent to $v(\nu^i) = T(\xi^{a(\nu^i)})$ for $i\in [m]$. To
	obtain a polynomial protocol, the prover interpolates a polynomial $h(X)=\sum_{i=1}^m \xi^{a_i}\tau_i(X)$, which satisfies
	$h(\nu^i)=\xi^{a(\nu^i)}$. To show that polynomial $h$ correctly ``exponentiates'' evaluations of $a(X)$, we consider the
	inverting polynomial $\ell(X)=\sum_{i=1}^N i\mu_i(X)$ which behaves like ``log'' over $\setN$ by evaluating to $i$ on $\xi^i$. Now, we see
	that all constraints are encoded as polynomial identities below:
\begin{equation}
	\DIFaddFL{\begin{aligned}
			\ell(h(X)) &= a(X) \quad \text{mod } Z_{\setV}\\  % & \quad\text{ encodes } & \quad \forall i\in [m]:& h(\nu^i) = \xi^{a(\nu^i)}  \\
			T(h(X)) &= v(X) \quad \text{mod } Z_\setV \\ % \quad\text{ encodes } & \quad \forall i\in [m]:& v_i = \vec{t}[a_i] \\
			Z_{\setN}(h(X)) &= 0 \qquad \text{mod } Z_\setV  %&\quad\text{ encodes } & \quad \forall i \in [m]:& h(\nu^i)\in \setN
		\end{aligned}
		\label{eq:comm-index-lookup}
}\end{equation}
\DIFaddFL{The last polynomial identity ensures that evaluations of $h$ on $\setV$ lie in $\setN$ (the set of roots of $\vpolyN$). Since the polynomial $\ell$ is one-one
	over $\setN$, the first equation implies $h(\nu^i)=\xi^{a_i}$ for all $i\in [m]$. The desired relation $v_i=T_{a_i}$ now follows from the second identity.
	The above formulation involves composition with polynomials $\ell,T$ and $\vpolyN$ of degree $O(N)$, which is inefficient. We use the trick from
	\mbox{%DIFAUXCMD
		\cite{EPRINT:PosKat22}}\hskip0pt%DIFAUXCMD
	, where we work with low-degree restrictions of $O(N)$-degree polynomials such as $T, \ell$ over the set
	$\setN_I=\{{h(\nu^i)}: i\in [m]\}=\{\xi^{a_i}:i\in I\}\subseteq \setN$, where $I=\{a_i: i\in [m]\}$. The prover
	commits to the polynomials $Z_I(X)=\prod_{i\in I}(X-\xi^i)$, $h(X)$ and low degree ($<m$) restrictions $T_I, \ell_I$ of $T$ and $\ell$
	on the $\setN_I$ respectively. The polynomial protocol then checks the following:
}\begin{equation}
	\begin{alignedat}{3}
		\DIFaddFL{T(X) - T_I(X) }&\DIFaddFL{= 0 \quad \text{ mod } Z_I ,}&\DIFaddFL{\quad}& \DIFaddFL{T_I(h(X)) }&\DIFaddFL{= v(X) \quad \text{ mod } Z_{\setV} }\\
		\DIFaddFL{\ell(X) - \ell_I(X) }&\DIFaddFL{= 0 \quad \text{ mod } Z_I ,}&\DIFaddFL{\quad}& \DIFaddFL{\ell_I(h(X)) }&\DIFaddFL{= a(X) \quad \text{ mod } Z_{\setV} }\\
		\DIFaddFL{Z_{\setN}(X) }&\DIFaddFL{= 0 \quad \text{ mod } Z_I ,}&\DIFaddFL{\quad}& \DIFaddFL{Z_I(h(X)) }&\DIFaddFL{= 0 \quad \text{ mod } Z_{\setV}
	}\end{alignedat}
	\DIFaddFL{\label{eq:poly-comm-index}
}\end{equation}
\DIFaddFL{It must be noted that the above identities imply the earlier polynomial identities in \eqref{eq:comm-index-lookup}. This is so because evaluations
	of $h$ on $\setV$ are roots of $Z_I$, which implies $T_I(h(\nu^i))=T(h(\nu^i))$, $\ell_I(h(\nu^i))=\ell(h(\nu^i))$ and $\vpolyN(h(\nu^i))=0$ over $\setV$.
	While the identities on the left still involve a degree $N$ polynomial, we can use the $\srs$ to check the polynomial
	identity at the point $\tau$ encoded in the $\srs$. For example, we can evaluate the encoded quotient $\gtwo{Q(X)} =$
	$\gtwo{\frac{(T(X) - T_I(X)}{Z_I(X)}}$ using the relation:
}\begin{equation*}
	\DIFaddFL{\gtwo{\frac{T(X)-T_I(X)}{Z_I(X)}} = \sum_{i\in I}\frac{1}{Z_I'(\xi^i)}\gtwo{\frac{T(X)-t_i}{X-\xi^i}}
}\end{equation*}
\DIFaddFL{By pre-computing the $\kzg$ proofs $W_1^i=\gtwo{\frac{T(X)-t_i}{X-\xi^i}}$ for all $i\in [N]$, the encoded quotient can be
	evaluated using $O(m)$ $\Gtwo$-operations and $O(m\log^2 m)$ $\F$-operations.
	The identity is then checked using a real pairing check $$e(\gone{T(X)}-\gone{T_I(X)},\gtwo{1})=e(\gone{Z_I(X)},\gtwo{Q(X)}).$$
}\DIFaddendFL 
%\DIFdelbeginFL %DIFDELCMD < \item %%%
%\DIFdelFL{$\prover$ sends $c_Q'=\gone{Q'(X)}$.
%}%DIFDELCMD < \item %%%
%\DIFdelFL{$\verifier$ sends $x\gets \F$.
%}%DIFDELCMD < \end{enumerate}
%%DIFDELCMD < 						
%
%%DIFDELCMD < 						{\bf %%%
%\DIFdelFL{Round 6}%DIFDELCMD < }%%%
%\DIFdelFL{: Prover sends more evaluations.
%}%DIFDELCMD < \begin{enumerate}[leftmargin=1em, label=\arabic*.]
%%DIFDELCMD < 							\item %%%
%\DIFdelFL{$\prover$ computes $\val{x}{u}=u(x)$}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{e(}\gone{T(X)}\DIFaddFL{-}\gone{T_I(X)}\DIFaddendFL ,\DIFdelbeginFL \DIFdelFL{$\val{\nu x}{u}=u(\nu x)$}\DIFdelendFL \DIFaddbeginFL \gtwo{1}\DIFaddFL{)=e(}\gone{Z_I(X)}\DIFaddendFL ,\DIFdelbeginFL \DIFdelFL{$\val{x}{h}=h(x)$, $\val{x}{Q'}=Q'(x)$
%}%DIFDELCMD < \item %%%
%\DIFdelFL{$\prover$ sends $\val{x}{u}$, $\val{\nu x}{u}$, $\val{x}{h}$, $\val{x}{Q'}$.
%}%DIFDELCMD < \item %%%
%\DIFdelFL{$\verifier$ checks $\val{x}{Q'}(x^m-1)=\val{\nu x}{u}(1+\beta \tau_1(x))-\val{x}{u}(\alpha - \val{x}{h})$.
%}%DIFDELCMD < \item %%%
%\DIFdelFL{$\verifier$ sends $r_a, r_h, r_q, r_v, r_K, r_Q, r_Z\gets \F$ }\DIFdelendFL 
\DIFaddbeginFL 
\DIFaddFL{Similarly, we also pre-compute the encoded quotients $W_2^i=\gtwo{\frac{\ell(X) - i}{X-\xi^i}}$ }\DIFaddendFL 
%and \DIFdelbeginFL \DIFdelFL{$r_h', r_u', r_Q'\gets \F$.
%}%DIFDELCMD < \end{enumerate}
%%DIFDELCMD < 						
%
%%DIFDELCMD < 						\begin{center}
%%DIFDELCMD < 							%%%
%\DIFdelFL{Continue in Figure ~\ref{fig:complete-listing-3}.
%}%DIFDELCMD < \end{center}
%%DIFDELCMD < 						
%
%%DIFDELCMD < 						
%%DIFDELCMD < 					\end{mdframed}
%%DIFDELCMD < 					%%%
%%DIFDELCMD < \caption{%
%{%DIFAUXCMD
%	\DIFdelFL{Batching-Efficient RAM Protocol: Continued}}
%%DIFAUXCMD
%%DIFDELCMD < \label{fig:complete-listing-2}
%%DIFDELCMD < 				\end{figure}
%%DIFDELCMD < 				
%
%%DIFDELCMD < 				\begin{figure}[t!]
%%DIFDELCMD < 					\begin{mdframed}
%%DIFDELCMD < 						\begin{center}
%%DIFDELCMD < 							%%%
%\DIFdelFL{Continued from Figure \ref{fig:complete-listing-2}
%}%DIFDELCMD < \end{center}
%%DIFDELCMD < 						\item {\bf %%%
%\DIFdelFL{Round 7}%DIFDELCMD < }%%%
%\DIFdelFL{: Check aggregated evaluation.
%}%DIFDELCMD < \begin{enumerate}[leftmargin=1em, label=\arabic*.]
%%DIFDELCMD < 							\item %%%
%\DIFdelFL{$\prover$ computes:
%}\begin{align*}
%	\DIFdelFL{\Phi_\alpha(X) }&\DIFdelFL{= r_a a(X)+ r_h h(X) + r_q q(X) + r_v v(X) }\\
%	&\DIFdelFL{\quad + r_K K(X) + r_Q Q(X) + r_Z Z_I(X) }\\
%	\DIFdelFL{\Phi_x(X) }&\DIFdelFL{= r_h' h(X) + r_u' u(X)+r_Q'Q'(X)
%}\end{align*}%DIFAUXCMD
%%DIFDELCMD < 							\item %%%
%\DIFdelFL{$\prover$ computes $\Pi_\alpha = \KZGopen(\srs, \Phi_\alpha(X), \alpha)$.
%}%DIFDELCMD < \item %%%
%\DIFdelFL{$\prover$ computes $\Pi_x = \KZGopen(\srs, \Phi_x(X), x)$.
%}%DIFDELCMD < \item %%%
%\DIFdelFL{$\prover$ computes $\Pi_g = \KZGopen(\srs, g(X), \val{\alpha}{h})$.
%}%DIFDELCMD < \item %%%
%\DIFdelFL{$\prover$ computes $\Pi_u = \KZGopen(\srs, u(X), \nu x)$.
%}%DIFDELCMD < \item %%%
%\DIFdelFL{$\prover$ sends $\Pi_\alpha$, $\Pi_x$, $\Pi_g$ and $\Pi_u$.
%}%DIFDELCMD < \item %%%
%\DIFdelFL{$\verifier$ computes:
%}\begin{align*}
%	\DIFdelFL{\gone{\Phi_\alpha} }&\DIFdelFL{= r_a c_a + r_h c_h + r_q c_q + r_v c_v + r_z c_z + r_K c_K + r_Q c_Q. }\\
%	\DIFdelFL{\gone{\Phi_x} }&\DIFdelFL{= r_h' c_h + r_u' c_u + r_Q' c_Q' }\\
%	\DIFdelFL{V_{\alpha} }&\DIFdelFL{= r_a \val{\alpha}{a} + r_h \val{\alpha}{h} + r_q \val{\alpha}{q} + r_v \val{\alpha}{v} }\\
%	&\DIFdelFL{\quad + r_z\val{\alpha}{Z} r_K \val{\alpha}{K} + r_Q \val{\alpha}{Q}. }\\
%	\DIFdelFL{V_x }&\DIFdelFL{= r_h' \val{x}{h} + r_u' \val{x}{u}+r_Q' \val{x}{Q'}
%}\end{align*}%DIFAUXCMD
%%DIFDELCMD < 							\item %%%
%\DIFdelFL{$\verifier$ checks:
%}%DIFDELCMD < \begin{itemize}[leftmargin=1em]
%%DIFDELCMD < 								\item %%%
%\DIFdelFL{$\KZGverify(\srs, \gone{\Phi_\alpha}, V_\alpha, \alpha, \Pi_\alpha)$.
%}%DIFDELCMD < \item %%%
%\DIFdelFL{$\KZGverify(\srs, \gone{\Phi_x}, V_x, x, \Pi_x)$.
%}%DIFDELCMD < \item %%%
%\DIFdelFL{$\KZGverify(\srs, c_g, \val{h(\alpha)}{g}, \val{\alpha}{h}, \Pi_g)$.
%}%DIFDELCMD < \item %%%
%\DIFdelFL{$\KZGverify(\srs, c_u,\val{\nu x}{u}, \nu x, \Pi_u)$.
%}%DIFDELCMD < \end{itemize}
%%DIFDELCMD < 							\item %%%
%\DIFdelFL{$\prover$ and $\verifier$ set $c_S=(c_a, c_v)$, $c_S'=(c_a, c_v')$, $c_o=(c_\op, c_a, c_w)$.
%}%DIFDELCMD < \item %%%
%\DIFdelFL{$\prover$ and $\verifier$ execute argument for $(c_S, c_o, c_S')\in \CLRAM$ (Section ~\ref{sec:poly-proto-ram}).
%}%DIFDELCMD < \end{enumerate}
%%DIFDELCMD < 					\end{mdframed}
%%DIFDELCMD < 					%%%
%%DIFDELCMD < \caption{%
%{%DIFAUXCMD
%	\DIFdelFL{Batching-Efficient RAM Protocol-Continued}}
%%DIFAUXCMD
%%DIFDELCMD < \label{fig:complete-listing-3}
%%DIFDELCMD < 				\end{figure}
%%DIFDELCMD < 				%%%
%\DIFdelend 

\DIFaddbegin \DIFadd{$W_3^i=\gtwo{\frac{\vpolyN(X)}{X-\xi^i}}$ for all $i\in [N]$.
	The quotients can be computed in time $O(N\log N)$ using the techniques in ~\mbox{%DIFAUXCMD
		\cite{EPRINT:FeiKho23}}\hskip0pt%DIFAUXCMD
	. Using $\kzg$ commitment
	scheme the polynomial relations over $Z_\setV$ can be checked in a standard manner
	by having the prover send evaluation proofs for the committed polynomials at a random point chosen by the verifier.
	The total prover effort incurred is $O(m^2)$ group and field operations.
	Thus, we have:
}\begin{lemma}\label{lem:comm-index-lookup}
	\DIFadd{Assuming $\kzg$ is extractable polynomial commitment scheme, there exists a succinct argument of knowledge for
		the relation $\RLOOK$ with prover complexity of $O(m^2)$, given access to pre-computed parameters of size $O(N)$.
}\end{lemma}
\DIFaddend 






\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
